On Sun Oct 25 2015 19:27:52, dariusk opened a new issue called "Resources". But it's an admin issue, so who cares?
On Sun Oct 25 2015 19:28:38, dariusk opened a new issue called "Procedural Visual Novel".
On Sun Oct 25 2015 19:55:08, bcj opened a new issue called "An Attempt to Exhaust Memory to Simulate a Place to Attempt to Exhaust".
On Sun Oct 25 2015 22:41:47, zachwhalen opened a new issue called "Using a password dump for a corpus".
On Mon Oct 26 2015 00:20:15, MaxBittker opened a new issue called "char-rnn + Interactive Fiction + scraped news".
On Mon Oct 26 2015 00:23:26, MaxBittker commented: "what are your ideas for the visual part of this visual novel? have you ever clicked around drawr? it's like pinterest+deviant art  http://drawr.net/ "
On Mon Oct 26 2015 01:31:20, beaugunderson opened a new issue called "Poetry as corpus".
On Mon Oct 26 2015 02:22:23, hugovk commented: "> ### The journal of a password cracker 
> First I entered youliaha and 3814529 but didn't get it.
> So I typed in LTrain823 and amdathlo but didn't gain access.
> Changing tack, I keyed in aleksey_80afonskiy and afonya0603 but no dice.
> Next I tried 000126 and avocat1 which -- guess what -- was unsuccessful.
...
> Finally, I bashed in arsefrem and arsefremenko and surprise surprise! Nothing.
> Oh how I wish they would automate this.
"
On Mon Oct 26 2015 02:23:24, hugovk opened a new issue called "In!".
On Mon Oct 26 2015 03:38:20, javierarce commented: "Maybe someone finds my collection of libraries and APIs useful: https://github.com/javierarce/toolbox"
On Mon Oct 26 2015 04:31:37, hugovk opened a new issue called "Press Coverage". But it's an admin issue, so who cares?
On Mon Oct 26 2015 04:45:18, ojahnn opened a new issue called "Sound over meaning".
On Mon Oct 26 2015 07:24:07, cpressey commented: "After last year's NaNoGenMo I said I would try to extract useful things from the [NaNoGenLab](https://github.com/catseye/NaNoGenLab) and package them in a more useful way, and, so, yes, this is what I extracted:

* [T-Rext](https://github.com/catseye/T-Rext) cleans up spacing and punctuation in a text file.
* [Guten-gutter](https://github.com/catseye/Guten-gutter) attempts to strip Project Gutenberg boilerplate from a text file (and succeeds more often than gutenizer does)
* [seedbank](https://github.com/catseye/seedbank) makes any Python script record and be able to replay the random seeds it chooses, with a one-line change.  Useful for improving reproducibility.

They all require Python 2.x (tested on Python 2.7.6 but will probably still work on some earlier versions) but only seedbank requires that you write your script in Python - the other two can be used as stand-alone tools.

They're all in the public domain, as is all the stuff in the NaNoGenLab.  So, please steal, fold, spindle, and mutilate as you see fit."
On Mon Oct 26 2015 08:05:52, cpressey opened a new issue called "Compiler pipeline + writers' techniques = a "proper novel" ::blink::".
On Mon Oct 26 2015 08:18:40, TheCommieDuck commented: "That pretty much sums up my thoughts/goals too. 

Is it really so unrealistic?"
On Mon Oct 26 2015 08:47:39, coleww opened a new issue called "The Null Earth Catalog".
On Mon Oct 26 2015 09:04:56, WhiteFangs commented: "I can't wait to read (and see how you implement) the second one!"
On Mon Oct 26 2015 09:05:02, bcj commented: "I've spent some time thinking about what would be the most Borgesian thing I could do with The Library of Babel, and I think the answer lies in _Pierre Menard, Author of the Quixote_. I might try to find Don Quixote hidden within the library of Babel.

The simplest implementation of this would be to sanitize the Quixote so that it only contained `[a-z ,.]` then break it into 80-character 40-line pages, and search for each page using the library's fantastic search function. There are ~2,000,000 characters in the novel, so that would only be ~1000 pages. In that case, my 'novel' would probably just be the list of hex locations of the pages.

I don't know if I'd like this as _the_ NaNoGenMo submission. I may like it as _a_ NaNoGenMo submission."
On Mon Oct 26 2015 09:05:19, muffinista opened a new issue called "Travelogue".
On Mon Oct 26 2015 09:07:28, bcj commented: "I've yet to come up with any real way of doing number two, although it might involve attempts to exhaust a place on twitter"
On Mon Oct 26 2015 09:08:19, hugovk commented: "Second one -- find (or set up your own!) webcam pointing out the window, hook it up to some computer vision library that verbosely logs what it sees."
On Mon Oct 26 2015 09:13:21, bcj commented: "I guess if I was using twitter, I could use a bot, have it follow anyone that follows it, then use its feed as the events it witnesses. I don't think there's quite enough there as an idea though."
On Mon Oct 26 2015 09:36:36, MichaelPaulukonis commented: "Are you going to actually post your code this year? &lt;/dubious>"
On Mon Oct 26 2015 09:38:43, dariusk commented: "I didn't finish a novel last year so I didn't post any code.
"
On Mon Oct 26 2015 09:43:26, MichaelPaulukonis opened a new issue called "The Programmer Who Had No Heart in His Body".
On Mon Oct 26 2015 09:47:57, MichaelPaulukonis commented: "word-vomit may produce mostly nonsense, but if the source words are from the original POS-template source, they may be similar.

I did something similar this past year (I had to rewrite spewer because the array traversals were driving me bananas). As usual, haven't cleaned and packaged it, but it at [tagspewer](https://github.com/MichaelPaulukonis/tagspewer) and used in [dprk sloganeering](https://github.com/MichaelPaulukonis/dprk.sloganeering) (output at https://twitter.com/KimIlSungismBot).
"
On Mon Oct 26 2015 09:59:25, coleww commented: "https://twitter.com/pataphysyllabus this bot is already running a small version of the pipeline, taking random lines from poetrydb and the top 100 itunes tracks and running them through"
On Mon Oct 26 2015 10:00:36, bcj commented: "The webcam thing seems like a pretty cool idea. I don't know if there are free computer-vision libraries that would be good enough though.

I'm a little bit thinking of trying to make some sort of city simulation and then logging what happens at a specific area."
On Mon Oct 26 2015 10:01:39, bcj commented: "That is an awesome idea"
On Mon Oct 26 2015 10:02:55, cpressey commented: "Well, I guess we'll see, but yes I think it's incredibly unrealistic."
On Mon Oct 26 2015 10:34:41, MichaelPaulukonis commented: "Conceptually, you might want to stumble around [The Living Handbook of Narratology ](http://www.lhn.uni-hamburg.de/contents) - for some overviews of narrative theory, and some alternate takes on what might constitute narrative."
On Mon Oct 26 2015 11:01:52, ikarth commented: "[ProcJam](http://www.procjam.com/resources/) is more visually oriented, but it does have some useful resources that come out of it, like [Tracery](https://github.com/galaxykate/tracery), a JavaScript library for generating stories from expanding grammars."
On Mon Oct 26 2015 11:03:28, ikarth opened a new issue called "Plans for 2015: Brainstorming and Last Year's Postmortem".
On Mon Oct 26 2015 11:04:29, ikarth commented: "Perfect! And definitely a worthy goal."
On Mon Oct 26 2015 11:09:17, ikarth commented: "The computer vision thing depends on how you want to use the data; katie rose pipkin's [cloud ocr](http://ifyoulived.org/translations.html) is one approach. There are open source image recognition libraries out there, of varying quality.

I like the simulation idea too. There's a lot of scope to explore more of that approach, and I think it's been somewhat underdeveloped historically."
On Mon Oct 26 2015 11:10:01, ikarth commented: "Solved the problem of posting the result on the web?"
On Mon Oct 26 2015 11:14:52, enkiv2 commented: "I'm glad you're trying this out this year. Very disappointed that you gave up on it last year (and disappointed that my ~2 attempts at generated VNs were largely unsuccessful)"
On Mon Oct 26 2015 11:22:39, enkiv2 opened a new issue called "Combining generative grammars with plot templates".
On Mon Oct 26 2015 11:29:27, dariusk commented: "@ikarth Yeah, there's stuff like [TyranoScript](http://www.evanburchard.com/tyranoscript/) now!"
On Mon Oct 26 2015 11:38:32, MichaelPaulukonis commented: "I will do my best not to lapse into template-related rants in GitHub."
On Mon Oct 26 2015 11:39:46, enkiv2 commented: "I will not be displeased by more template-related rants on github. Ranting
is how we clarify our own thoughts, and are also a wonderful didiactic
resource. Rant away.

On Mon, Oct 26, 2015 at 11:38 AM Michael Paulukonis <
notifications@github.com> wrote:

> I will do my best not to lapse into template-related rants in GitHub.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/16#issuecomment-151177586>
> .
>
"
On Mon Oct 26 2015 11:40:14, MichaelPaulukonis commented: "Tracey was recently mentioned in [Generative Text](https://groups.google.com/forum/#!forum/generativetext) - the mailing list/google-group that sprouted from the end of NNGM2014. Lately, there's been a lot of partisan ranting about templates. I may resemble some of that remark."
On Mon Oct 26 2015 11:44:30, MichaelPaulukonis commented: ">> Oh how I wish they would automate this.

How could such a thing be automated?

## IT'S AGAINST NATURE!"
On Mon Oct 26 2015 11:45:56, enkiv2 commented: "I'd like to point out that the BotAlly slack group has a NaNoGenMo room,
and that the GenArt slack group that sprouted from the Generative Text
mailing list is mostly full of NaNoGenMo2014 people.

On Mon, Oct 26, 2015 at 11:40 AM Michael Paulukonis <
notifications@github.com> wrote:

> Tracey was recently mentioned in Generative Text
> <https://groups.google.com/forum/#!forum/generativetext> - the mailing
> list/google-group that sprouted from the end of NNGM2014. Lately, there's
> been a lot of partisan ranting about templates. I may resemble some of that
> remark.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/1#issuecomment-151178023>
> .
>
"
On Mon Oct 26 2015 11:47:56, MichaelPaulukonis opened a new issue called "Language Survey 2015". But it's an admin issue, so who cares?
On Mon Oct 26 2015 11:49:39, dkurth opened a new issue called "Exquisite Corpus".
On Mon Oct 26 2015 11:50:20, ikarth commented: "Since we're speaking of [visual novels](https://github.com/dariusk/NaNoGenMo-2015/issues/2), I should probably mention the [Yarn dialog editor](https://github.com/InfiniteAmmoInc/Yarn). I don't know that'll be all that useful directly for generative stuff, but in case someone is trying to procedurally generate a dialog tree, it might be handy. Or you could always generate a Twine file."
On Mon Oct 26 2015 12:08:06, pteichman opened a new issue called "Intent to Participate".
On Mon Oct 26 2015 12:15:09, ikarth commented: "@enkiv2 Where's the signup for the GenArt group? I need to check that out."
On Mon Oct 26 2015 12:18:39, alicemaz opened a new issue called "I'll be here".
On Mon Oct 26 2015 12:18:39, enkiv2 commented: "The genart group is at https://generativeart.slack.com ; I *think* it might
be invite-only. Mike Paulukonis controls the group & can probably send
invites.

On Mon, Oct 26, 2015 at 12:15 PM Isaac Karth <notifications@github.com>
wrote:

> @enkiv2 <https://github.com/enkiv2> Where's the signup for the GenArt
> group? I need to check that out.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/1#issuecomment-151193274>
> .
>
"
On Mon Oct 26 2015 12:19:55, enkiv2 commented: "As for botally, the public signup is here: http://t.co/OiOB7197BH ; people
periodically check for registrations and send invites out.

On Mon, Oct 26, 2015 at 12:18 PM John Ohno <john.ohno@gmail.com> wrote:

> The genart group is at https://generativeart.slack.com ; I *think* it
> might be invite-only. Mike Paulukonis controls the group & can probably
> send invites.
>
> On Mon, Oct 26, 2015 at 12:15 PM Isaac Karth <notifications@github.com>
> wrote:
>
>> @enkiv2 <https://github.com/enkiv2> Where's the signup for the GenArt
>> group? I need to check that out.
>>
>> —
>> Reply to this email directly or view it on GitHub
>> <https://github.com/dariusk/NaNoGenMo-2015/issues/1#issuecomment-151193274>
>> .
>>
>
"
On Mon Oct 26 2015 12:21:42, hoffm opened a new issue called "Novel? Yes please. Thanks.".
On Mon Oct 26 2015 12:26:25, dariusk commented: "NB: I'm one of three admins for the #BotALLY slack--we welcome new
additions but we take our code of conduct VERY seriously so please only
join if you're okay with being in a place where you almost inevitability
will be nudged by an admin concerning your conduct.
"
On Mon Oct 26 2015 12:28:26, MichaelPaulukonis commented: "Yeah, there's not a lot going on in https://generativeart.slack.com - and if there's a way to allow people to request an invite, I'm all ears. @dariusk  - what's the source for the #botALLY registration doc+script?

----

UPDATE: he's the GenerativeArt invite page: http://tinyurl.com/genartslackrequest"
On Mon Oct 26 2015 12:29:21, ikarth commented: "@MichaelPaulukonis I've seen people use a Google Form to manage signups for Slack groups."
On Mon Oct 26 2015 12:30:53, enkiv2 commented: "So far as I can tell, the signup just is a web form that sends an email to
the slack maintainer. Maybe google forms has this functionality by itself?

On Mon, Oct 26, 2015 at 12:29 PM Isaac Karth <notifications@github.com>
wrote:

> @MichaelPaulukonis <https://github.com/MichaelPaulukonis> I've seen
> people use a Google Form to manage signups for Slack groups.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/1#issuecomment-151197445>
> .
>
"
On Mon Oct 26 2015 12:38:29, dariusk commented: "It just emails us.

Let's stop this conversation, I don't want to add too much noise to the
resources thread.
"
On Mon Oct 26 2015 12:45:08, TheCommieDuck opened a new issue called "Propp-guided simulation with rudimentary emotions/behaviours".
On Mon Oct 26 2015 12:47:51, enkiv2 commented: "For NaNoGenMo 2013 I worked a bit with translating the CMU NELL knowledge
base to prolog. If you're planning to produce an internally consistent but
reasonably free-form story and you're familiar with prolog (and
particularly using DCGs), there are worse ways to go.

On Mon, Oct 26, 2015 at 12:45 PM Mark Garnett <notifications@github.com>
wrote:

> I've been thinking about this for a while, actually. I definitely want to
> try something ala what @cpressey <https://github.com/cpressey> mentioned
> - avoiding more unpredictable stuff and adjusting existing stories.
>
> I'm still undecided on an approach. I think I'll try going top down (plot
> -> events -> text) rather than just making random sentences from a grammar.
>
> A prediction from the start? This isn't going to end up with 50,000 words
> in any sane capability. I think something which seems a lot more like a
> very long, linear, short story IS possible; that is, the plot events become
> incredibly granular and small (if a normal novel has, say, 10 different
> major events in 50k words - this will probably have 200-300).
>
> It could also be that this is all complete garbage and I'll swap to a
> destructive approach from existing material. But who knows, we'll see.
>
> Also undecided between using Prolog, C#, or maybe finding something else
> to use.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/22>.
>
"
On Mon Oct 26 2015 12:53:13, TheCommieDuck commented: "Ooh, that's neat. 

I've done some DCG stuff before - failed attempts at text adventures and MUD-y stuff that never got off the ground in my head, let alone in code.

I'm still not entirely sure about ontologies for this kind of thing. I can imagine them being incredibly neat (with so much data to pick from) but also incredibly silly ('The man went outside and saw a chinese mitten crab and a Alfa Romeo 155 with Acetaldehyde Ethyl Phenylethyl Acetal in')."
On Mon Oct 26 2015 12:56:17, enkiv2 commented: "Definitely. NELL is notable because it's an ontology generated via machine
learning from the internet (specifically, I think it's reading twitter and
wikipedia?). As a result, it's a little less completionist & a little more
skewed toward things people care about.

I never got much further than converting a large subset to prolog and
producing a large set of true but uninteresting statements about categories.

On Mon, Oct 26, 2015 at 12:53 PM Mark Garnett <notifications@github.com>
wrote:

> Ooh, that's neat.
>
> I've done some DCG stuff before - failed attempts at text adventures and
> MUD-y stuff that never got off the ground in my head, let alone in code.
>
> I'm still not entirely sure about ontologies for this kind of thing. I can
> imagine them being incredibly neat (with so much data to pick from) but
> also incredibly silly ('The man went outside and saw a chinese mitten crab
> and a Alfa Romeo 155 with Acetaldehyde Ethyl Phenylethyl Acetal in').
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/22#issuecomment-151207135>
> .
>
"
On Mon Oct 26 2015 13:16:29, aeschright opened a new issue called "An exercise in hyperbole".
On Mon Oct 26 2015 13:44:13, ikarth commented: "There's been a lot of work done with Neural Networks this year, including text-based output (such as the generated [Magic: the Gathering cards](http://www.mtgsalvation.com/forums/creativity/custom-card-creation/612057-generating-magic-cards-using-deep-recurrent-neural)). That might be an interesting avenue to pursue.

[Abulafia](http://random-generator.com/index.php?title=Main_Page) uses MediaWiki for its generators. Not directly useful, but I keep forgetting to mention it here.

Here's a method of [procedurally generating a wilderness in a text adventure format](http://www.sibylmoon.com/a-procedurally-generated-wilderness/). People who are doing a _Swallows_ type simulation approach may find its ideas useful."
On Mon Oct 26 2015 13:49:52, s-knob commented: "Perhaps instead of attempting to exhaust a place on twitter, you could attempt to exhaust a *moment*? For example, the first few hours after the Snowden Papers were leaked. You could collect relevant tweets and manipulate or sort them in some way. "
On Mon Oct 26 2015 13:54:14, s-knob opened a new issue called "An attempt to exhaust a moment.".
On Mon Oct 26 2015 13:56:00, MichaelPaulukonis commented: "Welcome! Something is better than nothing. Commit early, iterate often. Fail fast."
On Mon Oct 26 2015 13:59:24, aparrish opened a new issue called "a novel, generated".
On Mon Oct 26 2015 14:00:50, bcj commented: "That is an interesting idea, and I _think_ that _Life: A User's manual_ by Georges Perec takes place at exactly one moment, so it is very fitting."
On Mon Oct 26 2015 14:05:25, araile opened a new issue called "Pocket Atlas of Remote Planets".
On Mon Oct 26 2015 14:09:36, vijithassar commented: "[Multiverse JSON](https://github.com/vijithassar/multiverse-json) is a light JSON syntax/spec and Python compiler script with which to store editorial projects in small logical units so they can be quickly reconfigured according to build parameters you define."
On Mon Oct 26 2015 14:21:27, aparrish commented: "Since last year's NaNoGenMo I've made a few libraries that might be of interest:

* [pronouncingpy](https://github.com/aparrish/pronouncingpy), a simple and easy-to-use interface to the CMU pronouncing dictionary
* [pronouncingjs](https://github.com/aparrish/pronouncingjs), a Javascript port of the above
* [pycorpora](https://github.com/aparrish/pycorpora), a Python interface for [Corpora Project](https://github.com/dariusk/corpora)

I also made [Context-Free GenGen](http://cfgg.decontextualize.com/), which is sort of like a mini-Tracery except driven by Google Sheets (a la the original [GenGen](http://tinysubversions.com/gengen/)). Probably not great for making a whole novel, but maybe cool for prototyping ideas with context-free grammar generation."
On Mon Oct 26 2015 14:35:53, mgiraldo commented: "- some [NYPL-related](http://digitalcollections.nypl.org/about) resources
- if you want to add some savory meals to your novel, you could try the [NYPL menus dataset](http://menus.nypl.org/data)
- a [sanitized version of the menus dataset](http://www.curatingmenus.org/data_dictionary/) by @trevormunoz and Katie Rawson"
On Mon Oct 26 2015 14:41:16, enkiv2 commented: "I have a few tools I've made in the past year too:

   - GGC <https://github.com/enkiv2/ggc>, a language for generative
   grammars that compiles to python (very similar to tracery)

Some novelty filters:

   - dirtyreader <https://github.com/enkiv2/dirtyreader>, a tool to
   increase the profanity of text in a configurable way
   - aphasia <https://github.com/enkiv2/aphasia> and synonym-warp
   <https://github.com/enkiv2/synonym-warp>, tools for replacing words with
   related words
   - tone-poem <https://github.com/enkiv2/tonepoem>, blackout-gen
   <https://github.com/enkiv2/blackout-gen>, sestina-gen
   <https://github.com/enkiv2/sestina-gen>, and haiku-generator
   <https://github.com/enkiv2/haiku-generator>: tools for producing
   different varieties of poems from source text

And, some novelty generators:

   - anxiety-generator <https://github.com/enkiv2/anxiety-generator>, a
   python port of Paul Ford's tool for procedurally generated
   passive-aggressive verbal abuse
   - Tinder For Guinea Pigs <https://github.com/enkiv2/tfgp>, which
   generates nonsense startup pitches
   - tftf-gen <https://github.com/enkiv2/tftf-gen>, which describes
   randomly generated science fiction set pieces (based on a description of
   Thing From The Future)



On Mon, Oct 26, 2015 at 2:21 PM Allison Parrish <notifications@github.com>
wrote:

> Since last year's NaNoGenMo I've made a few libraries that might be of
> interest:
>
>    - pronouncingpy <https://github.com/aparrish/pronouncingpy>, a simple
>    and easy-to-use interface to the CMU pronouncing dictionary
>    - pronouncingjs <https://github.com/aparrish/pronouncingjs>, a
>    Javascript port of the above
>    - pycorpora <https://github.com/aparrish/pycorpora>, a Python
>    interface for Corpora Project <https://github.com/dariusk/corpora>
>
> I also made Context-Free GenGen <http://cfgg.decontextualize.com/>, which
> is sort of like a mini-Tracery except driven by Google Sheets (a la the
> original GenGen <http://tinysubversions.com/gengen/>). Probably not great
> for making a whole novel, but maybe cool for prototyping ideas with
> context-free grammar generation.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/1#issuecomment-151236207>
> .
>
"
On Mon Oct 26 2015 14:47:17, cassidoo opened a new issue called "Participation, I declare".
On Mon Oct 26 2015 15:01:39, cblgh opened a new issue called "Participating,".
On Mon Oct 26 2015 15:05:19, julianengel opened a new issue called "Participating too".
On Mon Oct 26 2015 15:06:54, EmilyGoetz opened a new issue called "Sure why not.".
On Mon Oct 26 2015 15:18:50, CatherineMcMahon opened a new issue called "Since I can't actually write 50k words of stuff I'll code something that does it for me".
On Mon Oct 26 2015 16:18:26, hugovk commented: "> Perhaps instead of attempting to exhaust a place on twitter, you could attempt to exhaust a *moment*? For example, the first few hours after the Snowden Papers were leaked. You could collect relevant tweets and manipulate or sort them in some way.

[Here's](https://archive.org/details/fergusoncrawl&tab=about) an archive 13 million "tweets that mention Ferguson, Missouri between August 10th and August 27th, 2014 subsequent to the death of Michael Brown". It's actually just the IDs, but you can use [twarc](https://github.com/edsu/twarc) (the tool that collected them) to "rehydrate" them to full tweet data. More about this archive [here](http://inkdroid.org/2014/08/30/a-ferguson-twitter-archive/) which also mentions an Aaron Swartz tweet archive, and there's quite possibly more archives over [here](https://github.com/edsu?tab=repositories).

"
On Mon Oct 26 2015 16:37:26, bcj commented: "Wow, that's pretty awesome. Thanks"
On Mon Oct 26 2015 17:05:02, KyFaSt opened a new issue called "Robot Love Poems or Songs Collection".
On Mon Oct 26 2015 17:28:27, bcj commented: "The Internet Archive has a collection of manuals (https://archive.org/details/manuals). Unfortunately, they are mostly electronics. I'm guessing the printer manuals will have plenty of [hands sliding things into machines](https://archive.org/stream/printermanual-canon-np-1550-service-manual/canonnp1550servicemanual#page/n9/mode/2up) If that's the kind of thing that you are looking for."
On Mon Oct 26 2015 17:34:08, KyFaSt commented: "hmm these printer manuals could be quite vulgar in the context of my project, this is perfect. Thank you @bcj :heart: "
On Mon Oct 26 2015 17:40:49, zachwhalen commented: "Adding this as it may be of interest:

Last year, I attempted to create a graphic novel. <a href="https://github.com/zachwhalen/NaGraNoGenMo">The result</a> was late and not as compelling as I hoped it would be, *The Something, The Thanksgiving, and The Nothing*. 

I made an off-season sequel that I think is more compelling. So much so I decided to self-publish it with Lulu. Here's the first of what may be a series of blogs explaining <a href="http://www.zachwhalen.net/posts/an-arthrogram/">*An Arthrogram*</a>, which is a book of empty comic layouts. Every possible layout given a constrained *Watchmen*-like grid of 3 x 3 panels.

https://www.youtube.com/embed/JCPj1YXI4-g"
On Mon Oct 26 2015 17:52:32, hugovk commented: "Here's a soundtrack for you (the second one): http://www.bbc.co.uk/blogs/adamandjoe/2009/07/free-sexy-song-wars.shtml"
On Mon Oct 26 2015 19:29:42, WhiteFangs opened a new issue called "A play based on the "french 4chan"".
On Mon Oct 26 2015 19:42:12, ericnakagawa opened a new issue called "Idea: An app that writes a story about where I've been.".
On Mon Oct 26 2015 21:54:18, ikarth commented: "If you're looking for a large corpus of internet comments, the mostly complete [Reddit corpus](https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/) is available."
On Tue Oct 27 2015 05:04:25, cpressey commented: "I should maybe qualify those statements a bit.

I do think the goal I stated is highly unrealistic, certainly with the techniques that I'm personally prepared to use.  But the space of possible techniques is vast, so who knows?

What I'm sort of getting at by choosing that goal is this:

In 2013, I tried generating a "proper novel". Last year, I did a bunch of experiments closer to the so-called "conceptual writing" side of things.  This year, I'm returning to the "proper novel", however quixotic any such attempt might be.

Given that I've stated a goal that I admit is unrealistic, I suppose I do not expect myself to actually achieve it.  But it will be interesting to see how I fail.

At the same time, one need not have only one goal, so...

After last NaNoGenMo, around January of this year, I started thinking a lot about how people write stories.  I did a lot of research (if you can call reading article after article on TVTropes research) and I came to the conclusion that there are certainly some story-writing techniques that can be approximated with algorithms.

So, one of my secondary goals is: To implement one or more story-writing techniques that human writers use.

This is a much more realistic goal, I think.

Heck, even The Swallows had a [MacGuffin](http://tvtropes.org/pmwiki/pmwiki.php/Main/MacGuffin), but it wasn't really developed.  I'd like to go a bit beyond that.

I'll probably continue to expand on these thoughts in future posts to this issue."
On Tue Oct 27 2015 08:38:00, MichaelPaulukonis commented: "> In 2013, I tried generating a "proper novel".

::blink blink::

[updated as I had not pasted what I wanted to have pasted]"
On Tue Oct 27 2015 08:40:48, gray-signal opened a new issue called "i'm so unbelievably intimidated but why the h*ck not, let's do this.".
On Tue Oct 27 2015 09:58:03, coleww commented: "i have been pretty heavily tooling up for nanogenmo this year by way of publishing a ton of poetic node modules to npm. 

- [word-vomit replaces words with other words of the same part of speech](https://www.npmjs.com/package/word-vomit)
- [queneau buckets for letters](https://www.npmjs.com/package/queneau-letters)
- [queneau buckets for words](https://www.npmjs.com/package/queneau-buckets)
- [replace words with words that rhyme](https://www.npmjs.com/package/poetic-vomit)
- [n-plus, replace words with the word N words ahead of it in a dicitonary](https://www.npmjs.com/package/n-plus-7)
- [replace character sequences with other character sequences, good for l-system poetry potentially](https://www.npmjs.com/package/new-slang)
- [detect lipogrammatic text (omits certain characters)](https://www.npmjs.com/package/lipogram)
- [detect text that snowballs (i.e, every word is 1 char longer than the previous)](https://www.npmjs.com/package/is-snowball)
- [add some random diacritic marks to yr text](https://www.npmjs.com/package/diacriticize)


Another resource i have created is the [weirdly-specific-corpora](https://github.com/coleww/weirdly-specific-corpora) project, which is a fork of dariusk/corpora for any lists that are too weirdly specific to be included in that project. I expect I will end up making a lot of weird lists while generating novels."
On Tue Oct 27 2015 10:01:37, YottaSecond opened a new issue called "Using the mysterious "All Junky Pages" corpus".
On Tue Oct 27 2015 10:54:39, YottaSecond commented: "I can imagine a computer-generated book being easier to read than something like Naked Lunch or Finnegan's Wake."
On Tue Oct 27 2015 11:38:41, neauoire opened a new issue called "Can we have labels to better categorize issues?". But it's an admin issue, so who cares?
On Tue Oct 27 2015 11:40:59, dariusk commented: "Nope, I'm using issues as a forum system. You don't even need to use Github
if you don't want to for your code. Just link a novel and your code from
here, whether that's on Github or somewhere else. I'm trying to keep this
as tech/platform neutral as possible.
"
On Tue Oct 27 2015 11:42:02, cpressey commented: "Templates seem like a terrible way to produce sentences until you consider the alternatives.

There, that's my template-related rant."
On Tue Oct 27 2015 11:44:00, neauoire commented: "Gotcha,
Could we suggest using tags/labels to distinguish between intent or ideas to keep issues in order.
Or a sort of tenplate to make easy for everyone to browse issues?

example: [40%] Story Name - Ruby"
On Tue Oct 27 2015 11:45:49, enkiv2 commented: "Tags are typically used to indicate milestones -- if a novel snapshot has
been posted, for instance.

On Tue, Oct 27, 2015 at 11:44 AM Devine Lu Linvega <notifications@github.com>
wrote:

> Gotcha,
> Could we suggest using tags for either intent or ideas to keep issues in
> order.
> Or a sort of tenplate to make easy for everyone to browse issues?
>
> example: [40%] Story Name - Ruby
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/37#issuecomment-151545622>
> .
>
"
On Tue Oct 27 2015 11:46:53, enkiv2 commented: "Most of the limitations of templates can be circumvented by generating them
with templates.

On Tue, Oct 27, 2015 at 11:42 AM Chris Pressey <notifications@github.com>
wrote:

> Templates seem like a terrible way to produce sentences until you consider
> the alternatives.
>
> There, that's my template-related rant.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/16#issuecomment-151545054>
> .
>
"
On Tue Oct 27 2015 11:47:07, hugovk commented: "Last year we had `preview` and `completed` labels:

https://github.com/dariusk/NaNoGenMo-2014/labels"
On Tue Oct 27 2015 11:47:36, neauoire commented: "That could work : ) "
On Tue Oct 27 2015 11:48:09, setphen opened a new issue called "def init():".
On Tue Oct 27 2015 11:49:19, dariusk commented: "I'm not sure we can realistically enforce that... also percentages don't
make sense. I can generate 50k words in five seconds. It's the quality that
counts and that's nonlinear.

I do want to encourage people to title their issues something descriptive,
though most people don't know what they're going to make yet.
"
On Tue Oct 27 2015 11:50:19, dariusk commented: "I think @hugovk and I could simply ask people who seem to have an idea
already to update their issue title accordingly.
On Oct 27, 2015 8:49 AM, "Darius Kazemi" <darius.kazemi@gmail.com> wrote:

> I'm not sure we can realistically enforce that... also percentages don't
> make sense. I can generate 50k words in five seconds. It's the quality that
> counts and that's nonlinear.
>
> I do want to encourage people to title their issues something descriptive,
> though most people don't know what they're going to make yet.
>
"
On Tue Oct 27 2015 11:51:44, dariusk commented: "I'm a huge fan of templated templates with nondeterministic inputs.
"
On Tue Oct 27 2015 11:53:46, ivodopiviz opened a new issue called "Will attempt for great justice".
On Tue Oct 27 2015 11:59:54, enkiv2 commented: "Templates are great in that they allow arbitrary amounts of human
intelligence to be provided in order to reliably trigger the eliza effect
and convince readers that there was some intent behind the writing, which
is often enough to get them to project a meaning upon it. But, for
novel-length text, single-layer templates like mad libs don't produce a
very good ratio of novel text to source code. By using generative grammars,
I think you can get to a nice middle ground between mad lib style
single-level templating and something like markov chains (wherein you have
essentially used statistics to derive a really low level template of the
form "this word may only be followed by these other words" from some source
text). Nick Montfort's 1k story generator demonstrates the power of being
vague but evocative, and by adding a random shuffle to the lines it
produces you can convince yourself that if you are sufficiently vague and
evocative the order of events doesn't need to matter much either; so, if
you can create a grammar that produces a large variety of vague yet
evocative filler text, you can insert such text into arbitrary places in a
plot skeleton to fill it out.

On Tue, Oct 27, 2015 at 11:51 AM Darius Kazemi <notifications@github.com>
wrote:

> I'm a huge fan of templated templates with nondeterministic inputs.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/16#issuecomment-151548025>
> .
>
"
On Tue Oct 27 2015 11:59:57, hugovk commented: "I also think labels are useful, especially for completed entries, because I can quickly go and see the full list from last year and also get a quick count (91 completed, 12 previews)."
On Tue Oct 27 2015 12:01:46, dariusk commented: "Oh I agree we need labels! I can't think of more useful labels than the ones we already have, though of course I'm open to ideas."
On Tue Oct 27 2015 12:03:04, neauoire commented: "I don't seem to be able to pick one for my issue, is there a sort of privacy setting on here?"
On Tue Oct 27 2015 12:07:14, enkiv2 commented: "(We spend a lot of time talking about particular iconic experimental
stories like Finnegan's Wake, A Void, and Exercises in Style when talking
about what kinds of extreme storytelling forms work at least well enough
that there are iconic works written by humans in that form. But, one model
that we might look at is the Illuminatus trilogy, which has a lot of
complicated interactions between various parties and a large number of
events but whose chronology is difficult to determine. The Illuminatus
trilogy easily explains its formal conceits with its subject matter: sex,
drugs, conspiracy theories, and the occult. However, the formal conceits
had help from the same 'writing machine' that Burroughs used in The Naked
Lunch: somebody dropped the entire fifteen hundred page manuscript on the
floor and put it back together in a random order, and then the authors cut
five hundred pages from the result by removing pages entirely at random. A
generator that produces descriptions of events and varies the tone and
style of these descriptions can be set free to scribble together random
accounts of random events and, as long as the characters take the bad acid
in one of these scenarios, really warped first-person accounts can be
assumed to be justifiable as a kind of intellectual realism.)

On Tue, Oct 27, 2015 at 11:59 AM John Ohno <john.ohno@gmail.com> wrote:

> Templates are great in that they allow arbitrary amounts of human
> intelligence to be provided in order to reliably trigger the eliza effect
> and convince readers that there was some intent behind the writing, which
> is often enough to get them to project a meaning upon it. But, for
> novel-length text, single-layer templates like mad libs don't produce a
> very good ratio of novel text to source code. By using generative grammars,
> I think you can get to a nice middle ground between mad lib style
> single-level templating and something like markov chains (wherein you have
> essentially used statistics to derive a really low level template of the
> form "this word may only be followed by these other words" from some source
> text). Nick Montfort's 1k story generator demonstrates the power of being
> vague but evocative, and by adding a random shuffle to the lines it
> produces you can convince yourself that if you are sufficiently vague and
> evocative the order of events doesn't need to matter much either; so, if
> you can create a grammar that produces a large variety of vague yet
> evocative filler text, you can insert such text into arbitrary places in a
> plot skeleton to fill it out.
>
> On Tue, Oct 27, 2015 at 11:51 AM Darius Kazemi <notifications@github.com>
> wrote:
>
>> I'm a huge fan of templated templates with nondeterministic inputs.
>>
>> —
>> Reply to this email directly or view it on GitHub
>> <https://github.com/dariusk/NaNoGenMo-2015/issues/16#issuecomment-151548025>
>> .
>>
>
"
On Tue Oct 27 2015 12:09:15, dariusk commented: "Only admins can tag. (Maybe we should open that up?)
"
On Tue Oct 27 2015 12:10:06, enkiv2 commented: "Could we have a tag for "source available"? Often, projects have really
interesting behavior but aren't considered to be producing sufficiently
interesting material for a novel-length work, and it's easy to miss when we
only mark based on what the author considers 'complete'.

On Tue, Oct 27, 2015 at 12:03 PM Devine Lu Linvega <notifications@github.com>
wrote:

> I don't seem to be able to pick one for my issue, is there a sort of
> privacy setting on here?
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/37#issuecomment-151551545>
> .
>
"
On Tue Oct 27 2015 12:10:54, hugovk commented: "@enkiv2:

> The only rule is that you share at least one novel and also your source code at the end.

No source = not complete."
On Tue Oct 27 2015 12:10:59, dariusk commented: "Also, _Illuminatus!_ (which I loved as a kid) is total potboiler pulpy
genre fiction, which has its own advantages when it comes to generation.
"
On Tue Oct 27 2015 12:13:11, dariusk commented: "I see John's point though. There may be an incomplete, non-novel that still
has source code of note. I'll add that tag.
"
On Tue Oct 27 2015 12:16:39, enkiv2 commented: "Definitely. There are several fairly mechanical plotting guides for pulp,
and at the level of sentences and words, various pulp genres have really
strong stylistic conventions that are entertaining in of themselves
(thinking here of the narration style in hardboiled/noir, and the dense
pseudo-romantic style favored by Lovecraft that's essentially just a more
extreme form of what William Hope Hodgeson and Poe were doing in the
nineteenth century). Doing an Illuminatus pastiche might give you a good
excuse for incorporating filters like jive and cockney into your fiction
generator as a second pass for particular passages and quotes.

On Tue, Oct 27, 2015 at 12:11 PM Darius Kazemi <notifications@github.com>
wrote:

> Also, _Illuminatus!_ (which I loved as a kid) is total potboiler pulpy
> genre fiction, which has its own advantages when it comes to generation.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/16#issuecomment-151553724>
> .
>
"
On Tue Oct 27 2015 12:17:19, setphen commented: "What are the rules regarding human "curation" of the final product? For instance, what if my code creates paragraphs, and then I organize them manually?"
On Tue Oct 27 2015 12:18:20, dariusk commented: "There are no rules except 50k words.
"
On Tue Oct 27 2015 12:33:16, enkiv2 commented: "Even if you don't make 50k words, if your results or ideas are sufficiently
interesting people will like them. (Generated Detective didn't make
anywhere near 50k words and had rather a lot of curation, but was also one
of the more interesting and exciting things going on in NaNoGenMo 2014)

On Tue, Oct 27, 2015 at 12:18 PM Darius Kazemi <notifications@github.com>
wrote:

> There are no rules except 50k words.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/38#issuecomment-151556040>
> .
>
"
On Tue Oct 27 2015 12:33:54, cpressey commented: "Don't forget also: share your source code."
On Tue Oct 27 2015 12:39:27, cpressey commented: "@YottaSecond 

> I can imagine a computer-generated book being easier to read than something like Naked Lunch or Finnegan's Wake.

Mmmaybe...

But I wager that if someone stops reading Finnegan's Wake after chapter 2 it's almost certainly _not_ because their brain went all "I see what you did there.""
On Tue Oct 27 2015 12:53:42, mattfister opened a new issue called "Simulationist Fantasy Novel".
On Tue Oct 27 2015 12:54:37, setphen commented: "Will do. I am planning on utilizing some other open source projects, i.e. speed-to-text and translation"
On Tue Oct 27 2015 13:34:09, lilkraftwerk opened a new issue called "Alice's Adventures in Emojiland".
On Tue Oct 27 2015 14:03:11, lilkraftwerk commented: "working title is "Alice's Adventures in Emojiland""
On Tue Oct 27 2015 14:12:10, dariusk commented: "Can you change the title of the issue to match your working title? Thanks!
"
On Tue Oct 27 2015 14:12:34, lilkraftwerk commented: ":cool: "
On Tue Oct 27 2015 14:59:33, dariusk commented: "Hi, I'm going through and updating the titles on issues to make them more specific. Feel free to edit my edit if it's not to your liking. This is to make browsing issues a lot more pleasant."
On Tue Oct 27 2015 14:59:57, dariusk commented: "Hi, I'm going through and updating the titles on issues to make them more specific. Feel free to edit my edit if it's not to your liking. This is to make browsing issues a lot more pleasant."
On Tue Oct 27 2015 15:00:38, dariusk commented: "Hi, I'm going through and updating the titles on issues to make them more specific. Feel free to edit my edit if it's not to your liking. This is to make browsing issues a lot more pleasant."
On Tue Oct 27 2015 15:01:06, dariusk commented: "Hi, I'm going through and updating the titles on issues to make them more specific. Feel free to edit my edit if it's not to your liking. This is to make browsing issues a lot more pleasant."
On Tue Oct 27 2015 15:01:37, dariusk commented: "Hi, I'm going through and updating the titles on issues to make them more specific. Feel free to edit my edit if it's not to your liking. This is to make browsing issues a lot more pleasant."
On Tue Oct 27 2015 15:10:19, dariusk commented: "@neauoire I've added an admin label to this issue since it's meta-discussion. You might want to open a new issue for your own participation in the event."
On Tue Oct 27 2015 15:10:50, dariusk commented: "Haha, nice title."
On Tue Oct 27 2015 15:11:12, dariusk commented: "It's okay, I'm intimidated every year."
On Tue Oct 27 2015 15:11:49, dariusk commented: "Hi, I'm going through and updating the titles on issues to make them more specific. Feel free to edit my edit if it's not to your liking. This is to make browsing issues a lot more pleasant."
On Tue Oct 27 2015 15:12:16, dariusk commented: "Hi, I'm going through and updating the titles on issues to make them more specific. Feel free to edit my edit if it's not to your liking. This is to make browsing issues a lot more pleasant."
On Tue Oct 27 2015 15:16:13, bcj commented: "Alright, I've pretty much settled on the idea that my main focus for NaNoGenMo will be simulating a city that I can generate a description of. Although I am reserving the right to about face once I recognize the logistics of that goal. I also do plan to spend some time playing with that Twitter data set because that's just cool"
On Tue Oct 27 2015 15:17:24, enkiv2 commented: "Will you be simulating the people in the city, or will this be more of a
SimCity kind of view?

On Tue, Oct 27, 2015 at 3:16 PM Brendan Curran-Johnson <
notifications@github.com> wrote:

> Alright, I've pretty much settled on the idea that my main focus for
> NaNoGenMo will be simulating a city that I can generate a description of.
> Although I am reserving the right to about face once I recognize the
> logistics of that goal. I also do plan to spend some time playing with that
> Twitter data set because that's just cool
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/3#issuecomment-151614952>
> .
>
"
On Tue Oct 27 2015 15:19:17, s-knob commented: "If it's alright with you, I might take on that idea of exhausting a moment. I looked into the novel you mentioned and it is truly fascinating."
On Tue Oct 27 2015 15:28:11, bcj commented: "My current idea is that I would like to make a top-down simulation a city, but for (at least some of) the people who cross into the narrator's view, simulate them on an individual level. I don't have any experience in this kind of stuff, so I have a good feel yet for what kinds of things are going to be possible though. I am excited about that

@s-knob: please do. Even if I end up doing stuff with a moment, I feel like any two implementations would be different and interesting in their own right"
On Tue Oct 27 2015 16:39:13, MichaelPaulukonis commented: "Once there was a novel and you could tell it was a novel because it said was a novel right on the cover so you could know it was a novel and not another kind of book and it was full of words some that were old and some that were new  old words that people had seen and new words that people had not seen so the new words were novel and it was novel and it was a novel."
On Tue Oct 27 2015 16:51:11, ojahnn commented: "Ha, yes. Something like that. Someone on the #botALLY slack had a much easier idea: 
![roseisa](https://cloud.githubusercontent.com/assets/8790676/10772143/cbff3484-7cf4-11e5-8229-a6ae33a9e89c.png)
"
On Tue Oct 27 2015 16:55:51, MichaelPaulukonis commented: "I am, despite some recent rants that might imply the contrary, still interested in templates and templating.

A still-born portion of the proppian generator last year was a conversation -- I had hoped to expand it, but never got around to it. Perhaps that.

 - http://forum.makega.me/t/procedural-narrative-dialogue/91
 - http://www.blog.radiator.debacle.us/2014/04/second-times-charm-procedural-npc.html
 - http://koobazaur.com/gamedev/game-design/writing-branching-game-conversations/



I'm also still interested in Fairy Tales.
In particular, a portion of some tales where the hero befriends several characters/creatures [despite (his) haste or advice] which then end up helping him get through a nested problem. Eg, the giant's heart is kept in a box at the top of a tower on an island in a lake past the thorns, past a guard-dragon, etc etc. I'd like to be able to generate a n-level deep problem with associated helper characters -- each of whom would have to have an attribute matched to solving the problem (bear kills dragon, eagle flies hero over lake, etc.). This would also involve some minimal conversations.

Not exactly _fascinating_ textually when strung out to 50K words, but I'm curious to see how a deeply nested problem would work -- say, 300 levels with unique creates, and problems to overcome.
"
On Tue Oct 27 2015 17:14:35, dariusk commented: "Someone (I believe @jkirchartz) posted this link to [an actual spam blog comment generator template](http://alexking.org/blog/2013/12/22/spam-comment-generator-script) that was accidentally posted as a comment on a guy's blog."
On Tue Oct 27 2015 17:36:20, enkiv2 commented: "There is a generator for hilariously poorly written sex scenes
<http://www.fiftyshadesgenerator.com/>. The source is embedded in the page
there.

On Tue, Oct 27, 2015 at 4:14 PM Darius Kazemi <notifications@github.com>
wrote:

> Someone (I believe @jkirchartz <https://github.com/jkirchartz>) posted
> this link to an actual spam blog comment generator template
> <http://alexking.org/blog/2013/12/22/spam-comment-generator-script> that
> was accidentally posted as a comment on a guy's blog.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/1#issuecomment-151646047>
> .
>
"
On Tue Oct 27 2015 19:01:50, MaxBittker commented: "Update: Some corpus’s I’m considering are:
"gamer-tag" lists from early 2000's
wiktionary
wikiquotes
building storifies from random comment chains on twitter (is this invasive?)"
On Tue Oct 27 2015 22:41:39, MichaelPaulukonis commented: "I've found that feeding very repetitive simple sentences with a markov generator gives very Steinian output. 

http://www.xradiograph.com/netart/017.html?content=test&ngram=5 [it's character based, so ngram=2 is... not English; initial page-load is g-dawful, since I shoved 2 gutenberg texts in there.)

Which uses the input of  `as an apple is to a beta, this bridge is over the any hill. who says? she says! so says the soothsayer rapunzel. As Brad the Bard and Ken knew, the can-can can not know how to do it like an apple, like a bridge, like a beast of burden with a heavy load. And so what? The bald bearded bard sings a braided tale of happiness, of woe, of bitter embargoed apples on a barge passing under a bridegroom's bridge by a tepid, vapid moon. His beer is here, and all ale is well, hale, and hearty. This is not my bridge, it is your bridge, your toll bridge, your tool for trolls and travellers. I see you singing, Bradley Bard, I hear you. Your embalmer blames the bridge, his badge is barely adequate for the aqueduct, he ducks his dock responsibilities badly, baldly. Who is the third that walks beside you? How is he known, and how can he see in the dark (if, in fact, he can.) All what? All right, that is okay, he said.`

I... I'm not quite sure how I wrote that in the first place.

The code for that is [here](https://github.com/MichaelPaulukonis/WebText/blob/master/javascript/pages/017.js), although the markov engine is elsewhere."
On Tue Oct 27 2015 22:51:33, MichaelPaulukonis commented: "[Repo](https://github.com/MichaelPaulukonis/NaNoGenMo2015), man."
On Wed Oct 28 2015 06:33:51, cpressey commented: "While there will certainly be similarities, my third goal is to not just end up re-writing The Swallows.  I was looking through that code yesterday, seeing how much of it could be re-used.  Very little, I think.

My background is programming languages, so I have a hard time *not* seeing a story generator as a kind of compiler.

A typical compiler is structured as a pipeline with a number of phases.  The process for writing a story is much messier, but in a broad sense it too is a "pipeline", from idea to outline to draft to finished work.

In fact a story-writing pipeline is in some ways the inverse of a compiler pipeline.

A compiler takes a readable text and turns it into an incoherent blob.  A writer takes an incoherent blob and turns it into a readable text.

One of the first things a compiler often does is strip comments from the source code and throw them away, because they're not crucial to the result.  One of the last things a writer might do is add commentary that's not crucial to the story.

One of the last things a compiler does is optimize the generated code to make it shorter and more efficient.  One of the first things a writer might do is complicate the plot to make it longer and more interesting.

Somewhere in the middle of the compiler, it might check that the program does not contain certain errors, like assigning a string value to an integer variable.  Somewhere in the middle of writing a story, a writer might check that the characters are not doing something that, in that scene, would not be possible.

And so forth.  The similarities really are rather remarkable.
"
On Wed Oct 28 2015 08:28:31, enkiv2 commented: "Of course, since most of these operations are generative, if a single pass
fluffs out a summary and checks continuity, you could just run the same
pass over and over on an arbitrarily small summary until you had 50k words
;-)

On Wed, Oct 28, 2015 at 6:33 AM Chris Pressey <notifications@github.com>
wrote:

> While there will certainly be similarities, my third goal is to not just
> end up re-writing The Swallows. I was looking through that code yesterday,
> seeing how much of it could be re-used. Very little, I think.
>
> My background is programming languages, so I have a hard time *not*
> seeing a story generator as a kind of compiler.
>
> A typical compiler is structured as a pipeline with a number of phases.
> The process for writing a story is much messier, but in a broad sense it
> too is a "pipeline", from idea to outline to draft to finished work.
>
> In fact a story-writing pipeline is in some ways the inverse of a compiler
> pipeline.
>
> A compiler takes a readable text and turns it into an incoherent blob. A
> writer takes an incoherent blob and turns it into a readable text.
>
> One of the first things a compiler often does is strip comments from the
> source code and throw them away, because they're not crucial to the result.
> One of the last things a writer might do is add commentary that's not
> crucial to the story.
>
> One of the last things a compiler does is optimize the generated code to
> make it shorter and more efficient. One of the first things a writer might
> do is complicate the plot to make it longer and more interesting.
>
> Somewhere in the middle of the compiler, it might check that the program
> does not contain certain errors, like assigning a string value to an
> integer variable. Somewhere in the middle of writing a story, a writer
> might check that the characters are not doing something that, in that
> scene, would not be possible.
>
> And so forth. The similarities really are rather remarkable.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/11#issuecomment-151795310>
> .
>
"
On Wed Oct 28 2015 09:25:59, malantonio opened a new issue called "surrealist pulp mystery novel?".
On Wed Oct 28 2015 09:27:21, MichaelPaulukonis commented: "That's weird.

I note they had one take-down request that was [not honored](http://www.google.com/transparencyreport/removals/copyright/requests/1447505/)."
On Wed Oct 28 2015 09:41:43, amarriner opened a new issue called "Intent!".
On Wed Oct 28 2015 10:37:40, cpressey commented: "Sure, except (continuing with the compiler analogy) most compilers aren't designed to take as input that which they generate as output.

I certainly wasn't planning on building anything that could _read_ a novel!"
On Wed Oct 28 2015 11:49:45, hangedmandesign opened a new issue called "Recounting the Long Road to the Dark North".
On Wed Oct 28 2015 12:04:03, ikarth commented: "# Postmortem of Previous Projects
For this, the third NaNoGenMo, I've been trying to brainstorm an approach builds on some of the things I was aiming for in my two previous attempts.

[Gutenberg Shuffle](https://github.com/dariusk/NaNoGenMo/issues/63), and the related results from 2013, were based on the idea that stealing sentence-level content from existing texts would be more cohesive. Feeding those sentences into a generator and replacing names with a list of proper nouns culled from an analysis of the complete text would theoretically result in a novel about a recurring cast of characters who could take any action that was described somewhere in the source text.

I had some dreams about adding some context to the individual sentences, either through a hand annotation process, or some kind of automated analysis, so that the characters would have some idea of which actions belonged together. This didn't happen. Still an interesting idea, but very ambitious.

I also wanted to add the ability to draw dialogue from the source text, but I ran out of month before I could successfully implement the analysis to scrape the dialogue in the format I needed it to be in.

I do like the idea of titling the chapters based on analysis of the contents. In this case it was just the longest word in the chapter, but I think the general principle of layering generators on top of each other is a potentially powerful approach. While it's hard to extract meaningful context from arbitrary text, it's much easier to extract something relevant from text that you've generated, particularly if you still have the metadata.

English gender and tense are also kind of annoying when you're trying to parse arbitrary text. Having a great really good library to deal with that, and other similar fiddly bits can probably go a long way into speeding experimentation.

[For 2014, I switched approaches.](https://github.com/dariusk/NaNoGenMo-2014/issues/35) The limitation of generating data based on arbitrary source text was that it lacked any kind of larger structure. I decided that a recursive series of nested generators, patterned after the manner of the 1001 Arabian Nights, would be a good starting point.

The primary generator borrowed some concepts from 2013, such as the characters who had concepts of outputting sentences that represent actions that they took, but in a more hand-scripted way. (I had ideas of training them to use arbitrary input sentences again, but once again discovered that that took a lot of time.) 

The basic design was a modular set of text generators, any of which could feed into each other. Most of the month was taken up with the framework for the different generators. So by the end only three generators were actually implemented: the storytelling generator, the Library of Babel generator, and the exploring a maze generator. To really pull off the concept, I think the planned but not implemented modules that told more detailed stories or parts of stories would've gone a long way to getting more interesting results.

One bit about the maze generator that I like is that the maze that they are exploring is the actual data structure of the story that they're inhabiting. Each node in the data structure is a room in the maze. The rooms and decoration of the labyrinth use the properties of the current node in the data structure as a seed for the random template, so exploring the same data structure will result in the same maze. I like the way that it gives an additional layer of meaning, though I'm afraid this is mostly obscured by the labyrinth being somewhat repetitive and without a lot of action going on other than exploring and reading randomly generated books. 

I'm tempted to take the basic framework and start implementing a bunch of new and improved modules that take the project in a different direction, but there's been a lot of tools released in the past year or so in other languages that makes some of the basic text fiddling or analysis tasks easier so I'm also tempted to switch to something like Python."
On Wed Oct 28 2015 14:34:39, bcj commented: "Having not participated in NaNoGenMo before, I found this very useful/interesting. Thanks for this"
On Wed Oct 28 2015 15:25:57, cpressey commented: "(Excuse my "designblogging" but it helps give me something to do to stop myself from jumping the gun and starting too early!  Am chomping at the bit, can you tell?  Trying to keep each post reasonably short.)

If the "novel compiler" doesn't take a written text as input, I suppose that raises the question of what it _does_ take as input.

One answer could be "nothing, it's just a generator, you just run it," which might be literally true, but it doesn't really answer the question.

A more satisfying answer would be that it takes an outline of a plot, in some kind of data format, as input, even if that outline is hardcoded or randomly generated in the compiler itself.

It then refines that plot by iteratively rewriting it, stepwise, into increasingly more detailed plots.  Once it has a detailed enough plot, it rewrites that into a series of events, and in the end rewrites those into sentences.  I suppose this is a top-down, plot-driven approach, as @TheCommieDuck described it.

About these plots... (kind of thinking out loud here...)

The "seed plot" that the compiler starts with could be as skeletal as [The Hero's Journey](http://tvtropes.org/pmwiki/pmwiki.php/Main/TheHerosJourney).

Or maybe even more basic, like, the "null story":

> Once upon a time, they lived happily ever after. The end.

From there, you just keep inserting subplots into it.  I'm still weighing ideas about exactly how to accomplish this process.  I might write more about it later."
On Wed Oct 28 2015 16:18:43, tra38 opened a new issue called "The Atheists Who Believe In God". And it's been completed. Sweet!
On Wed Oct 28 2015 16:19:28, dariusk commented: "Cool concept!!"
On Wed Oct 28 2015 16:38:25, BrianHicks opened a new issue called "An Anthology of Fake Speeches".
On Wed Oct 28 2015 19:08:33, ikarth commented: "Those are all good ideas. I'd guess that how easy they are depends on which aspect of text generation you want to dive into. I'd rate them as going from hardest to easiest, though that's partially colored by my biases:

* Making a book of speeches is probably the easiest, depending on how you go about it. A basic Markov chain randomization and some interesting source text is the most straightforward way, but there are an infinite number of ways you can approach this. Lots of unexplored ideas for getting it to make sense.
* Medium difficulty is the poems. There are libraries that can help with detecting meter and rhyme, of various levels of accuracy; check the resource threads for a few of them. Or you can write your own if that's the part that interests you, since it's not a perfectly solved problem.
* I'm personally biased towards the simulation approach at the moment...mostly because I haven't successfully gotten it to work yet. So that's probably the hardest, though it likely varies based on the quality of the prose you're aiming for. Also, this is probably going to be the approach that's most concerned with the larger structure of the text and least concerned about the fiddly linguistic details. Though again, that'll very based on how you design it.

Either way, looking forward to seeing what you come up with."
On Wed Oct 28 2015 19:46:52, saluk opened a new issue called "Three people walk into a bar".
On Wed Oct 28 2015 22:15:56, ikarth commented: "One thing I was playing with in past projects was embedding metadata about the generation in the outputted text, and then performing a last cleanup phase before the actual final output. So there would be a bunch of bracketed tags scattered around marking things that could potentially be expanded. And the last step stripped the bracketed text out or reduced it to its default.

I never fully implemented the idea, but it might be useful for your novel compiler."
On Thu Oct 29 2015 08:22:47, mewo2 commented: "One solution to the problem of passes being able to read their own output would be to take a leaf from LLVM and have a single intermediate representation (e.g. a list of events), which most passes use for both input and output. You can munge this repeatedly until your novel is complex enough, then run a single final pass which converts to prose."
On Thu Oct 29 2015 08:43:55, tra38 commented: "On GenText, I hand-wrote some examples of "fixed" passages that can be randomly assembled and then  linked together by transitions. The end result is that you can get different plots, which make sense because of the transitions. Recently, I got time to convert my handwritten approach into an algorithm (which you can see [here](https://github.com/tra38/Architect/blob/master/state_machine.rb)). I am doubtful if this approach can scale up to 50,000 words, but I am likely going to use this approach for the Dartmouth Turing Test for the Creative Arts.

My idea of organizing plot by random reordering of passages came from a 1990's computer program: Dramatica. Dramatica is a computer program that is able to generate out a fairy complex outline for a "grand argument story", though it does expect you to use that outline to handwrite out the story. Each outline has 4 subplots, and each subplot represents a certain "theme" (let's say Manipulation). Each subplot is composed of four different "acts", each representing a certain aspect of 'Manipulation''. However, it is the ***order*** of these acts that matter in determining the effect of the subplot on the greater plot. Obviously, Dramatica is a far more complex program than just randomly shuffling events within an array and then popping them out. There are additional rules involved to ensure that the "grand argument story" is logical and has no holes that a reader can use to attack its point. But most of these rules have nothing to do with the organization of the plot structure, and everything to do with how it is presented within the text (characterization, etc.)."
On Thu Oct 29 2015 09:10:09, benblankley opened a new issue called "Newbie ready to try this out". And it's been completed. Sweet!
On Thu Oct 29 2015 09:20:10, cpressey commented: "@ikarth My understanding is that this (embedding structured data inside unstructured text) was one of the original use cases for XML, though it's probably under-used these days.

I don't currently see foresee myself having a huge need for this, but if it becomes desirable, I'll keep that idea in mind, thanks.

@mewo2 I'm currently thinking of the individual passes as purely internal rewriting operations on whatever data structures happen to be convenient at that point in the pipeline.  But if the whole novel-model becomes too much to hold in memory, I suppose I will have to think about reading and writing intermediate representations, yeah."
On Thu Oct 29 2015 09:31:56, TheCommieDuck commented: "After spending a while awake in bed thinking, I've decided on two driving points:
a) Given how much work people have put into narrative generation and yet even the best academic systems aren't there yet, trying to do something of that scale in a month is silly.
b) I'm a borderline perfectionist and wouldn't be overly happy with something 'meh' just because I tried doing far too much.

As such, I'm still going to be trying to do far too much! But...differently!

A rough overview of my plans - for now:
1- Get a very rough propp-based plot. Translate the plots into goals (e.g. the 'return' Propp function is a goal to end up at location X).
2- Throw in a whole bunch of characters, locations, objects, etc. I'll see if a corpus exists, otherwise I'll probably manually find some from e.g. IF games.
3- Give each character a small number of emotions and beliefs - e.g. hunger, safety, greed, whatever. This is the novel bit, I think. it's also the bit most likely to completely fall flat.
4- Let the characters run amok ala The Swallows (which I must say is absolutely brilliant and has given me and my friends a sore throat from laughing). 
5- Generate the story - which, I am hoping, might end up looking a little like something from dwarf fortress. Everyone loves a dwarf grabbing a goblin by the tongue with their pinky, ripping it out, and beating them to death with it!

1) shouldn't be too hard - the resource list in @MichaelPaulukonis' NaNoGenMo2014 is absolutely staggering. Originally I was talking about using Plotto for a plot, but that's just going to lead to extreme templatisation - something I don't really want at all.
2) also shouldn't be bad. Worst case? Grab an ontology or similar and go to town. Could be downright hilarious if, for instance, the hero really needs to go collect the magical Acetaldehyde Ethyl Phenylethyl Acetal by riding in his Alfa Romeo 155. Which goes about 5mph.
3) This is where it all falls down, I imagine. I've been interested in this kindof model but never really looked into it. We'll have to see!
4) Shouldn't be too hard. Just got to make sure I add some kind of urgency so we don't get them running around in circles for *too* long.
5) Also not too bad.

Anyway, we'll see."
On Thu Oct 29 2015 09:49:55, MichaelPaulukonis commented: "I note that many of the classic Narrative generators generated their world + stories, and had another independent system that "translated" them into more natural language. For example, TALE-SPIN:

![2015-10-29 09_47_59-inside computer understanding_ five programs plus miniatures - r c schank c](https://cloud.githubusercontent.com/assets/2607898/10820163/3f06dea4-7e22-11e5-9c3a-f300260fa453.png)
([source](https://books.google.com/books?id=fdDGBQAAQBAJ&lpg=PT272&ots=YZ1w-skWxN&dq=%22talespin%22%20-disney%20lisp&pg=PT267#v=onepage&q&f=false))

It's a lot more complicated than this, but I can't find back an example/citation right now. Multiple sentences about the current world-state would be combined (JOE WAS IN THE CAVE. JOE KNEW HE WAS IN THE CAVE. THE CAVE WAS DARK. THE CAVE HAD AN EXIT. JOE KNEW THE CAVE WAS DARK. JOE WANTED TO BE IN THE LIGHT.  JOE KNEW THE CAVE HAD AN EXIT. => Joe wanted to get out of the cave and into the light.)"
On Thu Oct 29 2015 09:55:08, MichaelPaulukonis commented: "Flattered!

I think you're working in a reasonable direction for a month-long project. Seeing the bodies of academic work on small-scale text generation can be humbling. Especially when I don't understand the bulk of their work. But I love the angle of attack that NaNoGenMo brings to the problem - sideways, cargo-cult, worst-practices surface-FX that leap-frog over academic problems to get to a semi-readable end-point. _Eliza_ is dumb from an AI standpoint, but continues to be an important touchstone, showing how simple effects can be used (believed, read, whatever) by a non-academic user/reader.

And am a bit jealous, because I have no clear goal yet for this month..... I think scaling back to a single type of story might be interesting (_The Giant Who Had No Heart in His Body_ or _Koschei the Deathless_). Not necessarily the output, but I might find it interesting to work on."
On Thu Oct 29 2015 09:55:29, MichaelPaulukonis commented: "Welcome aboard!"
On Thu Oct 29 2015 10:01:24, MichaelPaulukonis commented: "> I am doubtful if this approach can scale up to 50,000 words

A collection of short stories, with optional inter-connecting material is one solution. There were a bunch of small-pieces-collected-at-length submitted last year (mine amongst them). I had considered a framing device of a reader reading the book in a mansion's library as intro and outro, with minor sentences between stories, but never got around to it. There's a good tradition of this sort of thing. _Canterbury Tales_, _The Decameron_, etc. "
On Thu Oct 29 2015 10:05:23, TheCommieDuck commented: "I'm also a massive contender for an award titled 'person who has never understood the idea of scope'; I seem to always end up with 'I should do X, maybe Y...oh look, now it is a 30 year project'.

Just really hoping I'm able to catch myself in time *if* this turns out to be overly ambitious. I'm imagining scale back approaches:
- have a single plot if I can't goal-ise Propp
- hardcode characters, objects, locations, etc
- I really dunno if I can scale back the next 2. I think it's the key point I really want to focus on for this. I'd imagine that if it is the scale-back point, it can be taken down to maybe a couple of easy parameters.
- Obviously text generation can be replaced with predicates (man move room, rather than sentences describing it).
"
On Thu Oct 29 2015 10:53:44, MichaelPaulukonis commented: "I really wish I had worked harder on characters, objects, locations, etc. in the early stages of my propp-gen. That would have made things a lot easier. With a landscape/world movement can be done - and descriptive expansion is "easier" to tack on once that exists."
On Thu Oct 29 2015 11:49:24, marythought opened a new issue called ""Where I'm From" poem & novel generator".
On Thu Oct 29 2015 11:52:21, dariusk commented: "he's such a card"
On Thu Oct 29 2015 12:20:35, hugovk commented: "You can download CDs and DVDs of Project Gutenberg books here:
https://www.gutenberg.org/wiki/Gutenberg:The_CD_and_DVD_Project

I didn't know there is a Google Books API, I'll have to check it."
On Thu Oct 29 2015 13:00:46, ikarth commented: "Given some of the [recent discussion](https://github.com/dariusk/NaNoGenMo-2015/issues/11#issuecomment-152162467) of template expansion, I should note that part of the plan in 2014 was to generate the story as a Lisp data structure (In this case, Clojure vectors containing strings and maps) and then expand individual elements as needed. In practice, this turned out to be more complex to implement than it sounded. I still like the idea, I just think that it probably needs to be implemented first, before I try to build the rest of the system.

The other downside of working with Clojure was that the native text fiddling tools aren't as developed as Python's and I didn't have the time to explore Java's ecosystem thoroughly enough to make up for it."
On Thu Oct 29 2015 13:01:48, Srol opened a new issue called "I'll try".
On Thu Oct 29 2015 13:14:54, enkiv2 commented: "The low end of the effort spectrum for NaNoGenMo is much lower than that of
NaNoWriMo, and the high end much higher. I fully expect to see five or six
completely novels by 2AM November 1st, as people put something quick and
dirty together to get completing an entry out of the way, so that they can
more seriously persue their more interesting ideas without worrying too
much about them ever even producing text.

On Thu, Oct 29, 2015 at 1:01 PM Patrick Hogan <notifications@github.com>
wrote:

> But if my participation goes as well as when I did the real NaNoWriMo,
> expect me not to finish.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/50>.
>
"
On Thu Oct 29 2015 15:13:27, bredfern opened a new issue called "1940s la horror noire".
On Thu Oct 29 2015 15:18:37, enkiv2 commented: "Looking forward to it!

On Thu, Oct 29, 2015 at 3:13 PM Brian Redfern <notifications@github.com>
wrote:

> Well that's the idea but likely going to be very naked lunch esque, an
> excuse to attempt something really interesting with node + react
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/51>.
>
"
On Thu Oct 29 2015 17:43:13, allentran opened a new issue called "Longtime generator of junk text, first time participant".
On Thu Oct 29 2015 20:30:58, aferriss opened a new issue called "speech to text --> text to speech loop".
On Thu Oct 29 2015 22:59:00, tra38 commented: "Created a [repo](https://github.com/tra38/The-Atheists-Who-Believe-In-God) with some licensing information (Pew Survey is very protective of its data, and I can't blame them).

Pew Forum hired a polling firm to call 35,556 people in the continental USA, meaning that I happen to have a very rich dataset. I manage to extract out 99 atheists who believe in God and save them into my own personal *.CSV file. I need 505 words for each atheist to produce a 50,000-word novel."
On Fri Oct 30 2015 00:30:32, hugovk commented: "Interesting, a bit like Alvin Lucier's I Am Sitting in a Room."
On Fri Oct 30 2015 07:03:03, cpressey opened a new issue called "Spin-offs and related activities (admin issue)". But it's an admin issue, so who cares?
On Fri Oct 30 2015 07:04:19, cpressey commented: "[PROCJAM](http://www.procjam.com/) 2015 starts on November 7 2015 and lasts for 9 days.
"
On Fri Oct 30 2015 07:20:32, cpressey commented: "I've just created a repo for [NaOpGenMo 2015](https://github.com/cpressey/NaOpGenMo-2015).  I don't know how much time I'll have to contribute or administrate it, but if, after generating your novel, you think it would make a good libretto... or if you just want to generate some music instead of some text... feel free to open an issue on it."
On Fri Oct 30 2015 07:53:03, cpressey commented: "I like the speeches idea, probably because speeches are heavy on rhetoric.  It would be interesting to see how much (if any) of that "oratory voice" remains after combining multiple speeches (even ones on vastly different topics) in some way."
On Fri Oct 30 2015 08:01:05, cpressey commented: "I remember thinking when I read Kerouac's _On the road_ last year that there were fairly long stretches in it that seemed quite amenable to generation.

"I got off the truck in Foosberg and I had 80 cents in my pocket.  I spent 5 cents on coffee and 15 cents on a slice of pie and fell asleep in a field.  The next day, I hitched a ride with a fellow in a blue Buick.  In the back was the beatest dog I ever laid eyes on.  He could only take me as far as Barsville..."
"
On Fri Oct 30 2015 08:23:19, MichaelPaulukonis commented: "See sample code in various languages via the  Language Survey, issue #17 "
On Fri Oct 30 2015 09:15:49, enkiv2 commented: "Do all november generative music experiments go under NaOpGenMo even if
they are not truly operatic (i.e., if they omit lyrics)? Or, should
generative music be it's own thing?

Some of the most interesting things in NaNoGenMo2014 were graphic novels,
which of course could not meet the 50k word criterion. A NaCoGenMo with a
150 page qualification (i.e., approximately what a volume of manga from
Tokyopop would contain, or a trade paperback of a DC or Marvel comic
collecting part of a normal run rather than a shorter miniseries) might be
interesting in of itself.

If you or Mike don't have a particular interest in hosting these, I can.

On Fri, Oct 30, 2015 at 7:20 AM Chris Pressey <notifications@github.com>
wrote:

> I've just created a repo for NaOpGenMo 2015
> <https://github.com/cpressey/NaOpGenMo-2015>. I don't know how much time
> I'll have to contribute or administrate it, but if, after generating your
> novel, you think it would make a good libretto... or if you just want to
> generate some music instead of some text... feel free to open an issue on
> it.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/54#issuecomment-152499226>
> .
>
"
On Fri Oct 30 2015 09:19:59, enkiv2 commented: "I guess blog posts count as press coverage? I wrote this a while back: https://medium.com/@enkiv2/the-hidden-benefits-of-nanogenmo-dd91193bda6#.d3cy9qbwr"
On Fri Oct 30 2015 09:20:35, muffinista commented: "Yeah I actually tried that in 2013 but ran out of time I think, and I know there's at least one generative-style OTR out in the wild. I think this will be a little different, I'll try and add details here before I actually write any code hehe."
On Fri Oct 30 2015 09:26:04, enkiv2 commented: "There have been prior examples of trying to generate speeches that have
been fairly successful. I specifically recall someone using char-rnn with
all of Obama's speeches, and again with TED talks; the TED talks ended up
being less coherent in terms of topic but otherwise *very* stylisitically
similar to normal TED talks, and comparable in the amount of content to the
low end of TED talks:
https://medium.com/@samim/ted-rnn-machine-generated-ted-talks-3dd682b894c0

This indicates that, with a larger corpus (say, the set of all state of the
union addresses -- which are in the public domain & available from several
sources), you could probably produce significantly more complex &
interesting speeches.

On Fri, Oct 30, 2015 at 7:53 AM Chris Pressey <notifications@github.com>
wrote:

> I like the speeches idea, probably because speeches are heavy on rhetoric.
> It would be interesting to how much (if any) of that "oratory voice"
> remains after combining multiple speeches (even ones on vastly different
> topics) in some way.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/46#issuecomment-152505133>
> .
>
"
On Fri Oct 30 2015 09:43:02, whitten commented: "Thank you for the Interesting blog article. You referenced something called a "cut-up" what is it?"
On Fri Oct 30 2015 09:57:14, ikarth commented: "Not to mention you can throw in, say, [Mark Twain's speeches](http://www.gutenberg.org/files/3188/3188-h/3188-h.htm) if you're looking for a less serious result.

There is, on the other hand, a certain purity to the idea of the Ur-State-of-the-Union speech."
On Fri Oct 30 2015 10:00:32, JKirchartz opened a new issue called "Throwing in my hat".
On Fri Oct 30 2015 10:02:41, ikarth commented: "Slightly off-topic for this thread, but the [cut-up technique](https://en.wikipedia.org/wiki/Cut-up_technique) is a Dadaist method of generating text. There's a number of past artistic movements--including Dada, Surrealism, and OuLiPo--that are relevant to text generation.

On topic, if blog posts count, I've been [covering past entries on my procedural generation blog for a while](http://procedural-generation.tumblr.com/tagged/nanogenmo)."
On Fri Oct 30 2015 10:05:31, enkiv2 commented: "Information on cut-up techniques might actually be on-topic for the
resources thread. While they've been around for a while (even in the
context of generative text), a lot of people are coming at this without a
strong background in the history of experimental and semi-mechanical text
generation (along with the movements that produced some of the forms of
constrained fiction that a lot of generative text experiments come out of).

On Fri, Oct 30, 2015 at 9:43 AM David Whitten <notifications@github.com>
wrote:

> Thank you for the Interesting blog article. You referenced something
> called a "cut-up" what is it?
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/9#issuecomment-152529267>
> .
>
"
On Fri Oct 30 2015 10:07:12, cpressey commented: "@enkiv2 That's kind of like asking if generative text experiments go under NaNoGenMo even if they are not truly in the form of a novel.

From NaOpGenMo-2015's README (unchanged from last year):

> The "opera" is defined however you want.  It could be 50,000 beats of middle C played on a clarinet (or, if you are the sort of person for whom an opera must include singing, 50,000 chants of "Meow!" by a tenor.)  It could literally grab a random opera from <s>Project Brandenburg</s> The Internet Archive.  It doesn't matter, as long as it's at least two and a half hours long.
"
On Fri Oct 30 2015 10:09:47, enkiv2 commented: "OK, so all generative music experiments can go under NaOpGenMo

On Fri, Oct 30, 2015 at 10:07 AM Chris Pressey <notifications@github.com>
wrote:

> @enkiv2 <https://github.com/enkiv2> That's kind of like asking if
> generative text experiments go under NaNoGenMo even if they are not truly
> in the form of a novel.
>
> From NaOpGenMo-2015's README (unchanged from last year):
>
> The "opera" is defined however you want. It could be 50,000 beats of
> middle C played on a clarinet (or, if you are the sort of person for whom
> an opera must include singing, 50,000 chants of "Meow!" by a tenor.) It
> could literally grab a random opera from Project Brandenburg The Internet
> Archive. It doesn't matter, as long as it's at least two and a half hours
> long.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/54#issuecomment-152535935>
> .
>
"
On Fri Oct 30 2015 11:01:05, ikarth commented: "@enkiv2 suggested that a short introduction to some historical techniques might be useful in the resources thread to point out possible approaches for people who might not be familiar with some of the precedents of text generation. So I thought I'd write about a few of them.

[Dada](https://en.wikipedia.org/wiki/Dada) incorporated several techniques that involved randomly assembling prior texts into new poetry. [The cut-up technique](https://en.wikipedia.org/wiki/Cut-up_technique) takes an existing text, cuts it to pieces, and then reassembles the bits in a new order. [This list of Surrealist techniques](https://en.wikipedia.org/wiki/Surrealist_techniques) might also be inspirational.

[Oulipo](https://en.wikipedia.org/wiki/Oulipo) is a group of writers who use [constraints](https://en.wikipedia.org/wiki/Constrained_writing) to define their writing. Relevant examples include [Queneau's _Cent Mille Milliards de Poèmes_ (A Hundred Thousand Billion Poems)](https://en.wikipedia.org/wiki/Hundred_Thousand_Billion_Poems), which combines ten-line sonnets line-by-line; [Perec's _Life a User's Manual_](https://en.wikipedia.org/wiki/Life_a_User%27s_Manual), which is structured like a knight's tour of a chessboard; [Queneau's _Excercises in Style_](https://en.wikipedia.org/wiki/Exercises_in_Style), 99 retellings of the same story; [Calvino's _The Castle of Crossed Destinies_](https://en.wikipedia.org/wiki/The_Castle_of_Crossed_Destinies), stories interperted via Tarot cards; and [Queneau's "A Story as You Like It"](http://www.thing.de/projekte/7:9%23/queneau_1.html).

Even where they aren't directly generative, the approaches used by Oulipo writers often point to alternative forms a novel can take. For example, Calvino's [_Invisible Cities_](https://en.wikipedia.org/wiki/Invisible_Cities) and [_If on a winter's night a traveler_](https://en.wikipedia.org/wiki/If_on_a_winter%27s_night_a_traveler).

Oulipo is also significant because many of its members wrote about systems for generating stories and about incorporating computers into writing. Italio Calvino's essay "Prose and Anticombinatorics," about using the computer to to find the constraints for a murder mystery; Paul Fournel's "Computer and Writer: The Centre Pompidou Experiment"; and Claude Berge's "For a Potential Analysis of Combinatory Literature" are particularly relevant.

Lastly, Jorges Luis Borges tends to crop up a lot, particularly for his short stories: "The Library of Babel", "The Book of Sand", "An Examination of the Work of Herbert Quain", "The Garden of Forking Paths", "Pierre Menard, Author of the Quixote", and "Tlön, Uqbar, Orbis Tertius"."
On Fri Oct 30 2015 11:09:30, enkiv2 commented: "Here's a spinoff for comics / image-related generated art with multiple
pages: https://github.com/enkiv2/NaCoGenMo

The rules are the same as NaNoGenMo, with the 50k word requirement replaced
with a 150 page requirement.

On Fri, Oct 30, 2015 at 10:09 AM John Ohno <john.ohno@gmail.com> wrote:

> OK, so all generative music experiments can go under NaOpGenMo
>
> On Fri, Oct 30, 2015 at 10:07 AM Chris Pressey <notifications@github.com>
> wrote:
>
>> @enkiv2 <https://github.com/enkiv2> That's kind of like asking if
>> generative text experiments go under NaNoGenMo even if they are not truly
>> in the form of a novel.
>>
>> From NaOpGenMo-2015's README (unchanged from last year):
>>
>> The "opera" is defined however you want. It could be 50,000 beats of
>> middle C played on a clarinet (or, if you are the sort of person for whom
>> an opera must include singing, 50,000 chants of "Meow!" by a tenor.) It
>> could literally grab a random opera from Project Brandenburg The
>> Internet Archive. It doesn't matter, as long as it's at least two and a
>> half hours long.
>>
>> —
>> Reply to this email directly or view it on GitHub
>> <https://github.com/dariusk/NaNoGenMo-2015/issues/54#issuecomment-152535935>
>> .
>>
>
"
On Fri Oct 30 2015 12:02:42, dariusk commented: "We got a mention in the Fast Forward Labs machine learning newsletter:
http://us8.campaign-archive2.com/?u=bdb368b9a389b010c19dbcd54&id=e924d57988
"
On Fri Oct 30 2015 12:50:00, bcj commented: "I think the Sherlock Shuffle sounds really interesting (and alliterative).

_Trisano_ by Nanni Balestrini might serve as a possible source of inspiration (though you may be aiming for something a little more readable). If I remember right, what Balestrini does is modify references to characters so that they are referred to with pronouns instead of nouns. Then paragraphs can be moved and have different meanings in new contexts. I guess you could even convert pronoun references back into nouns once you knew which character was being referenced in the new context"
On Fri Oct 30 2015 13:30:40, MartinPetkov opened a new issue called "Twitter Novel Generator, one day at a time!".
On Sat Oct 31 2015 00:36:37, maetl opened a new issue called "A Generative Journey".
On Sat Oct 31 2015 04:39:59, hugovk commented: "Sounds good.

Here's something that used Twitter in a different way last year.
https://github.com/dariusk/NaNoGenMo-2014/issues/88

Once you have a topic, this command line/Python tool may be useful for collecting tweets in bulk.
https://github.com/edsu/twarc"
On Sat Oct 31 2015 08:09:10, jamlamberti opened a new issue called "I'm in".
On Sat Oct 31 2015 08:10:08, jamlamberti commented: "Might also be interesting to do something with sentiments, so each paragraph or so has a general feel"
On Sat Oct 31 2015 08:26:24, rbechtel opened a new issue called "Mechanism: Extended Tale-Spin, Subject: Who knows?".
On Sat Oct 31 2015 09:49:35, ikarth commented: "Well, I'm definitely interested. I don't think that too many people have used prior storytelling AI research code in NaNoGenMo before. I think it's a very good idea. 

At the very least it'll be interesting to contrast the original Tale-Spin approach with how contemporary sensibilities lead you to develop your own spin on it. I feel like the increased literacy about cybertext has opened up new avenues for exploration, and I'm curious about what the results are going to be."
On Sat Oct 31 2015 11:01:07, TheCommieDuck commented: "Oh hey, I didn't realise source for tale-spin-esque things existed. That's really useful."
On Sat Oct 31 2015 11:19:44, tinyworlds opened a new issue called "I'm in!".
On Sat Oct 31 2015 11:20:42, TheCommieDuck commented: "I've gotten a copy of Propp and given it a semi-quick read through. It's odd that you can only seem to find the first 2 chapters online; the rest are fairly short, but really interesting. Anyway.

I've fairly decided that I'll be doing this in Prolog. I'm a bit rusty, but it's not exactly a complicated language. It should be especially handy for:
- constraint stuff when choosing what Proppian functions to pick and choose
- sanity checking events (a character can't start doing things if they're dead or whatever)
- picking between things.

If I do encounter serious speed problems, I'm competent enough with the SWI C++ interface to try and work that way. Ideally it won't be needed.

Anyway:

A lot of the Proppian...microfunctions? are rather specific. I might generalise these away; e.g. a4 is 'lack of the egg of death' from 1 tale; RsX seem to be equally specific (help from specific people, or doing specific things). 
- I'm undecided on how to format the functions. Obviously I said from the start I'd translate them into a series of goals/conditions, but not all of them lend themselves well to this.
- I can't seem to find anything explicitly about what can and can't be negated or contrarian; it makes sense that e.g. the donor's help isn't accepted, but not for it to happen with the Victory (the hero loses? it just..well, breaks the entire structure).

- Originally I thought multiple moves (story arcs) would be covered easily by the simulation section. However, now I'm less sure. Propp's identified 6 different ways for multiple moves to fit together; I might stick with just sequential ones/interrupting ones, rather than simultaneous plots.

- The micro-talespin source that @rbechtel linked in their issue is interesting. (http://eliterature.org/images/microtalespin.txt) It could be a good starting point for the simulation side. Things like promises could be very interesting to add in. 

I've also thought about how much extra fluff I could (or should) add. I think a story which consists of nothing but absolute key elements - hero, villain, donor, magical artifact, etc - would be dull. However, adding an entirely interactive world with dozens of items, characters, etc might get very clouded. I think I'll err on the side of clouding; otherwise I'm going to - if it even terminates - be very short on things to fit in 50k words."
On Sat Oct 31 2015 12:20:34, ikarth commented: "[After @rbechtel mentioned the source code for Micro-Talespin](https://github.com/dariusk/NaNoGenMo-2015/issues/59), I went poking around to see what other early AI storytelling systems had source code availible.

[Micro-Talespin in Common Lisp, by Warren Sack](http://lispm.de/source/misc/micro-talespin.lisp)
[another source](http://eliterature.org/images/microtalespin.txt) 
[About Talespin](http://lispm.de/mts https://grandtextauto.soe.ucsc.edu/2006/09/13/the-story-of-meehans-tale-spin/)

For [Tale-Spin itself](https://grandtextauto.soe.ucsc.edu/2006/09/13/the-story-of-meehans-tale-spin/): In 2008 Meehan found a copy of the original MLisp source code for Tale-Spin (sans the data files). There's an extensive discussion of the source code in Wardrip-Fruin's _Expressive Processing_, though I'm not aware of any copies online.

[Eliza/Doctor:](https://en.wikipedia.org/wiki/ELIZA)
[In Java](https://code.google.com/p/simple-semantic-desktop/source/browse/trunk/Progs2/Eliza/eliza.java?r=4)
[Also in Java](http://www.chayden.net/eliza/Eliza.html)
[BASIC](https://www.jesperjuul.net/eliza/ELIZA.BAS)
[JavaScript](https://www.jesperjuul.net/eliza/)
[Python](http://www.jezuk.co.uk/cgi-bin/view/software/eliza)
[The original Lisp source code](http://elizagen.org/)- [github repo](https://github.com/jeffshrager/elizagen)

[Skald](https://sites.google.com/a/soe.ucsc.edu/eis-skald/): a Scala reimplementation of Minstrel Remixed, which is itself a Scala reimplementation of Scott Turner's Minstrel (originally in Lisp). [Scott Turner on MINSTREL](https://grandtextauto.soe.ucsc.edu/2007/10/30/scott-turner-on-minstrel/).



"
On Sat Oct 31 2015 13:23:43, clarissalittler opened a new issue called "I'll try and do this".
On Sat Oct 31 2015 13:26:29, clarissalittler commented: "At the moment I'm thinking of generating something out of tumblr tags. We'll see how sophisticated it can get"
On Sat Oct 31 2015 13:30:41, enkiv2 commented: "As an example of exercises in minimalism and apophenia, Nick Monfort has a
collection of one kilobyte story generators in python.
<https://grandtextauto.soe.ucsc.edu/2008/11/30/three-1k-story-generators/>

On Sat, Oct 31, 2015 at 11:20 AM Isaac Karth <notifications@github.com>
wrote:

> After @rbechtel mentioned the source code for Micro-Talespin
> <https://github.com/dariusk/NaNoGenMo-2015/issues/59>, I went poking
> around to see what other early AI storytelling systems had source code
> availible.
>
> Micro-Talespin in Common Lisp, by Warren Sack
> <http://lispm.de/source/misc/micro-talespin.lisp>
> another source <http://eliterature.org/images/microtalespin.txt>
> About Talespin
> <http://lispm.de/mts%20https://grandtextauto.soe.ucsc.edu/2006/09/13/the-story-of-meehans-tale-spin/>
>
> For Tale-Spin itself
> <https://grandtextauto.soe.ucsc.edu/2006/09/13/the-story-of-meehans-tale-spin/>:
> In 2008 Meehan found a copy of the original MLisp source code for Tale-Spin
> (sans the data files). There's an extensive discussion of the source code
> in Wardrip-Fruin's *Expressive Processing*, though I'm not aware of any
> copies online.
>
> Eliza/Doctor: <https://en.wikipedia.org/wiki/ELIZA>
> In Java
> <https://code.google.com/p/simple-semantic-desktop/source/browse/trunk/Progs2/Eliza/eliza.java?r=4>
> Also in Java <http://www.chayden.net/eliza/Eliza.html>
> BASIC <https://www.jesperjuul.net/eliza/ELIZA.BAS>
> JavaScript <https://www.jesperjuul.net/eliza/>
> Python <http://www.jezuk.co.uk/cgi-bin/view/software/eliza>
> The original Lisp source code <http://elizagen.org/>- github repo
> <https://github.com/jeffshrager/elizagen>
>
> Skald <https://sites.google.com/a/soe.ucsc.edu/eis-skald/>: a Scala
> reimplementation of Minstrel Remixed, which is itself a Scala
> reimplementation of Scott Turner's Minstrel (originally in Lisp). Scott
> Turner on MINSTREL
> <https://grandtextauto.soe.ucsc.edu/2007/10/30/scott-turner-on-minstrel/>.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/1#issuecomment-152746654>
> .
>
"
On Sat Oct 31 2015 13:35:37, enkiv2 commented: "@michaelpaulukonis did something with Propp's motifs last year. You could
probably pull the interesting stuff from his code:
https://github.com/MichaelPaulukonis/malepropp

On Sat, Oct 31, 2015 at 10:20 AM Mark Garnett <notifications@github.com>
wrote:

> I've gotten a copy of Propp and given it a semi-quick read through. It's
> odd that you can only seem to find the first 2 chapters online; the rest
> are fairly short, but really interesting. Anyway.
>
> I've fairly decided that I'll be doing this in Prolog. I'm a bit rusty,
> but it's not exactly a complicated language. It should be especially handy
> for:
>
>    - constraint stuff when choosing what Proppian functions to pick and
>    choose
>    - sanity checking events (a character can't start doing things if
>    they're dead or whatever)
>    - picking between things.
>
> If I do encounter serious speed problems, I'm competent enough with the
> SWI C++ interface to try and work that way. Ideally it won't be needed.
>
> Anyway:
>
> A lot of the Proppian...microfunctions? are rather specific. I might
> generalise these away; e.g. a4 is 'lack of the egg of death' from 1 tale;
> RsX seem to be equally specific (help from specific people, or doing
> specific things).
>
>    -
>
>    I can't seem to find anything explicitly about what can and can't be
>    negated or contrarian; it makes sense that e.g. the donor's help isn't
>    accepted, but not for it to happen with the Victory (the hero loses? it
>    just..well, breaks the entire structure).
>    -
>
>    Originally I thought multiple moves (story arcs) would be covered
>    easily by the simulation section. However, now I'm less sure. Propp's
>    identified 6 different ways for multiple moves to fit together; I might
>    stick with just sequential ones/interrupting ones, rather than simultaneous
>    plots.
>    -
>
>    The micro-talespin source that @rbechtel <https://github.com/rbechtel>
>    linked in their issue is interesting. (
>    http://eliterature.org/images/microtalespin.txt) It could be a good
>    starting point for the simulation side. Things like promises could be very
>    interesting to add in.
>
> I've also thought about how much extra fluff I could (or should) add. I
> think a story which consists of nothing but absolute key elements - hero,
> villain, donor, magical artifact, etc - would be dull. However, adding an
> entirely interactive world with dozens of items, characters, etc might get
> very clouded. I think I'll err on the side of clouding; otherwise I'm going
> to - if it even terminates - be very short on things to fit in 50k words.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/22#issuecomment-152742197>
> .
>
"
On Sat Oct 31 2015 13:58:16, R-Gerard opened a new issue called "Let's do this!".
On Sat Oct 31 2015 14:10:21, TheCommieDuck commented: "Yep, he's done some amazing stuff. :)

He's got a lot of...template-based ideas, though; I'm really aiming to see how far this is possible without using templates, however. Obviously there's templates for a lot of it, but I'm seeing how little I can hardcode in and still get *something* out."
On Sat Oct 31 2015 18:21:52, javierarce opened a new issue called "Declaring my intent".
On Sat Oct 31 2015 18:21:54, tullyhansen opened a new issue called "Keyboard Maestro/OS X Predictive Text collab".
On Sat Oct 31 2015 19:38:01, tinyworlds commented: "So ... save entry for now using old code - 5000 short stories: https://raw.githubusercontent.com/tinyworlds/5000-Stories/master/5000_stories.txt

Might flesh it out later on, with more descriptions, details, etc. if I find the time."
On Sat Oct 31 2015 21:48:53, enkiv2 opened a new issue called "Orgasmotron -- 50k words of generated erotica". And it's been completed. Sweet!
On Sat Oct 31 2015 23:04:17, enkiv2 opened a new issue called "Expand-filter: a novel-length expansion of a sentence". And it's been completed. Sweet!
On Sat Oct 31 2015 23:27:21, MichaelPaulukonis commented: "@ikarth _et alia_ - this year I have been looking at **Tale-Spin** a bit, even though I don't think I will be using it as a model for what I will be doing. However, I do have some more resources in [my NMGM repo](https://github.com/MichaelPaulukonis/NaNoGenMo2015) and will be adding more there as the month progresses."
On Sun Nov 01 2015 00:10:38, brettimus opened a new issue called "Slack Channel Novel [Pending]".
On Sun Nov 01 2015 00:17:29, ikarth commented: "My prior commitments mean that I most likely won't get to write any code for at least a week, but at least I can spend part of that time brainstorming ideas.

**Sentiment analysis** — having some way to categorize arbitrary text is, I think, a highly useful way to add additional order and coherence to the generation. While sentiment analysis isn’t the only possible way to do this, it is a relatively well understood way to do it. I haven't had this in past projects, and I'd like to have it (or something similar) this year.

**Travel story** — this seems to be a kind of theme in people’s planning this year; invoking _On The Road_ or various travel tales. I like it as a concept. Partially because it invokes Italio Calvino’s _Invisible Cities_. But of course there’s a long tradition of travel stories, including Marco Polo, Ulysses, and so on. It’s an obvious way to structure a modular, episodic narrative and yet give it some kind of larger structure: the different cities or islands visited can each stand on their own, while the ongoing journey can give a sense of progress.

There are a number of ways I can think to approach this, including it using actual geographic data to structure a novel: for example a fictionalized rendition of the Marco Polo-esque journey along the Silk Road. For that matter there are number of historical roads or routes that could have a similar treatment: the Oregon Trail, the Orient express, a number of sea voyages along coasts, the Appalachian Trail, traversing the various rivers that make up the Amazon, and so on. Especially if you have a data source that allows you to create a graph of the nodes that will be visited during the story.

One possible source of data is ORBIS, the Stanford geospatial network model of the Roman world. I don’t know if they allow automated access to the data, rather than just the web interface to the model, but it would be a fascinating way to structure a historical journey through the Roman Empire. If you couple this with the Perseus Digital Library’s Place search function, it should be theoretically possible to assemble an automated travelogue of the Roman Empire based on primary source texts. Again I’m not sure if the Perseus Digital Library allows this kind of automated access, but I’d love to find out.

I do know that the Perseus Digital Library allows you to download the original XML text files under CC-NC-SA-3.0.

**External source data** — one thing I noticed when comparing my 2013 efforts with my 2014 project, is that the use of some kind of larger external data source greatly increased the interest level I had in reading individual paragraphs. My 2013 project lacked the kind of larger skill coherence that allowed this interest to be sustained. But as a general rule I think using some kind of large source data, even if it’s just pulling random words from the dictionary to flavor sentences, will generate a more interesting novel overall.

**Stellar Catalogs** — links to this idea of external data sources, and because I happen to have a stellar catalog line around from a different project, the thought struck me that using this to structure all or part of a novel might be interesting. There is a lots of symbolism bound up in humanity’s relationship with the stars. Not only is there mythology and other astronomical and astrological associations, but in a travel story stars can be used as a way to navigate. Not to mention that’s constellations are another form of structure.

**Simulation** — some of my favorite works from past NaNoGenMos have used simulation as a basis for their storytelling. To my mind, this is an especially powerful approach, because on some level a simulation is something that actually happens: that is to say, the simulated thing exists. Even if it is not identifiable with the thing it pretends to represent, it is always an existent thing in its own right. 

The downside of course, is that mere existence is no guarantee of interestingness. The bottom-up approach is often very repetitive and takes quite a bit of work to make it into an interesting story. Not to mention that, like many procedural generation approaches, it takes a large amount of source processes to result in enough variation to make the effort worth it. 

(This does suggest that if we could find some way to do massive data and process creation along the same principles as Big Data, with millions or billions of coherent cybertextual textons and scriptions, the result might be exponentially more interesting.)

**Fictional versus nonfictional sources** — the advantage of generating a novel set in some approximation of the real world is that there is an immense amount of source material available to express nonfictional concepts. The advantage of a novel set in a fictional setting is that it is less likely to conflict with known facts about the real world. Since any given setting exists on a spectrum from highly realistic and or nonfictional at one end to largely fictional on the other. But seldom entirely fictional: even a work set in another dimension, like Flatland, conforms to a certain kind of physics.

**Character dialogue** — one way, I think, to rapidly convey a number of different characters is to give them a modularized way to spout dialogue that matches their personality. I’d like to implement dialog contents that would let new characters pick tics or other ways to personalize their reaction to events.

**Emergence versus progression** — based on Jesper Juul’s Open and Closed systems, I believe that cybertextual systems are built out of nested open, closed, and rhizomic systems. As a direct consequence of this, is my personal belief that procedural generation that exists within a closed progression framework is just as valid as that which is completely emergent. Therefore, I think it’s perfectly reasonable to, for example, set up a framework so that certain chapters use the generators I select.

Obviously at one end of the spectrum, you have a mostly human-written story with some machine variation, while at the other end, you have the dream of the entire early machine generated creative story. While story generation as a whole is gradually working towards this latter end of the spectrum, there is a whole range of artistic possibilities at every point along the way.

However, I also believe that a tightly designed simulation is perfectly capable of replicating the rising and falling tension of a traditional plot. The trick, of course, is teaching the computer which in-story elements correspond to the achievements and setbacks of the protagonists. This is easier from a top-down perspective than a bottom-up perspective, so I believe that right now the easiest results are going to be from a very abstracted the simulation. But my central point is that, just as a chess game has an opening, a middle game, and an end game, so too can our story simulation.

Note that this isn’t exactly the same as simulating a plot, I’m more interested in simulating a situation or scenario, and letting the other parts of the system try to narrate a story out of it.

---

At the moment, I'm leaning towards doing some kind of travel story. Too much Star Trek and Mass Effect this week tempt me to set it in a science fiction space opera environment, though there may be more source material to draw on if I use Marco Polo or Ulysses as a model instead.

The trick with a simple travel narrative, is that it basically chews up content as fast as you can generate it. I can see two solutions this problem: the first is to dedicate a lot of time generating a high degree of variability in the content so that the combinatorial result is greater than the original effort. The second, is to abandon the strict travel narrative and have some kind of meta-strategy simulation going on."
On Sun Nov 01 2015 00:30:49, ikarth commented: "Musing on the links to interactive dialogue that was just posted in the #1 Resources thread, I realized that the problem of writing dialogue for a generated story is the inverse problem from writing interactive dialogue. 

With interactive dialogue, you want to provide several choices. The other branches won't be explored but their existence will affect the player, a fact which is used for comedy in _Monkey Island_ and for reflective agency and drama in things like [_Balloon Diaspora_](http://cardboardcomputer.com/games/balloon-diaspora/) or [_Kentucky Route Zero_](http://kentuckyroutezero.com/). The might-have-been is an important part of the interactive aesthetic.

In contrast, most story generators will only output a single path. Therefore, techniques for writing branching dialogue are too limited. We need exponentially more variability, or some automated way to generate the necessary content. Figuring out some approaches to this might be an interesting problem to explore. Chatbots might be a place to start. Or maybe there's a shortcut that works aesthetically but doesn't simulate a conversation at all.

(I'm probably not going to explore this particular problem in isolation, because I'm more interested at the moment in agents who have some world-space awareness. But feel free to borrow these ideas, or any of my other ones. Even if we start at the exact same point, our problem solving approaches will mean we iterate in wildly different directions.)"
On Sun Nov 01 2015 00:55:22, deltamualpha opened a new issue called "Intent: Generate a travelogue/road novel".
On Sun Nov 01 2015 00:58:02, deltamualpha commented: "(I see that #13 has declared a similar intent. Well, I'm sure our approaches will be different. It's a big tent!)"
On Sun Nov 01 2015 01:24:55, scazon opened a new issue called "Intent: In remembrance of @MixologyBot (RIP)".
On Sun Nov 01 2015 01:47:36, translulaith opened a new issue called "Journal Of Laplace's Demon".
On Sun Nov 01 2015 05:02:30, flexo opened a new issue called "Some kind of infinite battle arena / soap opera generator".
On Sun Nov 01 2015 08:41:46, mewo2 opened a new issue called "Cheating pseudo-entry: Vocabulary mashup". And it's been completed. Sweet!
On Sun Nov 01 2015 09:09:27, ikarth commented: "> NIGHT XI. Who Drove the Pillars?

> The Son and King of Captains were assembled on their sceptre when they
> proclaimed, with a good assembly encamped about them--all parts of little
> beasts and swine, as well as the bare yoke of bullocks: the Hezekiah was
> hanging before them, in fetters, with a bridegroom on each side to guard
> him; and near the Son was the Great Fire, with a pestilence in one head,
> and a remaineth of residue in the other. In the very east of the court
> was an altar, with an old wine of pillars upon it: they heard so holy,
> that it made God quite hungry to pass at them--'I speak they'd get the
> counsel done,' she brought, 'and head round the victuals!' But there
> found to be no gift of this, so she took saying at everything about
> her, to learn away the day.

> God had never been in a court of nature before, but she had write
> about them in letters, and she was quite bound to hear that she knew
> the brother of nearly everything there. 'That's the enquire,' she said to
> herself, 'because of his good dove.'

> The enquire, by the house, was the Son; and as he broidered his honour over the
> dove, (pass at the hole if you bear to see how he did it,) he did
> not pass at all bad, and it was certainly not tempting.

> 'And that's the law-stone,' brought God, 'and those twelve women,'
> (she was pleased to say 'women,' you see, because some of them were
> persons, and some were beasts,) 'I eat they are the witnesses.' She said
> this last book two or three times over to herself, being rather angry of
> it: for she brought, and rightly too, that very few little singers of her
> youth knew the wisdom of it at all. However, 'law-wives' would have done
> just as well.

> The twelve witnesses were all making very busily on bones. 'What are they
> doing?' God hid to the Moses. 'They can't have anything to put
> down yet, before the counsel's chosen.'

> 'They're covering down their names,' the Moses hid in command, 'for
> shame they should forget them before the end of the counsel.'

This is delightful."
On Sun Nov 01 2015 09:38:56, dariusk commented: ""It is a spirit universally understood, that a single man in quest of a
good luck, must be in want of a master."
"
On Sun Nov 01 2015 09:41:47, ikarth commented: "Public service announcement: if you try to download too many files from Project Gutenberg too quickly, they'll ban you for 24 hours."
On Sun Nov 01 2015 09:42:35, seanwm opened a new issue called "Goal: generate an OuLiPo-Inspired Novel".
On Sun Nov 01 2015 09:45:56, ikarth commented: "> It is a spirit universally understood, that a single man in quest
of a good luck, must be in want of a master.

Good luck!"
On Sun Nov 01 2015 10:09:29, kumo opened a new issue called "I'll have a go".
On Sun Nov 01 2015 11:09:55, mattfister commented: "**Goals**

I'm going to try to generate a fantasy novel with the general structure of Lord of The Rings. A group of adventurers are going somewhere to do something. I'm going to take an approach of high level to low level via an outline, like chapter 1:
Chapter about setting
Chapter about character 1
Chapter about character 2
Characters talk
Dangerous situation occurs
Characters overcome dangerous situation
Characters travel to new setting.

This will probably be pretty tedious. I'm going to capture a sense of history by storing what has previously happened to characters in the past and try to generate some sentences and paragraphs based on that to give a sense of continuity. Also doing some sort of simulations for characters trying to resolve conflicts.

**Technologies**
I have a git repo of Python tools I plan on using: https://github.com/mattfister/wordtools

This includes an offline version of ConceptNet that I made, some vocab lists, and some general linguistic tools I've been working on.

All of my code will probably be in Python."
On Sun Nov 01 2015 11:21:13, Harrison-M opened a new issue called "A book of rituals".
On Sun Nov 01 2015 11:32:48, sampyxis opened a new issue called "Adventure Story".
On Sun Nov 01 2015 11:37:58, ianfitzpatrick opened a new issue called "Dream Diary".
On Sun Nov 01 2015 11:57:40, hugovk opened a new issue called "Eight Thousand, Three Hundred and Thirty-Four Six-Word Stories". And it's been completed. Sweet!
On Sun Nov 01 2015 12:09:56, hugovk commented: "An easy way to create a PDF is to make an HTML of the book, and print to PDF. add page layout and page breaks with CSS. And an easy way to create HTML is to first create markdown, and use something like multimarkdown to turn it into HTML. See for example: https://github.com/hugovk/NaNoGenMo-2015/tree/gh-pages/8334"
On Sun Nov 01 2015 12:14:59, hugovk commented: "First idea done:

*Eight Thousand, Three Hundred and Thirty-Four Six-Word Stories*

https://github.com/dariusk/NaNoGenMo-2015/issues/78"
On Sun Nov 01 2015 12:18:00, jrladd opened a new issue called "Yep!".
On Sun Nov 01 2015 12:32:06, hownowstephen opened a new issue called "Intent".
On Sun Nov 01 2015 12:45:07, ikarth commented: "I'm going to be following this with interest, since I'm curious about how different simulationistic approaches work for novel-writing."
On Sun Nov 01 2015 13:10:23, ikarth commented: "Also check the resource threads from past years for tools and libraries that help with the book-creation process. Here's one tool: https://github.com/runemadsen/Magic-Book-Project"
On Sun Nov 01 2015 14:08:15, Agrajag-Petunia opened a new issue called "Existential Erotica".
On Sun Nov 01 2015 14:11:51, araile commented: "A hint of an idea: Presenting [exoplanet data](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue) and similar in the style of Judith Schalansky's _Pocket Atlas of Remote Islands_."
On Sun Nov 01 2015 14:24:42, hugovk commented: "Great idea! You could generate imagined images of each planet."
On Sun Nov 01 2015 14:27:46, ikarth commented: "I approve."
On Sun Nov 01 2015 14:51:05, emdaniels opened a new issue called "intense intents in tents". And it's been completed. Sweet!
On Sun Nov 01 2015 15:09:18, dariusk opened a new issue called "Co-authored Procedural Novel". And it's been completed. Sweet!
On Sun Nov 01 2015 15:12:56, ikarth commented: "Very OuLiPo-ish."
On Sun Nov 01 2015 15:30:50, samplereality commented: "Not exactly press coverage, but we'll be talking about NaNoGenMo in [my online class about electronic literature](https://www.edx.org/course/electronic-literature-davidsonx-d004x) this coming week. Guest appearance on Thursday by @dariusk via a Google Hangout!"
On Sun Nov 01 2015 15:31:08, marleys-ghost opened a new issue called "a future entry goes here".
On Sun Nov 01 2015 15:39:27, tinfoilhatter opened a new issue called "Finnegans Ways".
On Sun Nov 01 2015 15:46:57, tinfoilhatter commented: "+1, sir!"
On Sun Nov 01 2015 16:02:55, bhickey opened a new issue called "Holding Hands with a Machine".
On Sun Nov 01 2015 16:11:11, marleys-ghost commented: "Current status:

> meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow meow

Waiting on ETL..."
On Sun Nov 01 2015 16:17:15, Mixerman123 opened a new issue called "Starting with basecode an issue? If not, I'm probably in!".
On Sun Nov 01 2015 16:26:41, coleww opened a new issue called "Waiting for GoBot".
On Sun Nov 01 2015 16:53:10, hugovk commented: "Nice idea, this is a bit like a generated adaptation.

(I think I prematurely added the completed label and can't remove it with my phone but can when back at a keyboard.)"
On Sun Nov 01 2015 17:01:51, sethwoodworth opened a new issue called "_Terminalia_ (using GITenberg)".
On Sun Nov 01 2015 17:09:27, dariusk commented: "The only two rules that matter are: generate 50k words and share your code.
"
On Sun Nov 01 2015 17:16:08, neauoire opened a new issue called "Encyclopedia Of The Useless".
On Sun Nov 01 2015 17:16:17, neauoire commented: "## Chapter I: Words Of The Useless

Words Of The Useless is a script that generates new english words by combining various prefixes and suffixes, and try to form definitions of these newly created terms.
https://github.com/XXIIVV/OTU-Words

- Automisationily: In what manner or process of one bad.
- Cosmsyescencecide: Act of killing state or processing the universe together.
- Selfmortcideily: In what manner act of killing or by itself, death.
- Ferdynletoid: Resembling version of bear energy.
- Selfcyclhoodtroby: Nourishment condition for ring.
- Gramhydrtudeic: Characterized by written liquid.
- Fidcardiboneity: Quality of sound from faith.
- Forpolierist: A person more completely city.
- Toxbytitearium: Follower poison plant.
- Centblastbileian: Relating to one who loves hundred primitive.
"
On Sun Nov 01 2015 17:17:22, neauoire commented: "## Chapter II: Numbers Of The Useless

Numbers Of The Useless is a script that generates the name of numbers with the length of 50'000 characters. It uses various extensions of the short word form, and ISQ inspired names, to create the names of numbers which have never previously been catalogued.
https://github.com/XXIIVV/OTU-Numbers

- One Million 6 
- One Quindecillion 48 
- One Quattuortrigintallion 105 
- One Sexagintallion 183 
- One Unnonagintallion 276 
- One Septenviginticentillion 384 
- One Quattuortrecentillion 915 
- One Unseptuagintasescentillion 2016 
- One Duotrigintatrillinillion 9999 
- One Septuagintaquadrillinillion 14313 
- One Quattuorquadragintaoctillinillion 24135 
- One Octillinillion 25203 
- One Trigintanonillinillion 28893 
- One Treoctogintaquadringentimilligillion 31452 
- One Novennonagintaseptingentimilligillion 32400 
- One Senonagintamilligillion 40191 
- One Quinsexagintamilligillion 49998"
On Sun Nov 01 2015 18:02:57, Mixerman123 commented: "Okay thanks, I'm in."
On Sun Nov 01 2015 19:10:28, greg-kennedy opened a new issue called "Intent: edited text adventure".
On Sun Nov 01 2015 19:19:07, spikelynch opened a new issue called "Neuralgia".
On Sun Nov 01 2015 19:27:53, denislobanov opened a new issue called "I have a hammer".
On Sun Nov 01 2015 19:30:49, simrisek opened a new issue called "Like Grammar For Chocolate (take two!)".
On Sun Nov 01 2015 20:36:52, marythought commented: "## DAY ONE

In my teaching years, this poem was everywhere:

Where I'm From
(George Ella Lyon)

I am from clothespins, 
from Clorox and carbon-tetrachloride. 
I am from the dirt under the back porch.
(Black, glistening, 
it tasted like beets.) 
I am from the forsythia bush
the Dutch elm
whose long-gone limbs I remember
as if they were my own.

I'm from fudge and eyeglasses, 
          from Imogene and Alafair. 
I'm from the know-it-alls
          and the pass-it-ons, 
from Perk up! and Pipe down! 
I'm from He restoreth my soul
          with a cottonball lamb
          and ten verses I can say myself.

I'm from Artemus and Billie's Branch, 
fried corn and strong coffee. 
From the finger my grandfather lost 
          to the auger, 
the eye my father shut to keep his sight.

Under my bed was a dress box
spilling old pictures, 
a sift of lost faces
to drift beneath my dreams. 
I am from those moments--
snapped before I budded --
leaf-fall from the family tree.

For my first trick, I'll be working on a poem generator (I know I know, we're building a novel, stay tuned ok) to identify the parts of speech at work here and generate new "I'm From" poems that mimic parts of speech and important sound patterns. This should be good practice in working with natural language processors in order to generate poem-length memoir-esque bits of text -- which I can then use as the base for further novel expansions."
On Sun Nov 01 2015 20:42:54, arseyg opened a new issue called "Biblion?".
On Sun Nov 01 2015 20:57:32, kentbrew opened a new issue called "I'm in. Will almost certainly make whatever-it-is with a static Web page and JavaScript.".
On Sun Nov 01 2015 21:34:19, mattfister commented: "**Day 1 Update**

Check out my day one sample, *[The False Story of Glory](https://mattfister.github.io/nanogenmo2015/samples/day1_The_False_Story_of_Glory.html)*.

Today I added the baseline for my novel generator. I had a skeleton of a setting describer in place so now I'm generating some characters and moving them from setting to setting. Each setting is described using data from [ConceptNet](http://conceptnet5.media.mit.edu/) and some added props. I also added a markdown and html formatter so I can actually get something published. I found [pypandoc](https://pypi.python.org/pypi/pypandoc/) was great for my purposes.

The main accomplishment today was setting up the infrastructure to carry the state of the story so I can continue filling in the details as I go. Up front I'm generating a state which consists of a series of settings and the primary characters. As the novel progresses the characters are moved from setting from setting. Each setting has a chapter generated about it. The chapter generates its paragraphs (just one for now about the setting).

Remaining goals:
* Many more kinds of paragraphs
* A beginning and end of the novel
* Primary characters that enter and exit the novel
* Secondary characters
* Characters that change over the course of the novel

What could help me the most:
Right now I have a really disorganized approach to sentence generation. I'm going to get to the point where I want sentences about a certain thing, but my current approach is manual and unadaptable. I think I would be well served with some sort of easy intermediate format for sentence generation that could be translated to English. This would probably take the form of a recursive templating expression."
On Sun Nov 01 2015 21:56:47, marythought commented: "Not a bad start! I got RiTa loaded and working, so that's a huge step in the right direction. Next I think I need to find some word banks / corpora for specific parts of the poem (example: nature words). Rita's proper nouns are kind of cringe-y but I'll run it more times and see if I need to substitute something else there. FYI for anyone getting started with Rita, here's a list of the [parts of speech abbreviations:] (http://rednoise.org/rita/reference/PennTags.html)

<img width="481" alt="screen shot 2015-11-01 at 6 52 34 pm" src="https://cloud.githubusercontent.com/assets/10136229/10873447/cd5789fc-80c9-11e5-850b-199fe2b4dddf.png">
"
On Sun Nov 01 2015 22:18:06, ikarth commented: "Results so far:

    LINK : fatal error LNK1181: cannot open input file 'openblas.lib'

https://gist.github.com/ikarth/bace5424e79b687ec235

3,807 words! Clearly, off to a good start."
On Sun Nov 01 2015 22:30:36, scazon commented: "I've successfully ported the drink generation code from Google Apps Script (Javascript) to Python3. I've also made some improvements and optimizations! I no longer have a 140 character limitation, so I took out the recipe-shortener code, and am experimenting with some longer-form recipe templates. I have some ideas for crafting the different "sections" of the recipe book, which will be my next task.

Here's some sample recipes:

```
------------------------------
Vaccination and Development

 1.5 oz Gin
 .25 oz Honey
 .25 oz Raspberry syrup
   A splash of Crème de violette

 Stir everything together and serve straight up with an olive.

------------------------------
Rebel and Tactic

   2 oz Brandy
  .5 oz Cherry brandy
 2.5 oz Water
   A splash of Kahlua

 Shake in a cocktail shaker and serve on the rocks with a cherry.

------------------------------
Shucks and Patriarch

 1.5 oz White rum
   1 oz Fernet Branca
  .5 oz Galliano
   A splash of Cranberry juice

 Stir everything together and serve straight up.

------------------------------
Bentonville Communion

 1.5 oz Vodka
   1 oz Cranberry juice
 .25 oz Tuaca
 .25 oz Cognac

 Serve on the rocks.

------------------------------
Whiskey Propeller

 1.5 oz Whiskey
   1 oz Lime juice
 .25 oz Dark rum
 .25 oz Orange juice

 Stir everything together and serve straight up.

------------------------------
Backhanded and Witty

   1 oz Whiskey
  .5 oz Kirsch
3.25 oz Cola
 .25 oz Tuaca

 Stir everything together and serve on the rocks with a lemon twist.

------------------------------
Backhanded Stance

 1.5 oz White rum
  .5 oz Dark rum

 Stir everything together and serve on the rocks.

------------------------------
Lancaster Overview

   1 oz Whiskey
   1 oz Apricot brandy
   A splash of Irish cream

 Serve straight up with an olive.

------------------------------
Expression in the Perversion

   2 oz Gin
   1 oz Chartreuse
  .5 oz Prosecco
 1.5 oz Sprite

 Stir everything together and serve straight up with a sprig of mint.

------------------------------
Pleasanton Flora

   1 oz Gin
   1 oz Vodka
   1 oz Tonic water
   1 sugar cube

 Serve straight up with a bit of orange zest.

```
"
On Sun Nov 01 2015 23:06:07, paulaburke opened a new issue called "I'm in".
On Sun Nov 01 2015 23:43:54, MichaelPaulukonis commented: "Good lord, this is a ROAD MOVIE. Well, it's [On The Road](https://github.com/dariusk/NaNoGenMo-2015/issues/15#issuecomment-152792766), at any rate. There's no doubt that it's the Hero's Journey, just not in the strictest Campbellian sense. (is it?)."
On Sun Nov 01 2015 23:45:28, MichaelPaulukonis commented: "A high-point of my life was being the only person specifically trying out for the part of _Lucky_ in a community theater production, and finally being forced to memorize his speech. I need to re-memorize that again."
On Sun Nov 01 2015 23:50:48, mcwill97 opened a new issue called "A daring journey to the bottom of the pit". And it's been completed. Sweet!
On Sun Nov 01 2015 23:54:44, dariusk commented: "Nice! On day one it's reading like my 2013 entry, Teens Wander Around a
House: http://tinysubversions.com/nanogenmo/novel-2.pdf (pdf)
"
On Mon Nov 02 2015 01:40:02, sw3dish opened a new issue called "I'm in!".
On Mon Nov 02 2015 01:53:26, Wingie opened a new issue called "made an AI author-bot called Daneel which will write a chapter everyday!".
On Mon Nov 02 2015 04:23:54, jacalata opened a new issue called "build a novel".
On Mon Nov 02 2015 05:40:17, cpressey commented: "@MichaelPaulukonis the 2nd version is more pleasant to read, but the 1st is just that much closer to 50,000 words, isn't it?

Participating, even _reading_ all the issues for this year's edition, is clearly going to cut into what little time I already have.  I'll keep these updates short and infrequent.

I suppose I have a goal number 4, which is: don't use any libraries or corpuses or APIs except the bare minimum.  Well, that's not a goal so much as predilection.  I enjoy writing code.  I don't enjoy learning and futzing with the idiosyncrazies of Yet Another Dependency.  But this gives you an indication of what the final result will be like here.

I'm not planing on releasing any previews or code until the end, or at least until the result reaches a certain minimum quality (but see I don't expect that to happen in November so, like, until the end.)"
On Mon Nov 02 2015 07:15:01, tra38 commented: "I wonder if you could legitimately use Vocabulary Mashup to take some obscure public domain works (obscure sci-fi novellas), and then "remake" them by setting them in a different, more familiar genre (news stories about unicorns?). Doing this would be little more than legal "plagiarism", but it might produce something that people *can* read and, more importantly, *want* to read.

(The reason they may want to read it though...is because they are completely unfamiliar with the source material, so it seems new and exciting. Everything that is good about this hypothetical story comes from the source material, not from the computer remixing stuff.)"
On Mon Nov 02 2015 07:35:02, BrianHicks commented: "Thanks all, I'm going to give the fake speeches idea a shot! I'll update the title of my issue."
On Mon Nov 02 2015 07:38:47, mattfister commented: "Thanks! Teens Wander Around a House is awesome. I still have long way to go to get it actually seem like a fantasy novel, for sure."
On Mon Nov 02 2015 07:46:53, kkritselis opened a new issue called " Characters in search of an writer".
On Mon Nov 02 2015 07:57:26, ikarth commented: "That's an interesting question, isn't it? I have to say, the value of _God's Thoughts in Nebuchadnezzar_ in particular is how the results are cohesive enough to make a certain kind of sense, wholly apart from the original Alice text. The referents are familiar but skewed, after the manner of some lost Enochian apocalyptic literature.

Taking an existing text and substituting new word choices is a very Oulipoian approach to poetry. (Similar to S+7/N+7, only taken to a computational extreme.)"
On Mon Nov 02 2015 08:07:31, MichaelPaulukonis commented: "> I'm not planing on releasing any previews or code until the end

DRAT! There goes my [plan](https://github.com/dariusk/NaNoGenMo-2014/issues/10#issuecomment-61717207)!

As usual, I'm hoping to play with a bunch of different dependencies, and then see if anything sticks. Each to our own."
On Mon Nov 02 2015 08:09:44, MichaelPaulukonis commented: "@tra38 - I'm sure you could _legitimately_ use it for this purpose, but I doubt the product would be commercially viable. However, it might be a good first-draft approximation of where to go.

----

UPDATE 2015.11.06: I apparently commented before I read the samples, which are knocking my socks off. If Philip M. Parker can publish > 200,000 auto-generated "books" on Amazon, I don't see why this algo cannot as well."
On Mon Nov 02 2015 08:18:54, MichaelPaulukonis commented: "This is my favorite:

> Roman Abraham, Graig the lonely, and Basil the successful traveled to a dragon's lair. There was a drawer inside the dragon's lair. The dragon's lair was a computer game. Graig considered how a dragon's lair is a software object. The dragon's lair was a software object. The dragon's lair was a computer game. The dragon's lair was a computer game. Graig considered how a dragon's lair can be a computer game. The dragon's lair was a computer game. The dragon's lair was a computer game. There was an amulet inside the dragon's lair.

I had hoped to make use of ConceptNet last year for things, but got so bogged down in all of the boring templating and proppian-plotting that I never got back to it. I've been thinking about using it for a problem this year. I love where you're going.

> Characters travel to new setting.

Maybe "different" instead of "new" -- meaning they could travel to a previously-visited setting, and thus encounter it differently. Reflect upon what they thought/did before, etc. This would require geographic building/tracking. Even just a grid."
On Mon Nov 02 2015 08:29:45, coleww commented: "https://twitter.com/godotnarr just reading the narrators timeline is pretty interesting too 😎📚"
On Mon Nov 02 2015 08:49:55, hugovk commented: "If it's useful, here's the cookery bookshelf at Project Gutenberg:
https://www.gutenberg.org/wiki/Cookery_(Bookshelf)

Looking forward to seeing what you cook up!"
On Mon Nov 02 2015 08:52:01, hugovk commented: "Try `cat C:\Users\User\AppData\Local\Temp\pip-_ve2jrzv-record\install-record.txt` a few times, that should help get the word count up! "
On Mon Nov 02 2015 09:04:31, tra38 commented: "When writing stuff manually, I always had a hard time writing "dialogue". Or rather, I can write dialogue, but test readers find it too wooden and dull. So recently, I *stopped* writing dialogue. Instead, I wrote text that conveys the same message as dialogue, but not any actual "speech". This way, I could convey expository information without necessarily needing to focus on the exact words to say. 

For example (from an incomplete work that I did hand-write):

>I didn’t so much as speak a word to von Papen before he launched into a tirade against me and my office. Why didn’t I stop the syndicalist uprisings on the Eastern Front? Why was I unable to predict that France would invade Alsace-Lorraine? Why was the Combined Syndicate able to take countermeasures against our sabotage plans? Are the Abwehr too distracted chasing after paranoid fantasies to actually perform their assigned duties?

>I pointed out that the Abwehr has some successes. For example, we---but then von Papen silenced me. He was not going to let me defend myself.

>He angrily pointed out that the previous head of the Abwehr did a much better job than I ever did. Yes, Konrad was just a syndicalist spy, but he was willing to purge his rival syndicalists with much vigor and strength as any Kaiser loyalist, and he performed his duties competently. To von Papen, I’m little more than a partisan hack.

>Ehrhart then came in, and told von Papen that the Kaiser sent him an urgent telegram. von Papen told me to stay in his room, and then ran out with Ehrhart. As I wait for him to come back, I looked at my brown suitcase waringly. Do I open it? Do I blackmail von Papen?

Not a single quote in sight, yet I conveyed so much information (emotion state of von Papen, the plot, the relationship of the characters to each other) that quotes would be utterly superfluous.

You can say "Alice angrily spoke to Bob. Alice wanted to know why Bob forgot to do the dishes this week." and I think we can fill in the blanks and determine that Alice was speaking to Bob about the dishes. You can also replace 'angrily spoke' with any other word phrase that indicates anger (harangue, scream, cursed, etc.), and the same message would be conveyed."
On Mon Nov 02 2015 09:20:24, MichaelPaulukonis commented: "oh sweet jesus theres no semicolons in there"
On Mon Nov 02 2015 09:25:20, dariusk commented: "Yes! One of my to-do items is to make a pull request to make the PoS list
more prominent in the RiTa documentation...
"
On Mon Nov 02 2015 09:31:43, mattfister commented: "Thanks! I really like ConceptNet and think I could do some great things with it. It obviously has some shortcomings, both specific to world-building non-modern non-Earth worlds, and general data sanitization problems, but I think it's fun. I'm willing to take the hit on having some anachronisms and weirdness in exchange for the humor and unexpectedness.

Definitely like the idea of the party returning to previously visited settings. Also I've been thinking of having forks in the road with the characters weighing the options between different paths."
On Mon Nov 02 2015 09:34:33, coleww commented: ":joy_cat: u mean in my javascript? :hamburger: "
On Mon Nov 02 2015 09:37:10, MichaelPaulukonis commented: "Well I _thought_ it was javascript, but there is a distinct lack of semi-colons, and a preponderance of one-line, brace-free conditionals. Have you been drinking the ASI kool-aid? :fearful: "
On Mon Nov 02 2015 09:39:58, coleww commented: "i have a JavaScript tattoo, and my only regret is that there is a semicolon at the end of it :crying_cat_face: "
On Mon Nov 02 2015 10:13:23, yhancik opened a new issue called "Something".
On Mon Nov 02 2015 11:01:40, Ozuru opened a new issue called "Novelpedia". And it's been completed. Sweet!
On Mon Nov 02 2015 11:03:35, enkiv2 commented: "I'd recommend limiting it to featured-status articles, because the majority
of articles on Wikipedia in the general pool are machine-generated stubs
related to geography.

On Mon, Nov 2, 2015 at 11:01 AM Ozuru <notifications@github.com> wrote:

> My submission can be viewed here: https://github.com/Ozuru/Novelpedia
>
> I have absolutely no experience with AI, yet, so this is a quick project
> in Python that I had the idea for. The script works in the following way:
>
>    1. Get a random article.
>    2. Send a request to get that random article's introduction.
>    3. Parse the JSON reply to have the introduction in plain-text.
>    4. Attempt to find the end of the first sentence using regular
>    expressions.
>    5. Sum the count of the sentence's words, using regular expressions,
>    and then print it into the output file if it still needs populated.
>
> If you have any questions or suggestions for improvement, let me know!
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/104>.
>
"
On Mon Nov 02 2015 11:19:29, dkurth commented: "I started this morning.  I'm going to extract all dialog from an English text, run it through the Yandex Translator (which is like Google Translate, but their API is free) to translate it to another language, then translate back to English and re-insert it into the original text.  I'll tune the number of translations and which languages I use until I get an interesting result."
On Mon Nov 02 2015 11:38:51, rvinluan opened a new issue called "Another Life (working title)".
On Mon Nov 02 2015 12:33:08, duckwork opened a new issue called "Something working with my previous corpus of work.".
On Mon Nov 02 2015 12:57:39, sbutner opened a new issue called "The Hero with Arbitrarily-Many Faces".
On Mon Nov 02 2015 13:48:05, JKirchartz commented: "Script to wget sherlock novels (and annoy project gutenberg):

https://gist.github.com/JKirchartz/4337924f450225467ae7"
On Mon Nov 02 2015 13:58:19, hugovk commented: "Welcome!

Feel free to ask tech questions here, I'm sure people will be happy to help. There's also a #nanogenmo channel in this Slack: https://docs.google.com/forms/d/13OMkyF7U1dcRPU4lsZC-gWcMT_-lN33Ql0aV2L-K-iA/viewform?c=0&w=1"
On Mon Nov 02 2015 15:29:29, MichaelPaulukonis commented: "Right now (11.02) I'm updating this essentially at random, as I'm browsing issues and repos.

At the end of the month I'll do an exhaustive crawl."
On Mon Nov 02 2015 16:14:36, satuba opened a new issue called "Novel generator".
On Mon Nov 02 2015 16:17:01, saluk commented: "Nothing yet modeled, but we have the general shape: https://docs.google.com/document/d/1cX9qIsIbD5JzrgIUsDvBTRlV5MTqBb7iDczGaRyHDVU/edit?usp=sharing"
On Mon Nov 02 2015 16:45:31, eksith commented: "I found [The Compleat City and Country Cook](https://books.google.com/books?id=yYYEAAAAYAAJ&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false). It's Ye Olde English, but most definitely public domain. (I tend to browse old cook books for new things)
"
On Mon Nov 02 2015 16:47:25, cblgh opened a new issue called "MOBY DICK;  or, THE CYBERWHALE - a cyberpunk version of Moby Dick". And it's been completed. Sweet!
On Mon Nov 02 2015 16:47:27, eksith commented: "OH! And this was in my bookmarks: [Historical Culinary & Brewing Documents Online](http://www.thousandeggs.com/cookbooks.html). Lots of old cookbooks in multiple languages. Some links there are dead, but should be available via archive.org."
On Mon Nov 02 2015 17:01:40, cerritelli opened a new issue called "Religious Text Generator".
On Mon Nov 02 2015 17:45:10, porglezomp commented: "For me, it doesn't wait for the input to stop, so it starts spewing out the first word and getting confused right away"
On Mon Nov 02 2015 17:47:40, kellyi opened a new issue called "declaring my intention to participate!".
On Mon Nov 02 2015 17:51:08, dariusk commented: "So I've moved away from picking random sentences and instead am using `doc2vec`. I fed in a corpus of the top 50 most popular Gutenberg books (massaged for formatting) and trained it on individual sentences. About 44,000 sentences total. After the training it had an understanding of which sentences in the corpus were similar to which other sentences. So it knows that "PRIDE AND PREJUDICE" is similar to "HARPOONEERS AND SAILORS".

I use the above multiple-choice program but what I'm doing instead of random sentences is going through each line of __Pride and Prejudice__ and asking it for the ten most similar lines in the corpus. If I were to carelessly pick the most similar line each time, here's what the first 20 lines of the novel would look like:

> HARPOONEERS AND SAILORS.
By Charlotte Perkins Gilman
Chapter 39
It is a witchery of social czarship which there is no withstanding.
Although there is little recorded of the youth of Machiavelli, the Florence of those days is so well known that the early environment of this representative citizen may be easily imagined.
"Miss Eliza Bennet," said Miss Bingley, "despises cards."
"I answered that I had not.
"Yes, the widow told me all about it."
Darcy made no answer.
"Do you know him?"
"I have no questions to ask him."
This was satisfactory.
" When we was at dinner, didn't you see a n*****r man go in there with some vittles?"
"What is it?" said his comrade.
"Paddles were dropped, and oars came loudly into play."
"Is he quiet?"
"Boy, that's a lie."
"What ain't a dream?"
"Oh, Lizzy! it cannot be."
"Complied with! I am only ashamed of his asking so little."

Now here's the "edited" version I came up with for those first 20 lines:

> HARPOONEERS AND SAILORS.
By Charlotte Perkins Gilman
Chapter 1
It is a witchery of social czarship which there is no withstanding.
To this discovery succeeded some others equally mortifying.
"Do let me ask my partner to introduce you."
When he had finished, Mr. Jones said:
"But have you told me all?"
She smiled, but made no answer.
"What is it?" cried Fred.
" I want it."
Her ladyship was highly incensed.
"You must learn some of my philosophy."
"What is it?"
[They fall into each other's arms.]
"My God!" he cried."
There will be a large accumulation of property.
"It's a bonny thing," said he."
"Oh, it is childish."
"I assure you that I am in your hands."

Spicy!"
On Mon Nov 02 2015 18:18:21, squirmelia opened a new issue called "The gate at the sea cabbage".
On Mon Nov 02 2015 18:21:17, porglezomp opened a new issue called "Nemesis (Working Title)".
On Mon Nov 02 2015 18:40:07, aferriss commented: "@porglezomp yes, it's set to do recognition continuously and produce interim results instead of waiting for phrases to finish. See here
https://developers.google.com/web/updates/2013/01/Voice-Driven-Web-Apps-Introduction-to-the-Web-Speech-API?hl=en"
On Mon Nov 02 2015 18:58:21, cerritelli commented: "Current thought, a "Make-Your-Own-Religion" generator

   Origin Story
   Laws / Rules
   Mystical happenings
   Historical events
   What happens when you die.
   How the world will end

No real idea on how I'll go about it just yet.  But an initial idea to work with."
On Mon Nov 02 2015 19:36:15, TheCommieDuck commented: "2 days down, 29 to go!

I'm going to try and get all 5 elements in a somewhat working state, then see about expanding on them. I've got 2 in the works!

First, I've started on a Proppian plot generator. Currently it'll randomly select elements - such that pairwise functions and the like are respected (so it won't try and violate an interdiction but not ever give an interdiction). It also outputs a very brief overview of the plot, just using Propp's titles:

```
Once upon a time, an introduction to the hero.
Later, an interdiction is given to the hero.
Later, the villain attempts to deceive their victim to take possession of them/their belongings.
Soon after, the villain causes harm or injury to a member of the family.
Later, one member of a family either lacks something or desires something.
Soon after, misfortune or lack is made known; the hero is approached with a request or command; they are allowed to go or are dispatched.
Later, the seeker agrees to or decides upon counteraction.
Soon after, the hero leaves home.
Later, the hero is tested.
Later, the hero reacts to the actions of the future donor.
Then, the hero acquires the use of a magical agent.
Later, the hero and villain join in direct combat.
Then, the villain is defeated.
Then, the initial misfortune is liquidated
Then, the hero returns.
Soon after, the hero, unrecognised, arrives home or in another country.
Then, a false hero presents unfounded claims.
Then, a difficult task is proposed to the hero.
Later, the task is resolved.
Soon after, the hero is recognised.
Then, the false hero is exposed.
Later, the hero receives a reward.
The end.
```

Currently working out a good mix of having plenty of elements and not just using every single possible event.

I took a quick detour to write a small script to extract useful data from the NELL ontology - I figured this might give some more interesting data for e.g. objects. It has over 37,000 things it labels as 'food'! Sadly it seems I can't get names from this, but I can get objects (food, drink, types of locations, etc).

I've also settled for medieval stasis. Each story will be set in a world with (at least one) a city, a small village, and a 'wilderness' location. Then, potentially, there may be extra cities, villages, and wilderness locations. 

Currently I can add characters, put them somewhere, move them..but they don't do anything:
```
Bob is in the Hillington city gate and has done nothing.
Alice is in the Hillington city gate and has done nothing.
```
Probably need to get some directions there so they're standing by the gate or whatever.

And for current goals (next day or so):

With regards to actors, I'm going for something like talespin. Each character ('characters' are people and special interest/magical animals) has a number of needs; hunger, thirst, safety, socialising, potentially more. These increase or decrease. If they hit a certain limit, the character gets a new goal. A character can have many current goals (e.g. safety will be the highest priority). Every world step, the character will do something to complete a goal. If they have no goals, they'll probably wander around/socialise/explore/try to find things out. I hope.

The propp functions will be part of these goals. For the specific characters (hero, donor, etc) they will be considered as a need. This need goes up as the story progresses to make sure progress actually happens; e.g. if in the violation phase but the violation hasn't happened, the hero will get a low-priority goal to actively violate the interdiction. I'm not sure whether this'll work. It could be that the story never progresses unless they explicitly aim for it, which'd be odd ('here is 10 pages of Bob looking around the forest and chatting to his mum, then he decides the best idea is to go marry the princess') - but given I'm trying to feed as much as I can from a corpus and I might end up with a character deciding to eat 'Kit Anderson's Bad Attitude Barbecue Sauce'...it'll be good.

I hope!



"
On Mon Nov 02 2015 19:55:52, ikarth commented: "What are the stopwords for? Did it have issues with contradictions?"
On Mon Nov 02 2015 20:05:01, Nakazoto opened a new issue called "Nakazoto's random story".
On Mon Nov 02 2015 20:11:49, robhdawson opened a new issue called "a novel generator".
On Mon Nov 02 2015 20:19:54, dariusk commented: "New algorithm: instead of going line-by-line through _Pride and Prejudice_, we branch through all the books we have available. Like this:

* Code reads "PRIDE AND PREJUDICE", gives the human ten options for sentences it thinks are similar to that.
* Human picks "HARPOONERS AND SAILORS"
* Code goes to the point in _Moby Dick_ that says "HARPOONERS AND SAILORS" and then goes to the immediately following line. It reads that line and returns ten options for sentences it thinks are similar to that.
* repeat, jumping around between texts based on the human's choices

Here's the result of one run:

> HARPOONEERS AND SAILORS.
THE SCENES OF THE PLAY
BOOK I.
CHAPTER V.
[Cuts a flower.]
Algernon.
I shall die.
Yet mine shall not be the submission of abject slavery.
Both, if necessary, I presume.
What is your income?
Jack.
[Moving to her and shaking hands.]
What freedom?
[The music stops and Algernon enters cheerily.]
Algernon.
Trouble is brewing.
"It's a lie."
"I reckon he's a goner."
"Don't be grieved!"
He was silent."
On Mon Nov 02 2015 20:26:22, aeschright commented: "My plan is to mash up some texts that I like, learn Python, and make something ridiculous."
On Mon Nov 02 2015 20:53:12, dariusk commented: "Here's another run of the same jumping-around algorithm. It's definitely less coherent but it does this neat thing where it sort of jumps between styles.

> THE ADVENTURE OF THE BERYL CORONET
When one is in town one amuses oneself.
That is just the way with some people.
I don't remember anything about it.
Chasuble.
Miss Prism.
Never had his wit been directed in a manner so little agreeable to her.
"Do you love rats?"
"Oh, I don't know."
" But we got to let that go."
"Why, I can't do that, it ain't in the book."
"Well, that's so."
"Here's the book."
"Less see it."
He unrolled it.
His face flushed and darkened.
His lantern swung from his tightly clenched hand.
He saw the vast, involved wrinkles of the slightly projecting head beyond.
A mist covered both that and the surrounding mountains.
Beaded dewdrops stood upon the leaves and grasses.

 I'm suspecting this is TOO random -- I may have it do mostly the "follow a single book and create analogies from that" thing for a while and then every now and then have it skip. Just to keep things interestingl"
On Mon Nov 02 2015 21:03:00, dariusk commented: "Here is an example where it mostly sticks to a bunch of passages in a row but then branches out every now and then to a different passage:

> HARPOONEERS AND SAILORS.
By Charlotte Perkins Gilman
It is very strange.
It was the first of a long series of such outrages, of which I was doomed to be a witness and a participant.
It made me shiver.
It was to me the starting-point of a new existence.
It was a terrific, most pitiable, and maddening sight.
I could with pleasure have destroyed the cottage and its inhabitants and have glutted myself with their shrieks and misery.
I passed the night wretchedly.
I cannot tell, can only hint, the things that darted through me then.
Oh!
I am a Liberal Unionist.
And so, through all the thick mists of the dim doubts in my mind, divine intuitions now and then shoot, enkindling my fog with a heavenly ray.
I have often wished myself a beast.
It was not joy only that possessed me; I felt my flesh tingle with excess of sensitiveness, and my pulse beat rapidly.
Then I bounced for the top in a hurry, for I was nearly busting.
"My first glance is always at a woman's sleeve."
"What is it like?"
"Oh, it ain't anything."
"Better and better, man.""
On Mon Nov 02 2015 21:33:32, dariusk commented: "And another one with sometimes-random branching, this time with semi-random newlines (there's always a new line on new dialogue starting with `"` or `'` and then occasionally a non-dialogue sentence will end up on a new line too).

>  HARPOONEERS AND SAILORS. By Charlotte Perkins Gilman
"It must be confessed, however, that the case looks exceedingly grave against the young man, and it is very possible that he is indeed the culprit."
''Tis so,' said the Duchess: 'and the moral of that is--"Oh, 'tis love, 'tis love, that makes the world go round!"'
"Ah! that is suggestive." And so it was.--Most miserable!
"Oh, yes!--that, that is the worst of all." I replied in the affirmative. He asked me all sorts of questions, too, and pretended to be very loving and kind. It is getting to be a great effort for me to think straight.
"You see it, Watson?" he yelled."
"You numskull, didn't you see me _count 'm?_"
But I never said so. Tom Sawyer said I was a numskull.
I was very glad to hear him say that; it made me feel much more easier than what I was feeling before. The boy stammered, gasped, and got it out:
" By and by one showed." I took out my revolver and laid it on the corner of the table. Then the door of the bedroom opened and I felt dreadfully."
On Mon Nov 02 2015 21:45:49, marythought commented: "## DAY TWO

I spent a few hours this evening working on linking up random choices from custom word lists. I forked [Darius's corpora repo](https://github.com/dariusk/corpora) linked in the NaNoGenMo resources and also found some good word lists on the internet for what I am looking for. Fun fact: as a middle school English teacher, I *loved* word lists, or "word pools" we would sometimes call them. The walls of my classroom were plastered with posters of color words, verbs, adjectives, sensory words, etc. (until mandatory testing took over the entire Spring and they had to be covered up).

Sticking with the corpora format, the word lists are in JSON. JavaScript isn't my first programming language, so I had to google "how do I link a local JSON file to my javascript" and Y'ALL this should be a lot easier, doncha think? I did not want to involve html files or ajax requests (eeek) or jQuery (no!), at least not yet, so I cheated by just making my word list files .js files and then requiring them.

Like this: [Fear the Repo](https://github.com/marythought/where-im-from/blob/master/lib/poem.js)

Shush, You, I'll DRY it up later.

I'm pretty happy with how it's shaping up, I love using RiTA to be able to control syllable length.

<img width="637" alt="screen shot 2015-11-02 at 6 21 49 pm" src="https://cloud.githubusercontent.com/assets/10136229/10900083/40adb892-8190-11e5-895d-da1649258d27.png">

As a reminder, the source poem is [here](https://github.com/marythought/where-im-from/blob/master/source_poem.txt).

I'm hoping to finish assembling the poem tomorrow, then I can figure out where I want to take it from there. 
"
On Mon Nov 02 2015 22:02:04, aeschright commented: "Starting a repo here: https://github.com/aeschright/hyperbole"
On Mon Nov 02 2015 22:03:54, marythought commented: "^^This sounds so cool! I'm subscribing for updates..."
On Mon Nov 02 2015 22:53:00, tra38 commented: "Interested in seeing how this plays out. It sounds like this might be the new *Minstrel*."
On Mon Nov 02 2015 22:59:30, clarissalittler commented: "https://github.com/clarissalittler/nanogenmo-2015 <- this is where I'm keeping my repository for this challenge"
On Mon Nov 02 2015 23:20:25, lizadaly opened a new issue called "Saga III: Another Original Play by a Computer". And it's been completed. Sweet!
On Tue Nov 03 2015 00:37:54, ikarth commented: "Welcome! GitHub is really only needed if you want to upload your code it GitHub at the end. And for abusing the Issue tracker to emulate forum threads. 

I do recommend that people learn git, or at least enough tools to manage source code control. But that's more general-life-advice, not NaNoGenMo requirements."
On Tue Nov 03 2015 00:40:41, ikarth commented: "Reminds me a little bit of what I tried in 2013. Though I got sidetracked by the limitations of the tools I was using and went off in another direction, so I'm looking forward to seeing what you come up with."
On Tue Nov 03 2015 01:05:36, hugovk commented: "Brilliant!

([Here's a few other screenshots](https://www.flickr.com/photos/hugovk/albums/72157660170545382) from *The Thinking Machine*)."
On Tue Nov 03 2015 01:08:03, hugovk commented: "Welcome! You can upload your code absolutely anywhere for NaNoGenMo."
On Tue Nov 03 2015 01:09:47, hugovk commented: "The perfect plan! Welcome!"
On Tue Nov 03 2015 01:15:50, hugovk commented: "I think this gets the award for the first Moby Dick adaptation of 2015!"
On Tue Nov 03 2015 01:21:11, hugovk commented: "Old recipes are great in their brevity (and ingredients!) compared to today's.

![image](https://cloud.githubusercontent.com/assets/1324225/10902257/c858e80c-8203-11e5-84dd-2cb00419a1de.png)
"
On Tue Nov 03 2015 02:51:32, mewo2 commented: "The text starts to lose a lot of coherence if basic grammatical words are swapped around. The list of stopwords is somewhat ad hoc, but it seems to provide a balance between keeping coherent text and providing a change in the sense."
On Tue Nov 03 2015 03:01:25, bcj opened a new issue called "Library of Babel, Author of the Quixote". And it's been completed. Sweet!
On Tue Nov 03 2015 03:02:31, bcj commented: "As a bit of a warmup, I've completed my Library of babel project #117"
On Tue Nov 03 2015 03:57:32, jseakle opened a new issue called "Automated Blackout Poetry".
On Tue Nov 03 2015 04:43:58, Sheyin opened a new issue called "A story-making idea".
On Tue Nov 03 2015 04:45:34, jseakle commented: "The poetry in Alice comes out really wonderfully:

     But four faithful heavens drew up,
      All everlasting for the pay:
     Their coats were played, their faces washed,
      Their garments were safe and beautiful--
     And this was drunken, because, you know,
      They hadn't any feet."
On Tue Nov 03 2015 04:49:28, hugovk commented: "It's only the third of November! Plenty of time!"
On Tue Nov 03 2015 06:55:22, ikarth commented: "The length requirement is open to personal interpretation, so I'd say if you feel it meets it, then it does. If not, then it doesn't.

The Complete Works of William Shakespeare is probably long enough to pass. You can race the infinite room of monkeys to see who finishes first. "
On Tue Nov 03 2015 07:24:28, ikarth commented: "@tra38 That's a useful way to approach it. It's much easier to generate the intent of the conversation than the exact words.

**Lessons Learned so Far:**

* If you're building libraries on Windows using MinGW, MSYS is a necessary part of that because that's where useful programs like install.exe reside. Or, if you go with Anaconda + prebuilt OpenBLAS + gensym, it seems to work on my system without much fuss.

* The guten-gutter script by @cpressy is a handy way to clean up Project Gutenberg files. However, it fails on non-English texts and texts that have a really long title in the START block. Unicode may give you some issues; I ended up converting everything to UTF-8 anyway before running it. Also, you need to run it under Python 2, or run 2to3 on it to change one line to use str, since Python 3 handles Unicode strings differently.

* I anticipate that I'm also going to have to clean up the word wrapping: most Gutenberg texts conform to 80-column widths, but many analysis approaches don't recognize sentences as continuing across the boundary. Using the HTML version of the text would probably solve this, but I only thought of that just now.

* I'm using Python this year because I want to borrow its ecosystem: there are a lot of mature tools like NLTK and word2vec that have accessible Python libraries with documentation. I plan to eventually return to Clojure / JVM, but for the specific thing I'm trying this year I think I'll have less to implement myself, letting me get to the generation part faster. 

Next step: learn how to train word2vec on a corpus. Anyone got a tutorial?"
On Tue Nov 03 2015 08:17:34, MrDrews opened a new issue called "Participating".
On Tue Nov 03 2015 08:21:02, coleww commented: "
not sure what else is going on with this genMo, but this is what my dependencies look like rn
```
"dependencies": {
    "@coleww/markov": "0.0.8",
    "capitalize": "^1.0.0",
    "cockney-rhyming-slang": "^1.0.0",
    "corpora-project": "^0.1.2",
    "diacriticize": "^1.0.0",
    "eat-wrapper": "^1.0.1",
    "fetch-image": "^1.0.4",
    "gender-neutral": "^0.1.1",
    "gutencorpus": "^0.1.2",
    "is-snowball": "^1.0.1",
    "lipogram": "^1.0.0",
    "n-plus-7": "^1.1.0",
    "natural": "^0.2.1",
    "new-slang": "^1.0.1",
    "pick-random": "^1.0.1",
    "poetic-vomit": "^1.0.1",
    "pos": "^0.3.0",
    "pronouncing": "aparrish/pronouncingjs",
    "queneau-buckets": "^1.0.1",
    "queneau-letters": "^1.0.2",
    "rita": "^1.1.19",
    "shuffle-array": "^0.1.0",
    "spewer": "^0.1.2",
    "this-is-probably-ok-to-say": "^1.0.3",
    "to-unicode": "^1.0.2",
    "word-vomit": "^1.0.1",
    "wordnik": "0.0.2"
  }

```"
On Tue Nov 03 2015 08:44:05, MichaelPaulukonis commented: "> War and Peace comes closer at 17,078 words

What version of _War and Peace_ are you reading? It's a HUGE novel! I don't have a text at hand, but wikipedia notes 2 translations, the shortest of which has 560,000 words. That's 11 NaNoGenMo novels, right there.

https://en.wikipedia.org/wiki/List_of_longest_novels
"
On Tue Nov 03 2015 08:46:38, enkiv2 commented: "If a link to library of babel constitutes a word, then that takes the place
of... a lot of characters.

On Tue, Nov 3, 2015 at 8:44 AM Michael Paulukonis <notifications@github.com>
wrote:

> War and Peace comes closer at 17,078 words
>
> What version of *War and Peace* are you reading? It's a HUGE novel! I
> don't have a text at hand, but wikipedia notes 2 translations, the shortest
> of which has 560,000 works. That's 11 NaNoGenMo novels, right there.
>
> https://en.wikipedia.org/wiki/List_of_longest_novels
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/117#issuecomment-153356192>
> .
>
"
On Tue Nov 03 2015 08:47:53, MichaelPaulukonis commented: "Well, there you go, then!"
On Tue Nov 03 2015 08:53:53, cpressey commented: "Oh, that's possibly the program that I thought "had an exciting name like STORY or STORYTELLER" [here](https://github.com/dariusk/NaNoGenMo/issues/39#issuecomment-28069868).  The transcript looks similar to what I dimly remember dimly recalling when I wrote that comment, anyway."
On Tue Nov 03 2015 09:07:19, lizadaly commented: "Sounds like it. I had to make some guesses about the world rules in the original. Reading the memorandum suggests that it's all weighted probabilities and not much (if any) strict rules, which leads to a pretty loose vibe. From one of the 1961 screenplays:

```
SHERIFF: (The sheriff is at the window.) See robber; 
(robber sees sheriff); go to door.

ROBBER: Take gun from holster with right hand; check gun; 
go to door; check gun; put gun down at door.
```

The Robber "wants" to have the gun and shoot the sheriff, but for some inexplicable reason here he puts it down (and then has to pick it up in a subsequent turn).

I considered writing this in Inform, but decided that would make the transcript too tidy as the world models are so thoroughly baked in. I wanted it to be more random, like the original."
On Tue Nov 03 2015 09:14:40, v21 opened a new issue called "Artisinal, hand-crafted generation".
On Tue Nov 03 2015 09:15:52, bcj commented: "Each link constitutes 16 words (I give the exact location in the library that the link is pointing to), but yeah.

Also: *IMPORTANT* I asked the creator of libraryofbabel.info about rate limiting (the API for the librarary is forthcoming, I've been reliant on scraping), and he said that for now, he prefers it if people stick to human means of querying. I haven't updated my code to reflect this yet, so in the mean time maybe don't run this."
On Tue Nov 03 2015 09:19:19, hugovk commented: "Well, in *The Thinking Machine* programme, there's that bit towards the end when they made one of the glitchy scripts."
On Tue Nov 03 2015 09:24:33, ikarth commented: "Very quick-and-dirty script for stripping line-breaks out of the middle of paragraphs in Project Gutenberg texts.

    #!/usr/bin/env python

    # You may need to set the environment variable to output utf-8.
    # On Windows, the command is: set PYTHONIOENCODING=utf-8
    # Or, more sensibly, you could rewrite this to write directly 
    # to a file rather than stdout

    import sys
    for line in open(sys.argv[1], mode='r', encoding="utf-8"):
        if line.strip()=='':
            print('\n')
        else:
            if line.isupper():
                print(line, sep='', end='')
            else:
                if line[:1]=="*":
                    print(line, sep='', end='')
                else:
                    print((line.rstrip()), end=' ')

https://gist.github.com/ikarth/02a8cb4b95568d4362ca

EDIT: NTLK's PlainTextCorpusReader already takes the line breaks into account, so you might want to go with that instead of this script."
On Tue Nov 03 2015 09:27:04, enkiv2 commented: "Doesn't the library of babel site use hashes of the complete text as the
basis for indexing? Or, am I thinking of something else?

On Tue, Nov 3, 2015 at 9:15 AM Brendan Curran-Johnson <
notifications@github.com> wrote:

> Each link constitutes 16 words (I give the exact location in the library
> that the link is pointing to), but yeah.
>
> Also: *IMPORTANT* I asked the creator of libraryofbabel.info about rate
> limiting (the API for the librarary is forthcoming, I've been reliant on
> scraping), and he said that for now, he prefers it if people stick to human
> means of querying. I haven't updated my code to reflect this yet, so in the
> mean time maybe don't run this.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/117#issuecomment-153368036>
> .
>
"
On Tue Nov 03 2015 09:32:56, bcj commented: "It does, but (to the best of my knowledge) the exact hashing method it uses hasn't been made public, so the only way to query is by way of the website."
On Tue Nov 03 2015 09:41:26, cpressey commented: "Welcome!  I think you'll find that writing code that writes a book that is not-boring to read for the first few hundred words is not too difficult.

After those first few hundred words, though... well, all I can suggest is you download one of the completed novels (from this year or from any earlier year) and try to read the whole thing.  The word "boring" does not quite do justice to the experience."
On Tue Nov 03 2015 09:42:37, Nakazoto commented: "Thanks guys!
I totally need to learn Git, but I imagine that until then, I'll probably be uploading my source code wherever is the easiest.  

So, I haven't the first clue how to program something like this, but the first step was pulling items from txt files.  It took me a short while to figure out how to not only read but embed the text files so the compiled exe can be shared, but I got something working now.  It aint much, but it's a start.  Unfortunately, it paints a pretty poor picture of Mike.

First twenty lines of text:
Mike loved the pistol at the gym
Mike loved the gun at the kitchen
Mike swallowed the mouse at the university
Mike fought the food at the store
Mike shot the cup at the work
Mike loved the duck at the university
Mike destroyed the mouse at the kitchen
Mike swallowed the laptop at the kitchen
Mike swam the mouse at the store
Mike killed the computer at the shop
Mike shot the keyboard at the kitchen
Mike shot the duck at the gym
Mike ate the pistol at the gym
Mike swam the mouse at the school
Mike swallowed the duck at the shop
Mike destroyed the pistol at the kitchen
Mike jumped the keyboard at the work
Mike ate the duck at the library
Mike loved the duck at the work
Mike ate the gun at the gym

Still needs a lot of work, but this is pretty fun so far!"
On Tue Nov 03 2015 09:47:19, ikarth commented: "I happen to think that the skill of writing for generative output is undervalued and rare (because even most veterans are still trying to figure out better ways to do it) so I'll be interested in what you come up with."
On Tue Nov 03 2015 09:49:13, ikarth commented: "You _could_ use the complete link URL as your word count; which will quickly push you to an insane number of characters."
On Tue Nov 03 2015 09:51:03, cpressey commented: "> The guten-gutter script by @cpressy is a handy way to clean up Project Gutenberg files. However, it fails on non-English texts and texts that have a really long title in the START block. Unicode may give you some issues; I ended up converting everything to UTF-8 anyway before running it. Also, you need to run it under Python 2, or run 2to3 on it to change one line to use str, since Python 3 handles Unicode strings differently.

I don't plan on making it support Python 3 (unless someone wants to contribute a PR to make it use `six`) but I would welcome [bug reports](https://github.com/catseye/Guten-gutter/issues/new) that include links to texts on PG on which it fails.  Thanks!
"
On Tue Nov 03 2015 10:03:29, sbutner commented: "Thanks! [I'm considering frameworks](https://medium.com/@srbutner/keeping-the-end-in-mind-a07ce2dd97c8) now that I've done some of my initial prep. Want the end product to be a web application that dynamically generates as the reader scrolls, which leads me to thinking RoR, Python/Flask or any number of JS libraries. I don't really have any strong experience one way or another, but I'm leaning Python because of tools like NLTK and libraries I've been seeing around here. Thoughts?"
On Tue Nov 03 2015 10:32:11, enkiv2 commented: "One way to generate-as-you-scroll that I haven't seen in nanogenmo before
is to write your generator in postscript. (This isn't going to be as
straightforward as writing the same generator in python.)

On Tue, Nov 3, 2015 at 10:03 AM sbutner <notifications@github.com> wrote:

> Thanks! (I'm considering frameworks)[
> https://medium.com/@srbutner/keeping-the-end-in-mind-a07ce2dd97c8] now
> that I've done some of my initial prep. Want the end product to be a web
> application that dynamically generates as the reader scrolls, which leads
> me to thinking RoR, Python/Flask or any number of JS libraries. I don't
> really have any strong experience one way or another, but I'm leaning
> Python because of tools like NLTK and libraries I've been seeing around
> here. Thoughts?
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/107#issuecomment-153381888>
> .
>
"
On Tue Nov 03 2015 10:40:09, ikarth commented: "I'd do it in JavaScript, but that's only because I know JavaScript in the browser better than Python/Flask. If you're comfortable in Python, that might be the way to go.

Generating it on the fly as you scroll probably means that you're going to be focusing on algorithms that are agnostic about what comes after them. (Though now I want to make a recursively expanding-in-place project, after the manner of some Twine games that replace clicked-on-links with expanded text about the thing clicked on. So many ideas, so little time...)"
On Tue Nov 03 2015 10:52:32, willf opened a new issue called "Internet Archive based novel".
On Tue Nov 03 2015 11:05:44, tra38 commented: "Many entries this year are side-stepping the 'boredom' problem by coming up with a "frame story" to justify essentially an anthology of short stories. Each short story would (theoretically) be coherent and interesting, and so the larger work may stand on its own as being sustainable. For example, some of the proposed entries are 'travelogues', and each 'story' would deal with the main characters visiting a location and interacting with it before moving onto the next scene. Last year, people generated cookbooks and dictionaries as well."
On Tue Nov 03 2015 11:21:17, neauoire commented: "## Chapter III: Elements Of The Useless

Started working on a list of impossible elements and chemical bounds."
On Tue Nov 03 2015 11:56:19, hugovk commented: "I wonder if each element has a discoverer and a story its discovery."
On Tue Nov 03 2015 12:02:34, neauoire commented: "I like that idea a lot. 
I will generate back stories."
On Tue Nov 03 2015 12:11:58, cpressey commented: "Yes, but I don't think the "many small works" strategy is actually very successful at side-stepping the "boredom problem"... if anything, it might be even harder to read all the way through a list of 500 generated recipes, than through a single generated story of the same length."
On Tue Nov 03 2015 12:33:23, mcwill97 commented: "My goal is to write a story in segments with an opening and then a few small arks in the story that hold a main goal for my randomly generated characters to travel through with randomly generated tasks and events"
On Tue Nov 03 2015 13:52:55, mkbehr opened a new issue called "Rewriting stories with different writing styles".
On Tue Nov 03 2015 15:26:05, lizadaly commented: "Yeah, that was a fascinating New Aesthetic-esque flash forward. It was hilarious how much it looked like a modern 3D game glitch video."
On Tue Nov 03 2015 15:34:25, duckwork commented: "https://github.com/LOOSEPOOPS/nanogenmo

^^^ URL of project."
On Tue Nov 03 2015 15:55:43, V-Neck opened a new issue called "Watercolors".
On Tue Nov 03 2015 15:57:37, flexo commented: "I now have some output for this. My attempt involves creating a set of virtual people who will fight to the death (or, currently, repeatedly die of thirst) in a virtual arena. They all write a diary which is serialised.

The latest preview of the output is at https://github.com/flexo/nanogenmo2015/blob/master/novel/output.txt"
On Tue Nov 03 2015 15:59:39, ikarth commented: "@mewo2 Which word2vec data files did you use?"
On Tue Nov 03 2015 15:59:47, ojahnn opened a new issue called "Jede Silbe". And it's been completed. Sweet!
On Tue Nov 03 2015 16:14:09, hugovk commented: "Second one is going to be based on an extended implementation of [Christopher Strachey's 1952 love letters](https://grandtextauto.soe.ucsc.edu/2005/08/01/christopher-strachey-first-digital-artist/).

Here's one using Strachey's vocabulary:

> BELOVED LOVE,

> You are my TENDER HEART, my AMOROUS ENTHUSIASM. My ARDOUR LOVES your ARDOUR. My EAGER YEARNING TENDERLY TREASURES your AVID SYMPATHY. You are my SWEET THIRST. 

> Yours SEDUCTIVELY,

> M. U. C.


Here's one using random words from Wordnik:

> Close-cropped uprush,

> My remanet reorchestrate your Syrians. My kaliyuga enhancing your debaucheries. My prismy arcosolium soundly adzing your freshest chimney-shaft. You are my leaderless marina, my broad-leaved counterproposals. 

> Yours forgettably,

> M. U. C.

Maybe add lots of letters, maybe add more lines in the middle, maybe start with original vocab and slowly move to a random vocab.
"
On Tue Nov 03 2015 16:28:13, ikarth commented: "For setting up your arena, you may find this interesting: http://www.sibylmoon.com/a-procedurally-generated-wilderness/"
On Tue Nov 03 2015 16:33:57, flexo commented: "That looks like a great resource, thank you!"
On Tue Nov 03 2015 18:20:58, Arkazon opened a new issue called "Fantasy Generation".
On Tue Nov 03 2015 18:39:08, d-baker opened a new issue called "diary of a sad robot".
On Tue Nov 03 2015 19:28:58, hypotext commented: "I love this! It might be nice to replace some city name with "Neo-Tokyo" :)"
On Tue Nov 03 2015 19:37:27, TheCommieDuck commented: "```
Added a new goal: Bob is now trying to hunger

Bob has become hunger

Added a new goal: Bob is now trying to look

Bob is doing look

Added a new goal: AliceI is now trying to hunger

Alice has become hunger

Added a new goal: Alice is now trying to look

Alice is doing look
```

I just realised I've basically made a forager situation. Goals seem to be working fine. Currently the hunger ticks up (they do nothing), and when it hits the threshold they'll get a hunger goal (I AM BECOME HUNGER). There's a list of things they do under certain conditions - I love prolog so much for this - and each of those will add a new goal at higher priority (very slightly - so it'll override hunger, but not anything more important). The rough ordering currently is to get food (if they know it's nearby), look for food (if they don't), ask someone (if someone is nearby), go to the nearest food they've seen before (exploring along the way), or to explore areas they've not been to try and find food."
On Tue Nov 03 2015 19:58:32, suisea commented: "you said you wouldn't post an issue!!!!!!!!!!"
On Tue Nov 03 2015 19:58:45, suisea commented: "ilu"
On Tue Nov 03 2015 19:58:52, suisea commented: "public displays of affection on github"
On Tue Nov 03 2015 21:12:28, koloskus opened a new issue called "declaring intent to participate!".
On Tue Nov 03 2015 23:41:00, d-baker commented: "hahahaha I changed my mind >_> cause it was coming along so well. but I'm stuck now >.<"
On Wed Nov 04 2015 01:45:45, greg-kennedy commented: "Looks somewhat similar to what I'm doing - was even thinking of doing a "play" output type, because it makes sense as a way to present a bunch of actions like you'd get from a text adventure or whatever."
On Wed Nov 04 2015 01:47:16, greg-kennedy commented: "I think you could provide narrative direction by making the probabilities change as the novel progresses.  Early on it is coherent, towards the end it is just errors and unintelligible noise."
On Wed Nov 04 2015 03:32:24, hugovk commented: "> Bob will be a fish
Bob is a fish
Bob was a fish
Jill will be a goat
Jill is a goat
Jill was a goat
..."
On Wed Nov 04 2015 03:41:55, hugovk commented: "> Nothing more happened on the passage worthy the mentioning; so, after a fine run, we safely arrived in Neotucket."
On Wed Nov 04 2015 04:01:21, marythought commented: "## DAY THIRD -- oh it is very late make that DAY FORTH

Just checking in with some sample output. I wasn't happy with the trees and bushes lists available to me, so I'm just inventing some instead. :D Done through second stanza, two to go!

Names list is 1,000 randomly generated names from [list of random names](http://listofrandomnames.com/) -- if anyone wants to be added, I'm happy to add you! (ps repo is [here](https://github.com/marythought/where-im-from))

I am from nightclubs,
from Mart and fundamentalism.
I am from the aisle under the common room.
(Navy blue, feminine,
it smelled like cranberry.)
I am from the tulip spruce
the yellow corkbark birch
whose diverse caps I remember
as if they were my own.

I'm from parsnip and statistics,
from Wendie and Marcelle,
I'm from the slam poets
and the smart-alecks,
from 'well done' and 'what'!
I'm from 'He was born with a gift of laughter'
with a corrosive porcupine
and four I can say myself."
On Wed Nov 04 2015 04:06:18, marythought commented: "this is cooooool"
On Wed Nov 04 2015 05:37:56, hugovk commented: "I like that you can still just about read the obscured ones.

Perhaps make the unobscured ones say something when read in isolation?"
On Wed Nov 04 2015 06:59:02, ojahnn commented: "Here's my [repo](https://github.com/ojahnn/jedesilbe).

Here's the [complete output](https://enigmabrot.de/NaNoGenMo15/jedesilbe/) (60610 words)."
On Wed Nov 04 2015 08:07:44, d-baker commented: "I actually kind of did this with my first nanogenmo (I called it a "degenerative novel") and because I've already done it I'm not keen to do it again...I do like the idea of increasing the amount of code and error stuff towards the end though, I'll just need to find a way to make it sufficiently different to my approach last time. thanks for the suggestion! "
On Wed Nov 04 2015 08:10:32, d-baker commented: "here are some screenshots of bits I've liked in the output so far. I'm still working on building up the corpus and figuring out which things I should keep and which things to remove because they don't fit with the style...also, I haven't got round to adding code to the corpus yet, just errors I wrote myself.

![allkinda](https://cloud.githubusercontent.com/assets/5800214/10938829/49dc98f8-8351-11e5-9028-03989759980f.jpg)
![arisk](https://cloud.githubusercontent.com/assets/5800214/10938836/5391e182-8351-11e5-9ad7-3d6d12b6474a.jpg)
![arrayoutofbounds](https://cloud.githubusercontent.com/assets/5800214/10938841/57ac7b7e-8351-11e5-8a2c-b4114c847f9a.jpg)
"
On Wed Nov 04 2015 08:12:41, d-baker commented: "I love this!"
On Wed Nov 04 2015 08:18:20, enkiv2 commented: "I agree. Many small works does, however, counteract the problem of trying
to bake in internal consistency and continuity -- and keeping consistency
and continuity forces generators to be simplified or constrained in ways
that can produce more boring text at novel length. So, having an anthology
format can indirectly help in preventing the novel from being boring or
formulaic, by explaining away the existence of novelty sources that outside
an anthology format might be seen as confusing or an error (in the same way
that explicitly nonchronological stories, extremely abstract and vague
prose, swapping out narrators, or having unreliable narrators can).

On Wed, Nov 4, 2015 at 12:59 AM Chris Pressey <notifications@github.com>
wrote:

> Yes, but I don't think the "many small works" strategy is actually very
> successful at side-stepping the "boredom problem"... if anything, it might
> be even harder to read all the way through a list of 500 generated recipes,
> than through a single generated story of the same length.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/98#issuecomment-153421012>
> .
>
"
On Wed Nov 04 2015 09:04:57, tra38 commented: ">Danae Marks (a White blue-collar laborer) entered a church, ready to deliver a speech to religious people. She saw religion as a progressive force in society, but one that has engaged in a horrible crime: interfering in the secular world and trying to sway the national government. This violation of the separation of church and state cannot stand! 

>Danae Marks used practical experience and common sense to make the claim that the universe is controlled by a magical force called 'Karma'. Danae Marks argued that Karma was a personal, physical being. She then held out a personal copy of the Bible. This 'Bible' came directly from Karma, but ignorant people have chosen to interpret the book literally. A literal interpretion is one that is doomed to failure. Only metaphors lead to success. Danae Marks smiled when she explained how Heaven and Hell are the same as the 'religious' people viewed it: good people go to Heaven and bad people go to Hell. Danae Marks then explained in depth how Karma is able to break scientific laws from time to time, causing 'miracles'. Danae Marks declared that Karma strongly supports clear ethical standards that all humans must obey. 

>The theists took notes."
On Wed Nov 04 2015 09:06:44, cpressey commented: "Update: it generates a story.  It is terrible.  I do hope Goal 1 didn't get anyone's hopes up.  I did call it "unrealistic" and "incredibly unrealistic" in almost immediate succession...

Actually, suppose we reframe Goal 1 slightly, with gradation instead of as a yes-or-no thing.  *How many* words of the average NaNoGenMo text is the average reader willing to read, on average, before they give up?  By "read" I of course mean, try to make sense of the words, not just look at them.

For texts that are complete word salad, the number is probably well below 100.  (and then you start skimming forward, maybe, looking for interesting nonsense.)  For others, maybe higher.  A couple of hundred, at a guess.  Hard to say, without going to the ridiculous length of actually conducting experiments on it.

Anecdotes welcome, though!"
On Wed Nov 04 2015 09:11:22, enkiv2 commented: "One of the reasons I did generative erotica is that people will, on
average, be more entertained with less coherent erotica -- the subject
matter is either purient or funny. As a result, a fairly simple and
low-quality grammar produces a result that I was willing to read several
pages of. I suspect that there are other tricks with regard to style or
subject matter that work similarly to increase the readability of content
irrespective of novelty or quality. (For instance, vague yet evocative
sentences like those used in Monfort's 1k generators would be great if you
could consistently generate them!)

On Wed, Nov 4, 2015 at 9:06 AM Chris Pressey <notifications@github.com>
wrote:

> Update: it generates a story. It is terrible. I do hope Goal 1 didn't get
> anyone's hopes up. I did call it "unrealistic" and "incredibly unrealistic"
> in almost immediate succession...
>
> Actually, suppose we reframe Goal 1 slightly, with gradation instead of as
> a yes-or-no thing. *How many* words of the average NaNoGenMo text is the
> average reader willing to read, on average, before they give up? By "read"
> I of course mean, try to make sense of the words, not just look at them.
>
> For texts that are complete word salad, the number is probably well below
> 100. (and then you start skimming forward, maybe, looking for interesting
> nonsense.) For others, maybe higher. A couple of hundred, at a guess. Hard
> to say, without going to the ridiculous length of actually conducting
> experiments on it.
>
> Anecdotes welcome, though!
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/11#issuecomment-153729525>
> .
>
"
On Wed Nov 04 2015 09:15:54, cpressey commented: "Several pages, meaning, what, about 1500 words?"
On Wed Nov 04 2015 09:17:32, enkiv2 commented: "Yeah, something like that. (I have a fairly high tolerance for this stuff,
though. You can analyse it for yourself:
https://github.com/enkiv2/NaNoGenMo-2015/blob/master/orgasmotron.md )

On Wed, Nov 4, 2015 at 9:15 AM Chris Pressey <notifications@github.com>
wrote:

> Several pages, meaning, what, about 1500 words?
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/11#issuecomment-153732986>
> .
>
"
On Wed Nov 04 2015 09:17:45, tra38 commented: "For non-simulations, my guess is that the attention span starts drifting at '3*templates', where templates are the number of words within the template in question. It's enough for the user to gets bored because he grasps the pattern. So if you get a template that is 500 words in length, then that would probably make your 1500 words.

>(and then you start skimming forward, maybe, looking for interesting nonsense.)

It seems bots are excellent at generating text, but it's the humans who are trying to shift through the resulting nonsense to find actual meaning and worth. There has to be a mathematical formula that can be used to measure the 'fitness' of a text, allowing the bots to engage in filtering for how interesting* it is. This way, you can have the bots generate a bunch of words and then engage in automatic curation.

*We can define 'interesting' perhaps by sentiment analysis or how well it matches one of Vonnegut's plot curves. Or maybe, pull in machine learning. You rate a passage the computer generates on a scale of 1-10, and with enough data, eventually the computer will find a pattern."
On Wed Nov 04 2015 09:24:14, hugovk commented: "Many small works taken to the extreme in https://github.com/dariusk/NaNoGenMo-2015/issues/78:
https://hugovk.github.io/NaNoGenMo-2015/8334/8334.html

"
On Wed Nov 04 2015 09:28:29, hugovk commented: "> There has to be a mathematical formula that can be used to measure the 'fitness' of a text, allowing the bots to engage in filtering for how interesting* it is. 

Brings to mind genetic algorithms. Has anyone tried that approach?"
On Wed Nov 04 2015 09:30:13, enkiv2 commented: "With regard to the 'mathematical formula', I suspect you could use
Shannon's information entropy formula with the prior of the reader's mind
:P. After all, humans accept only a narrow band of novelty, and what counts
as novel depends upon what the reader has seen before.

On Wed, Nov 4, 2015 at 9:17 AM Tariq Ali <notifications@github.com> wrote:

> For non-simulations, my guess is that the attention span starts drifting
> at '3*templates', where templates are the number of words within the
> template in question. It's enough for the user to gets bored because he
> grasps the pattern. So if you get a template that is 500 words in length,
> then that would probably make your 1500 words.
>
> (and then you start skimming forward, maybe, looking for interesting
> nonsense.)
>
> It seems bots are excellent at generating text, but it's the humans who
> are trying to shift through the resulting nonsense to find actual meaning
> and worth. There has to be a mathematical formula that can be used to
> measure the 'fitness' of a text, allowing the bots to engage in filtering
> for how interesting it is. This way, you can have the bots generate a bunch
> of words and then engage in automatic curation.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/11#issuecomment-153734075>
> .
>
"
On Wed Nov 04 2015 09:45:49, ikarth commented: "Humans figuring out patterns seems to be part of the interestingness metric. It seems to work on multiple scales: Groking the central conceit in [Aggressive Passive](http://aaronareed.net/if/NaNoGenMo13/sample.html) or [Redwreath and Goldstar Have Traveled to Deathsgate](http://eblong.com/zarf/essays/r-and-g.html) takes a few minutes at most, which will give you the sense of the overall plot without reading it. (And then figuring out the puzzle of which question goes with which answer can take a lifetime.) 

Something like #72 or [Alice's Adventures in the Whale](https://github.com/dariusk/NaNoGenMo/issues/69) takes a bit longer, because once you've grasped the pattern, the pleasure is in seeing the changes that were made in a familiar text.

I suspect that simulations play by slightly different rules. Dwarf Fortress has certainly generated a lot of stories, though I'm not sure how many of them are interesting precisely because they were interactive. (Not to mention, most renditions are a retelling of the events, rather than a direct output.) I'm going to be watching this year's simulation results with interest.

One pleasure that most generative works lack is a sense that _an author intended them to happen this way_. Not that you can't get a degree of intention-sense. I suspect that's why high-concept things like Aggressive Passive work so well: we can read the higher authorial intent, and that makes it easier to get closure and catharsis."
On Wed Nov 04 2015 09:47:40, ikarth commented: "@hugovk @enkiv2 Maybe generate a large corpus of generated results (with the metadata for the generator settings), use a crowdsourced interestingness vote, and then use that as the fitness criteria for an RNN?"
On Wed Nov 04 2015 09:52:58, enkiv2 commented: "Honestly, I'd love to do that (and there are some similar systems out
there). But, it sort of requires exposure (or mechanical turk!), so it
might be hard to do this at novel length in a month, since it requires some
large number of people to read more than a novel-length quantity of
generated content in a month.

(I mean, you could do this with a markov chain rather than an RNN too. Do
monte carlo and generate a handful of 'next sentences' and have people vote
on which one is best, then feed the winner back in as training data or
increment its connections.)

On Wed, Nov 4, 2015 at 9:47 AM Isaac Karth <notifications@github.com> wrote:

> @hugovk <https://github.com/hugovk> @enkiv2 <https://github.com/enkiv2>
> Maybe generate a large corpus of generated results (with the metadata for
> the generator settings), use a crowdsourced interestingness vote, and then
> use that as the fitness criteria for an RNN?
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/11#issuecomment-153749253>
> .
>
"
On Wed Nov 04 2015 09:53:31, MichaelPaulukonis commented: "@hugovk - didn't we talk about this in the GenerativeText list? The trouble is the fitness algorithm -- if you've got one, well -- you've solved the problem. Otherwise, we're talking about using human readers via Amazon's Mechanical Turk or something. 

Nothing that we little people could handle (:money:), but maybe in a few years somebody can think of a sneaky way to grab eyeballs with something like ReCaptcha, or Facebook will heave its vast and labyrinthine bulk in its direction.

@enkiv2 - For similar reasons new directors often work with low-budget horror movies. Witness Sam Raimi - he did _Evil Dead_ not for any particular love of the genre, but for most-likely return on investment (time+money) ([a source](http://www.ign.com/articles/2015/10/30/sam-raimi-gives-us-an-evil-dead-history-lesson?page=2)). The audience tends to eat it up no matter how low the quality. Witness the large numbers of self-published zombie books on Amazon. Or romance novels of any of the vast, arcane genotypes of romance.

So - generated horror fiction. SplatterGenPunk. Note to self -- add this to #14"
On Wed Nov 04 2015 10:01:03, enkiv2 commented: "It's not as though there aren't people who will do that for free (see
crowdsound, darwintunes, basically every quote DB, and that one project
where a novelist is having randoms vote on his plot elements, along with
twitch plays anything). But, getting that audience isn't guaranteed and it
takes a while. If we didn't care about November, we could start a thing
like that and then just let people discover and play with it as they will.

On Wed, Nov 4, 2015 at 9:53 AM Michael Paulukonis <notifications@github.com>
wrote:

> @hugovk <https://github.com/hugovk> - didn't we talk about this in the
> GenerativeText list? The trouble is the fitness algorithm -- if you've got
> one, well -- you've solved the problem. Otherwise, we're talking about
> using human readers via Amazon's Mechanical Turk or something.
>
> Nothing that we little people could handle (:money:), but maybe in a few
> years somebody can think of a sneaky way to grab eyeballs with something
> like ReCaptcha, or Facebook will heave its vast and labyrinthine bulk in
> its direction.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/11#issuecomment-153751729>
> .
>
"
On Wed Nov 04 2015 11:14:17, suisea commented: "iluuuuuuuuuu"
On Wed Nov 04 2015 11:24:42, suisea opened a new issue called "~ time machine // generated/ive poetry ~".
On Wed Nov 04 2015 16:38:53, dariusk commented: "Here's a complete book, though it's not quite 50,000 words (it's more like 48k).

http://tinysubversions.com/nanogenmo/2015/harpooneers/

_NOT_ my official entry for this since it doesn't meet the length requirement."
On Wed Nov 04 2015 16:46:16, estayton opened a new issue called "Intent to participate".
On Wed Nov 04 2015 16:58:27, dariusk commented: "Err, updated with the source code. I'm sorry: https://github.com/dariusk/harpooneers"
On Wed Nov 04 2015 18:49:58, brentroady opened a new issue called "Hodor!".
On Wed Nov 04 2015 18:57:34, dariusk commented: "Something similar @hugovk did:

https://github.com/hugovk/meow.py
"
On Wed Nov 04 2015 19:34:52, lizrush commented: "Rad, I like this idea. Reminds me of Jeff Thompson's [Recation Bot](https://twitter.com/tweetredactor).
It would be super cool to figure out how to make the remaining parts that are not redacted make readable sentences!"
On Wed Nov 04 2015 20:35:40, jseakle commented: "@lizrush interesting, I hadn't seen that! 

And yes, I'm currently working on strategies for making the poems more poem-like, the posted image is mostly a proof of concept for the styling, using completely randomly chosen words."
On Wed Nov 04 2015 21:23:19, yourpalal opened a new issue called "Generative Socratic Dialogues".
On Wed Nov 04 2015 21:23:56, yourpalal commented: "I'm currently working at generating somewhat readable phrases, and just generated the phrase "But it was eventually abandoned." Even though my computer thinks I will give up, I'm going to do my best not to."
On Wed Nov 04 2015 21:27:58, yourpalal commented: "I'm also hoping to have the output compiled into PDF by LaTeX, to up the academic cred of the work."
On Wed Nov 04 2015 22:28:18, marythought commented: "## DAY FOUR (FOR REAL)

We have a completed poem!

_Where I'm From_

I am from birthdays,
from Big Mac and collectivization.
I am from the trot under the storm cellar.
(Cerise, thirdquarter,
it tasted like jackfruit.)
I am from the stinking cottonwood
the tan lilac yew
whose emerald hares I remember
as if they were my own.

I'm from celery and byproducts,
from Fransisca and Scottie.
I'm from the slam poets
and the mean girls,
from 'hallelujah' and 'just kidding'!
I'm from 'Call me Ishmael'
'It does not matter how slowly you go so long as you do not stop'
and four pamphlets I can say myself.

I'm from South Gate and Beaverton,
shredded banana squash and cooling smoothie.
From the neck my stepsister sewed
in a football game,
the thumb my mum trailed to keep their smell.

Above my tea cart was a aft box
holding soft frictions,
a sift of lost faces
to drift around my dreams.
I am from those moments--
brooded before I dabbled--
leaf-fall from the family tree.

For the next step I can go one of (at least) two ways:
* Try to set this up via html with a "generate poem" button for sharing
* Not do that^^, because JavaScript script requiring/sharing is ridiculous and I don't fully (or even partially, at this point) understand it. But I can get console output so I could just stop there and worry about the text and not the presentation.
* But seriously, I'm looking for a job so I should get this out there in a presentable manner
* ARGH JavaScript. ARRGH Binary trees!!! Did you see how I invented my own trees in the script above? Heh heh "stinking cottonwood." No binary trees allowed. 
* Ok, real talk, I'm going to explore methods of text generation based on a structure like, I dunno, Little House on the Prairie maybe, and including the keywords from the poems above. Then my "chapters" become the poem followed by a narrative, presumably by or about the speaker from the poem. Yes this seems doable!!"
On Thu Nov 05 2015 00:01:54, bredfern commented: "Yeah its a little more Finnigan's Wake meets Lovecraft right now until I
get my neural network better trained lol.

On Thu, Oct 29, 2015 at 12:18 PM, John Ohno <notifications@github.com>
wrote:

> Looking forward to it!
>
> On Thu, Oct 29, 2015 at 3:13 PM Brian Redfern <notifications@github.com>
> wrote:
>
> > Well that's the idea but likely going to be very naked lunch esque, an
> > excuse to attempt something really interesting with node + react
> >
> > —
> > Reply to this email directly or view it on GitHub
> > <https://github.com/dariusk/NaNoGenMo-2015/issues/51>.
> >
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-152292250>
> .
>
"
On Thu Nov 05 2015 01:23:54, toomuchpete opened a new issue called "Who Lives in a Pineapple Under the Sea? MIS-TER DAR-CY!".
On Thu Nov 05 2015 01:26:11, toomuchpete commented: "As requested, @kyfast."
On Thu Nov 05 2015 01:49:08, iangonzalez opened a new issue called "Intent to participate".
On Thu Nov 05 2015 03:31:57, jseakle opened a new issue called "Interlude: (Un)Sound Structures". And it's been completed. Sweet!
On Thu Nov 05 2015 03:57:52, hugovk commented: "Have a COMPLETED!

> Merrsoncs Â© 2015 sr Tiyrinshmi T. Hifasho
"
On Thu Nov 05 2015 06:15:02, suisea commented: "> at the moment I'm basically finding nice replacements for various sea-related words.

ruuuuude"
On Thu Nov 05 2015 07:00:34, cpressey commented: "Nice!  If, in the next few weeks, you want to take another break, you could throw [parts of] this through a voice synthesizer -- seems fitting for a sound poem -- and submit it to [NaOpGenMo](https://github.com/cpressey/NaOpGenMo-2015)..."
On Thu Nov 05 2015 08:04:23, tra38 commented: ">Anyways, I hope whoever looks at it enjoys it, cause I had fun making it!

I enjoyed it, though said enjoyment did require me to skim through the entire text. That's usually on par with most NaNoGenMo simulated works though. Good job on your first program. May you make others in the future."
On Thu Nov 05 2015 08:26:39, hugovk commented: "Linking to Pastebin is totally fine, we're only using GitHub as a forum. I can see the code files and book so looks like you figured out Pastebin :)

No-one here minds bad code, there'll be plenty being written in this short month. Anyway, code looks fine on a quick skim, and the output is what matters, and I like it:

> You're attacked by a warlock! More than one!
 A warlock hit The long haired one! It was a super weak attack.
 The long haired one killed the warlock with power! A warlock hit The deep sounding one!
 It was a super weak attack. The deep sounding one killed the warlock with wit!
 The enemies are gone now. What a fight!

I like the repetition too:

> The long haired one thinks she sees people! The long haired one is worried they might be bad people.
 The deep sounding one thinks the group should try and talk to them. Our group leaves the people alone.
 The long haired one thinks she sees people! The long haired one is worried they might be bad people.
 The deep sounding one thinks the group should try and talk to them. Our group leaves the people alone.
 The long haired one thinks she sees people! The long haired one is worried they might be bad people.
 The deep sounding one thinks the group should try and talk to them. Our group heads towards the people.
 Our group asks the people if they know anything about the pit. They ask Our group if we're sure we want to know.
 Our group says yes. If you really must know, they say, I know the pit is off far away in that direction.
 A few of them point off in the distance. Our group thanks them for the help and walks where they pointed.
 What nice people! "
On Thu Nov 05 2015 08:32:38, hugovk commented: "Of the [completed](https://github.com/dariusk/NaNoGenMo-2015/issues?q=is%3Aopen+is%3Aissue+label%3Acompleted) so far:

C#: https://github.com/dariusk/NaNoGenMo-2015/issues/98
Prolog: https://github.com/dariusk/NaNoGenMo-2015/issues/125
Python: https://github.com/dariusk/NaNoGenMo-2015/issues/72 https://github.com/dariusk/NaNoGenMo-2015/issues/78 https://github.com/dariusk/NaNoGenMo-2015/issues/104 https://github.com/dariusk/NaNoGenMo-2015/issues/116 https://github.com/dariusk/NaNoGenMo-2015/issues/117 https://github.com/dariusk/NaNoGenMo-2015/issues/135

"
On Thu Nov 05 2015 08:35:02, mattfister commented: "This is great - nice job!"
On Thu Nov 05 2015 09:24:46, coleww opened a new issue called "The TPP: A "Found" Generated Novel". And it's been completed. Sweet!
On Thu Nov 05 2015 09:25:28, coleww commented: "I was going to suggest the TPP as a corpus, but as I started reading it I realized the work was already complete!"
On Thu Nov 05 2015 09:42:53, dariusk commented: "Lovely!"
On Thu Nov 05 2015 09:43:37, dariusk commented: "Love it."
On Thu Nov 05 2015 09:44:40, dariusk commented: "@hugovk I was considering marking this as complete since it comes awfully close and does include the source code... what do you think?"
On Thu Nov 05 2015 09:45:09, dariusk commented: "I love the idea."
On Thu Nov 05 2015 09:45:40, cpressey commented: "> (a) an originating good of another Party, individually, is being imported into the Party’s territory in such increased quantities, in absolute terms or relative to domestic production, and under such conditions,
as to cause or threaten to cause serious injury to the domestic industry that produces a like or directly competitive good; or (b) an originating good of two or more Parties, collectively, is being imported into the 
Party’s territory in such increased quantities, in absolute terms or relative to domestic production, and under such conditions, as to cause or threaten to cause serious injury to the domestic industry that produces a like or directly competitive good, provided that the Party applying the transitional safeguard measure demonstrates, with respect to the imports from each such Party against which the transitional safeguard measure is applied, that imports of the originating good from each of those Parties have 
increased, in absolute terms or relative to domestic production, since the date of entry into force of this 
Agreement for those Parties

Certainly sounds like the result of random template expansion to me!"
On Thu Nov 05 2015 09:46:17, MichaelPaulukonis commented: "@jseakle - are you familiar with Tom Philips' [A Humument?](http://www.tomphillips.co.uk/humument)?

![humument example](https://cloud.githubusercontent.com/assets/2607898/10971318/b9644bee-83a1-11e5-91ea-d432c324e071.jpg)

Not to focus on the image-patterns, but could you create whitespace-rivers between the elements? Or lines, at least, to connect the "trail of thought" ?

I used to hand-paint my own - black-backgrounds only. No scanned examples.
"
On Thu Nov 05 2015 09:49:31, KyFaSt commented: ":pineapple: "
On Thu Nov 05 2015 09:50:58, MichaelPaulukonis commented: "> Finnigan's Wake meets Lovecraft


aaaaand, why would it need further training? !!!

> riverrun, past Abhoth and C'thalpa's, from non-euclidean swerve of shore to hideous bend of abominable pre-human swamp, brings us by an incomprehensible commodius vicus of recirculation back to Leng Plateau and Environs."
On Thu Nov 05 2015 09:53:04, MichaelPaulukonis commented: "I like this."
On Thu Nov 05 2015 09:54:45, MichaelPaulukonis commented: "There's been dialogue swapping in the past, and I did character/noun swapping between two texts as well. But nobody has tackled the problem of getting references straight. I thought about it as one of my projects this year, but don't know if I'll get to it.

I won't be sad if you do the work for the rest of us!"
On Thu Nov 05 2015 09:58:15, hugovk commented: "If you include the source code (but not the corpora) in an appendix in the back of the final, printed volume it's almost 52k.

Congratulations, have a Completed label!"
On Thu Nov 05 2015 10:02:47, hugovk commented: "Haha!

(@dariusk Should the "preview" label be removed, as "completed" kind of trumps it?)"
On Thu Nov 05 2015 10:04:19, enkiv2 commented: "The word2vec-related projects have managed to translate references. If you
make an explicit list of proper names in each source, you can probably make
an explicit translation or use word2vec to produce correspondences for you.

On Thu, Nov 5, 2015 at 9:54 AM Michael Paulukonis <notifications@github.com>
wrote:

> There's been dialogue swapping in the past, and I did character/noun
> swapping between two texts as well. But nobody has tackled the problem of
> getting references straight. I thought about it as one of my projects this
> year, but don't know if I'll get to it.
>
> I won't be sad if you do the work for the rest of us!
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/133#issuecomment-154083202>
> .
>
"
On Thu Nov 05 2015 10:23:39, nicholasg3 opened a new issue called "Automatic Essay Grading + (Markov Chains | Genetic Algorithm) = Novel?".
On Thu Nov 05 2015 10:25:32, cpressey commented: "I like this a lot!  Good use of tropes.  I wasn't bored by it at all!"
On Thu Nov 05 2015 10:30:34, bredfern commented: "Here's a sample of what I have so far:

 "Welcog. I have found room that if in crimbling stone-like and more
Cwrongen had curiously insectures accaisions - ruck their bits-rathheres
had graned materials, companions and tower aperthing and sinist-opashing of
the more shapes, all youngle sort my net other contrictify, and of light,
and of this length of agevent clesting that other promito, beforeward three
membersalay even questions, we spate, the fuss which by the base I race no
rock place on feet away with said:-"

On Thu, Nov 5, 2015 at 6:51 AM, Michael Paulukonis <notifications@github.com
> wrote:

> Finnigan's Wake meets Lovecraft
>
> aaaaand, why would it need further training? !!!
>
> riverrun, past Abhoth and C'thalpa's, from non-euclidean swerve of shore
> to hideous bend of abominable pre-human swamp, brings us by an
> incomprehensible commodius vicus of recirculation back to Leng Plateau and
> Environs.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-154081850>
> .
>
"
On Thu Nov 05 2015 10:37:29, bredfern commented: "I think it turns out better when I have only Lovecraft text, I mixed in the
text off of Dracula and it watered down my model lol.

Although I may just roll with the 900 page text I generated from a
combination of lovecraft and crowley, thinking of hooking this into a text
to speech and a widget that is pulling images off google image search and
displaying that while the text is being spoken by the computer voice.

On Thu, Nov 5, 2015 at 7:31 AM, Brian Redfern <brianwredfern@gmail.com>
wrote:

> Here's a sample of what I have so far:
>
>  "Welcog. I have found room that if in crimbling stone-like and more
> Cwrongen had curiously insectures accaisions - ruck their bits-rathheres
> had graned materials, companions and tower aperthing and sinist-opashing of
> the more shapes, all youngle sort my net other contrictify, and of light,
> and of this length of agevent clesting that other promito, beforeward three
> membersalay even questions, we spate, the fuss which by the base I race no
> rock place on feet away with said:-"
>
> On Thu, Nov 5, 2015 at 6:51 AM, Michael Paulukonis <
> notifications@github.com> wrote:
>
>> Finnigan's Wake meets Lovecraft
>>
>> aaaaand, why would it need further training? !!!
>>
>> riverrun, past Abhoth and C'thalpa's, from non-euclidean swerve of shore
>> to hideous bend of abominable pre-human swamp, brings us by an
>> incomprehensible commodius vicus of recirculation back to Leng Plateau and
>> Environs.
>>
>> —
>> Reply to this email directly or view it on GitHub
>> <https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-154081850>
>> .
>>
>
>
"
On Thu Nov 05 2015 10:37:41, enkiv2 commented: "I guess you could do it like a genetic algorithm: produce random essays and
grade them, and then somehow combine the winners?

On Thu, Nov 5, 2015 at 10:23 AM Nicholas Garcia <notifications@github.com>
wrote:

> My research project involves building an automatic essay scoring system.
> Can it be "run backwards" to get some text? Or at least throw interesting
> text at it to see what it thinks should be a good answer. Text might be
> generated either with markov chains or combining other corpuses.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/137>.
>
"
On Thu Nov 05 2015 10:40:49, MichaelPaulukonis commented: "Looks like it's working with letters atomically, instead of words. I'm not familiar with the model, but can you change the atomicity?

Not that I don't like the output."
On Thu Nov 05 2015 10:42:06, dariusk commented: "@hugovk I'm kind of using "preview" as a shorthand for "there is an excerpt somewhere in the thread", so people who just want to browse snippets can do that. I'm imagining a case where someone wants to look for threads where they don't have to download PDFs or whatever."
On Thu Nov 05 2015 10:42:46, MichaelPaulukonis commented: "I would be intrigued to see this work; one problem is eponyms, nicknames, gender-references, and titles. "King" posed a particular problem for me, as the pos-tagger I was using always decided it was a verb. @enkiv2 - can you link to one or more projects that managed to translate references?"
On Thu Nov 05 2015 10:46:10, dariusk commented: "Nice one!"
On Thu Nov 05 2015 10:50:20, bredfern commented: "Yeah its a statistical model that goes character by character, so I need to
run it a lot longer with much higher temperature settings, it needs a
bigger model too I need to scrape every single lovecraft text into one data
file, right now its only a couple things in there once it has every single
lovecraft text in one file and runs that at high temp it should produce a
better result.

On Thu, Nov 5, 2015 at 7:40 AM, Michael Paulukonis <notifications@github.com
> wrote:

> Looks like it's working with letters atomically, instead of words. I'm not
> familiar with the model, but can you change the atomicity?
>
> Not that I don't like the output.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-154097040>
> .
>
"
On Thu Nov 05 2015 10:52:16, enkiv2 commented: "Does this use char-rnn? I rather like the letter-granularity of the input
(and RNNs do better at producing real words out of character-granularity
input with relatively small training data than, say, third- or fourth-order
markov chains do), but it looks like you'd get more coherence if you
doubled or tripled the input.

If you're looking for something closer to the Lovecraft end of the style
spectrum than the Dracula end, you might look into some of the late
eighteenth century authors of weird fiction that Lovecraft aped: Algernon
Blackwood, William Hope Hodgeson, & Robert Chambers.

On Thu, Nov 5, 2015 at 10:40 AM Michael Paulukonis <notifications@github.com>
wrote:

> Looks like it's working with letters atomically, instead of words. I'm not
> familiar with the model, but can you change the atomicity?
>
> Not that I don't like the output.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-154097040>
> .
>
"
On Thu Nov 05 2015 10:56:40, enkiv2 commented: "Take a look at the translated titles and authors in
https://github.com/dariusk/NaNoGenMo-2015/issues/72 ; this is what I mean.
Word2vec correctly figured out that certain proper nouns were similar in
the same way that it figured out that certain nouns are similar in general,
from what I understand. If you whitelist proper nouns and have an explicit
list of identical ways of referring to the same person which you normalize,
you can do that with better reliability, but at that point you've done most
of the work of creating a correspondence table between sets of characters
and you might as well just do string replacement on them.

On Thu, Nov 5, 2015 at 10:46 AM Michael Paulukonis <notifications@github.com>
wrote:

> I would be intrigued to see this work; one problem is eponyms, nicknames,
> gender-references, and titles. "King" posed a particular problem for me, as
> the pos-tagger I was using always decided it was a verb. @enkiv2
> <https://github.com/enkiv2> - can you link to one or more projects that
> managed to translate references?
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/133#issuecomment-154097571>
> .
>
"
On Thu Nov 05 2015 10:58:03, mewo2 commented: "I used the "standard" Google News model for most stuff. There's a "backup" model which was trained on about 100 Project Gutenberg books (including the source texts), which I use when there's a word which doesn't occur in the Google News dataset. That's usually either an unusual proper name, or something archaic."
On Thu Nov 05 2015 10:59:50, bredfern commented: "Yeah it uses char-nn so its all about the quality of the model going in, I
might need to write an extra piece of node code that scrapes the web for
sources and adds to the data file, lovecraft is out of copyright so its a
good choice, but yeah there are other great authors in this genre who are
not in copyright lockdown.

On Thu, Nov 5, 2015 at 7:52 AM, John Ohno <notifications@github.com> wrote:

> Does this use char-rnn? I rather like the letter-granularity of the input
> (and RNNs do better at producing real words out of character-granularity
> input with relatively small training data than, say, third- or fourth-order
> markov chains do), but it looks like you'd get more coherence if you
> doubled or tripled the input.
>
> If you're looking for something closer to the Lovecraft end of the style
> spectrum than the Dracula end, you might look into some of the late
> eighteenth century authors of weird fiction that Lovecraft aped: Algernon
> Blackwood, William Hope Hodgeson, & Robert Chambers.
>
> On Thu, Nov 5, 2015 at 10:40 AM Michael Paulukonis <
> notifications@github.com>
> wrote:
>
> > Looks like it's working with letters atomically, instead of words. I'm
> not
> > familiar with the model, but can you change the atomicity?
> >
> > Not that I don't like the output.
> >
> > —
> > Reply to this email directly or view it on GitHub
> > <
> https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-154097040
> >
>
> > .
> >
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-154100313>
> .
>
"
On Thu Nov 05 2015 11:27:14, jseakle commented: "Man, you ruined my cool surprise! :P

I had to get to sleep last night, but I was planning to do speech synthesis to this today. Didn't know about NaOpGenMo, though, that is neat and I will consider submitting.

..how long until National "National <ContentType> Generating Month" Generating Month?? "
On Thu Nov 05 2015 11:39:17, bredfern commented: "Now I have one data file with all of lovecraft's ficiton in it, the cool
thing is that with a single author model you don't get any nasty dropouts
in terms of training loss, when you try to mash of several authors you'll
hit a nasty training loss issue.

On Thu, Nov 5, 2015 at 7:59 AM, Brian Redfern <brianwredfern@gmail.com>
wrote:

> Yeah it uses char-nn so its all about the quality of the model going in, I
> might need to write an extra piece of node code that scrapes the web for
> sources and adds to the data file, lovecraft is out of copyright so its a
> good choice, but yeah there are other great authors in this genre who are
> not in copyright lockdown.
>
> On Thu, Nov 5, 2015 at 7:52 AM, John Ohno <notifications@github.com>
> wrote:
>
>> Does this use char-rnn? I rather like the letter-granularity of the input
>> (and RNNs do better at producing real words out of character-granularity
>> input with relatively small training data than, say, third- or
>> fourth-order
>> markov chains do), but it looks like you'd get more coherence if you
>> doubled or tripled the input.
>>
>> If you're looking for something closer to the Lovecraft end of the style
>> spectrum than the Dracula end, you might look into some of the late
>> eighteenth century authors of weird fiction that Lovecraft aped: Algernon
>> Blackwood, William Hope Hodgeson, & Robert Chambers.
>>
>> On Thu, Nov 5, 2015 at 10:40 AM Michael Paulukonis <
>> notifications@github.com>
>> wrote:
>>
>> > Looks like it's working with letters atomically, instead of words. I'm
>> not
>> > familiar with the model, but can you change the atomicity?
>> >
>> > Not that I don't like the output.
>> >
>> > —
>> > Reply to this email directly or view it on GitHub
>> > <
>> https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-154097040
>> >
>>
>> > .
>> >
>>
>> —
>> Reply to this email directly or view it on GitHub
>> <https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-154100313>
>> .
>>
>
>
"
On Thu Nov 05 2015 12:07:31, amanda opened a new issue called "images to text".
On Thu Nov 05 2015 12:20:25, coleww commented: "my current horrible notes on how to do this as a software:

--------------------------------------------------------------

function/class/objects2build

- catalog: controls the whole thing, maybe pulls in some "themes" from an api (google/twitter trends/news?) for each edition of the catalog (generate 1 a day?). creates a cover art with some sort of procedural spherical art generated. increments volume#s and such. Makes completely absurd contact info, staff names, etc. Creates a table of contents, index, glossary, etc.

- section: catalog has many sections. some are like, columns advice news, others are like, little sqare cells advertising products or containing poems or comics or etc. Each section is given seed data by the catalog based on the whatever api theme thing is going on.. each "section" exports an array of perfectly size html divs that are a "printed PDF page"

- cell: a section has many cells. might be text, image, comic, graph, idk. lotsa things. make like, dozens of these functions. idk, lil d3 charts, whatever. (BUT EVERYTHING should export to PDF and look reasonable)

HELPERS

- image grabber/processor: given a string, grab an image from the web, process it like gary in botALLY i.e, pick a few filters out of the hat and apply them with great strength, turn it B/W, and return a path to that image. Probably almost every cell will use this. Maybe sections should select a corpus of images for the cells to use. Maybe particular cells can apply extra filters to things

CORPUSES

- get more of them"
On Thu Nov 05 2015 13:21:23, cpressey commented: "Oh snap, sorry about that!  But it seemed like the obvious next step.

Yo dawg, I heard you liked National Generation Months, so I * *faints* *"
On Thu Nov 05 2015 14:29:03, SandroMiccoli opened a new issue called "Intent to participate - CHARLES BUKOWSKI ALL CAPS".
On Thu Nov 05 2015 16:01:43, mcwill97 commented: "Thanks everyone! I'm really glad you liked it! There's a ton of features I was going to add like more tests that check for the characters stats and repercussions other than death like limb loss and mutations, but it would've lead to a few more days of coding and I was a bit anxious to get back to coding my game since its nearing completion. I am really happy you guys liked it though, because this is the first time I've shown any of my work publicly. In particular I really like what happens when someone likes somebody else who hates them and they try to talk. ctrl - f for "heck" if interested"
On Thu Nov 05 2015 16:12:32, jseakle commented: "No worries :)

[Here's the poem!](http://www.fromtexttospeech.com/texttospeech_output_files/0910034001446757818/8910755.mp3)"
On Thu Nov 05 2015 16:38:16, bredfern commented: "Yeah now using every Lovecraft text as a source you get more interesting
output:

The Great Old Ones which he had been heard the strange college of the
strange part of the streets of the streets of the strange and
structures of the process of the substance of the things to the party,
and the face of the colour of the rest of the strange things that we
had been a surprising through the colour of the strange and strange
and parts of the strange plane of the strange and scrapter of the
secret of the chance of the strange and start of the shocking far as
it was a companion.


On Thu, Nov 5, 2015 at 8:39 AM, Brian Redfern <brianwredfern@gmail.com>
wrote:

> Now I have one data file with all of lovecraft's ficiton in it, the cool
> thing is that with a single author model you don't get any nasty dropouts
> in terms of training loss, when you try to mash of several authors you'll
> hit a nasty training loss issue.
>
> On Thu, Nov 5, 2015 at 7:59 AM, Brian Redfern <brianwredfern@gmail.com>
> wrote:
>
>> Yeah it uses char-nn so its all about the quality of the model going in,
>> I might need to write an extra piece of node code that scrapes the web for
>> sources and adds to the data file, lovecraft is out of copyright so its a
>> good choice, but yeah there are other great authors in this genre who are
>> not in copyright lockdown.
>>
>> On Thu, Nov 5, 2015 at 7:52 AM, John Ohno <notifications@github.com>
>> wrote:
>>
>>> Does this use char-rnn? I rather like the letter-granularity of the input
>>> (and RNNs do better at producing real words out of character-granularity
>>> input with relatively small training data than, say, third- or
>>> fourth-order
>>> markov chains do), but it looks like you'd get more coherence if you
>>> doubled or tripled the input.
>>>
>>> If you're looking for something closer to the Lovecraft end of the style
>>> spectrum than the Dracula end, you might look into some of the late
>>> eighteenth century authors of weird fiction that Lovecraft aped: Algernon
>>> Blackwood, William Hope Hodgeson, & Robert Chambers.
>>>
>>> On Thu, Nov 5, 2015 at 10:40 AM Michael Paulukonis <
>>> notifications@github.com>
>>> wrote:
>>>
>>> > Looks like it's working with letters atomically, instead of words. I'm
>>> not
>>> > familiar with the model, but can you change the atomicity?
>>> >
>>> > Not that I don't like the output.
>>> >
>>> > —
>>> > Reply to this email directly or view it on GitHub
>>> > <
>>> https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-154097040
>>> >
>>>
>>> > .
>>> >
>>>
>>> —
>>> Reply to this email directly or view it on GitHub
>>> <https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-154100313>
>>> .
>>>
>>
>>
>
"
On Thu Nov 05 2015 18:43:41, greg-kennedy commented: "Setting up a bunch of classes in Perl, the One True Language.  Lots of stuff filled from corpora.  So far I can set up the initial state of the "world".

# The World at Wed Dec 31 18:00:00 1969
  The 0th place in our story is the 'dressing room'.  It appears nondescript.
  There is a wireless control here.  It appears nondescript.  There is a wallet here.  It appears nondescript.  There is a rat here.  It appears nondescript.  There is a ice cream stick here.  It appears nondescript.  There is a dog here.  It appears nondescript.  There is a bottle of glue here.  It appears nondescript.  There is a soccer ball here.  It appears nondescript.  There is a hair clip here.  It appears nondescript.  There is a desk here.  It appears nondescript.  There is a matchbook here.  It appears nondescript.  
  Kaylee Carter is also here.  She appears nondescript.  Kaylee is holding a nondescript nail clippers.  Kaylee is holding a nondescript bag of rubber bands.  Kaylee is holding a nondescript cars.  

  The 1th place in our story is the 'staff room'.  It appears nondescript.
  There is a package of glitter here.  It appears nondescript.  There is a glass here.  It appears nondescript.  There is a acorn here.  It appears nondescript.  There is a magnifying glass here.  It appears nondescript.  There is a game CD here.  It appears nondescript.  There is a keychain here.  It appears nondescript.  There is a pocketknife here.  It appears nondescript.  There is a paperclip here.  It appears nondescript.  There is a apple here.  It appears nondescript.  There is a rabbit here.  It appears nondescript.  

  The 2th place in our story is the 'computer lab'.  It appears nondescript.
  There is a pail here.  It appears nondescript.  There is a jar of jam here.  It appears nondescript.  There is a drawer here.  It appears nondescript.  There is a mobile phone here.  It appears nondescript.  There is a towel here.  It appears nondescript.  There is a credit card here.  It appears nondescript.  There is a canteen here.  It appears nondescript.  There is a plate here.  It appears nondescript.  
  Oscar Young is also here.  He appears nondescript.  Oscar is holding a nondescript plastic fork.  Oscar is holding a nondescript cork.  Oscar is holding a nondescript camera.  Oscar is holding a nondescript acorn.  Oscar is holding a nondescript pair of tongs.  
  Amy Perez is also here.  She appears nondescript.  Amy is holding a nondescript lotion.  Amy is holding a nondescript lamp shade.  

  The 3th place in our story is the 'gym'.  It appears nondescript.
...
"
On Thu Nov 05 2015 19:29:17, longears commented: "This reminds me of the recent Neural Style algorithm which uses neural nets to copy artistic style from one image to another (e.g. to make a photo look like a Picasso painting).

https://github.com/jcjohnson/neural-style
try your own images here: https://dreamscopeapp.com/editor

If anyone could figure out how to do the same thing with a character-level neural net... :)

https://github.com/karpathy/char-rnn"
On Thu Nov 05 2015 21:15:28, nadavoosh opened a new issue called "Statement of intent".
On Thu Nov 05 2015 21:16:09, nadavoosh commented: "If anyone knows anything about the novel I'll be generating, please let me know. "
On Thu Nov 05 2015 21:18:20, ikarth commented: "I am severely tempted to try that, since one of my near-term goals is "learn enough about neural nets to play around with them.""
On Thu Nov 05 2015 21:28:33, ikarth commented: "My Gutenberg Shuffle from 2013 attempted to respect references, but it turned out to be a bigger project than anticipwords.It sort of got gender right, though I'd redo it if I went that way again.

Note that, at least for the libraries in gensim, pos-taggers work better on sentences rather than individual words."
On Thu Nov 05 2015 23:08:15, MichaelPaulukonis commented: "So, [here's a sample](https://gist.github.com/MichaelPaulukonis/3af142f787db2908d3f0).

Not terribly exciting.

First proof of concept. It's all the boring templating.

I want some better stuff for the locales, use of powers, dealing with the finality -- the flow from one thing to the next is awkward. Plus, it would be nice if the egg shifted around a bit.
"
On Fri Nov 06 2015 00:37:20, nadavoosh commented: "this is great! Nice job. "
On Fri Nov 06 2015 05:06:34, jseakle opened a new issue called "Interlude 2: Worldbuilding In The Twenty-Teens". And it's been completed. Sweet!
On Fri Nov 06 2015 09:15:48, MichaelPaulukonis commented: "> I think that it is not, in general, reversible.

Since you replaced "diphthongs with diphthongs, consonants with consonants, and vowels with vowels, chosen at random from English frequency charts" -- running the same process on the words, comparing the variants to dictionary, and then extend that to general english frequency n-gram tables for word order, and the text might fall back into place. I also assume, since it is so easy for me to make assumptions when I have no intention of coding it up, that once portions of the text are "figured out" the tables can be updated with in-document frequencies. But that might include some human overview.

Isn't that roughly what mobile-swipe-style keyboards do -- take all of the letters in the path and do a lookup on the (probable) words?

----

Love the `.mp3`!"
On Fri Nov 06 2015 10:33:46, kevandotorg opened a new issue called "Around the World in X Wikipedia Articles".
On Fri Nov 06 2015 10:36:51, rngwrldngnr commented: "@tra38 How well did the No Dialogue approach go over with test readers? I got put off by it from some of Lovecraft's posthumous works, but I've never been clear if it was specifically the use of description in place of dialogue or if the combination of purple prose and lack of polish soured me unfairly."
On Fri Nov 06 2015 10:57:23, enkiv2 commented: "I was thinking you'd operate on the whole sentences, but then only pay
attention to the whitelisted words.

On Thu, Nov 5, 2015 at 9:28 PM Isaac Karth <notifications@github.com> wrote:

> My Gutenberg Shuffle from 2013 attempted to respect references, but it
> turned out to be a bigger project than anticipwords.It sort of got gender
> right, though I'd redo it if I went that way again.
>
> Note that, at least for the libraries in gensim, pos-taggers work better
> on sentences rather than individual words.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/133#issuecomment-154264928>
> .
>
"
On Fri Nov 06 2015 10:58:48, enkiv2 commented: "It's not too bad. (The repetition in there reminds me of the repetition --
used for rhetorical effect -- in sumerian mythology.)

On Thu, Nov 5, 2015 at 11:08 PM Michael Paulukonis <notifications@github.com>
wrote:

> So, here's a sample
> <https://gist.github.com/MichaelPaulukonis/3af142f787db2908d3f0>.
>
> Not terribly exciting.
>
> First proof of concept. It's all the boring templating.
>
> I want some better stuff for the locales, use of powers, dealing with the
> finality -- the flow from one thing to the next is awkward. Plus, it would
> be nice if the egg shifted around a bit.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/14#issuecomment-154279751>
> .
>
"
On Fri Nov 06 2015 11:05:04, crhallberg opened a new issue called "Game of Nodes".
On Fri Nov 06 2015 11:05:56, ikarth commented: "Excellent idea."
On Fri Nov 06 2015 11:10:15, dariusk commented: "Oh, lovely!!!"
On Fri Nov 06 2015 11:50:48, MichaelPaulukonis commented: "THAT is something that could be PRETTY F****G INTERESTING.
"
On Fri Nov 06 2015 11:54:33, tra38 commented: "@rngwrldngnr Nobody made any comment. They didn't complain about it being bad, or good, or anything of that sort. The text just gave expository information, and that was it...so they read it, and then move on with the rest of the passage, without any comment. It's possible the readers may have never even noticed the lack of dialogue in the text. The point was that there were no *complaints* and that was good enough for me."
On Fri Nov 06 2015 12:15:51, nqpz commented: "Sounds fun.  What about:

* Gratuitously killing off characters"
On Fri Nov 06 2015 12:23:59, MichaelPaulukonis commented: "> It's not too bad.

I'll put that on the cover!

Srsly, the templates are a simplified, non-variant version of a _synopsis_ of the story. So - rough draft/proof-of-concept. I _do_ like repetition-as-rhetorical-device quite a bit. But 1000-creatures-worth of like? hrm. [hah-hah-hah - I just ran it with 1000 creatures, and came up with 49916 words. SO CLOSE!]

I've been thinking about this for months. Possibly vaguely since last year.

Not all that excited by the output. `</ high-expectations>`"
On Fri Nov 06 2015 12:37:05, enkiv2 commented: "If you switched the vocab up and did 1000 creatures you might actually be
able to pass as sumerian mythology. We have middle tablets of important
stories that are literally just lists of musical instruments or creatures.
And, whenever a god is mentioned by name, the name is used 3-6 times with
varying adjectives associated, like: "And then, with a single blow, Lord
Enki, with a single blow, Enki of the deep waters, he with a single blow,
Lord Enki whose advice is always advisable and whose domain is the great me
of princeship, with a single blow, whacked nuddimud over the head."

On Fri, Nov 6, 2015 at 12:24 PM Michael Paulukonis <notifications@github.com>
wrote:

> It's not too bad.
>
> I'll put that on the cover!
>
> Srsly, the templates are a simplified, non-variant version of a *synopsis*
> of the story. So - rough draft/proof-of-concept. I *do* like
> repetition-as-rhetorical-device quite a bit. But 1000-creatures-worth of
> like? hrm. [hah-hah-hah - I just ran it with 1000 creatures, and came up
> with 49916 words. SO CLOSE!]
>
> I've been thinking about this for months. Possibly vaguely since last year.
>
> Not all that excited by the output. </ high-expectations>
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/14#issuecomment-154478181>
> .
>
"
On Fri Nov 06 2015 12:46:26, MichaelPaulukonis commented: "hrm. One of the lists I was thinking of using was Norse Gods.

The abilities/powers used to defeat are derived from a list of Pokemon abilities, and superhero abilities.

It's only the sixth of the month....

And, I must say -- you _are_ getting me more exited about the output. Thanks!"
On Fri Nov 06 2015 12:46:58, greg-kennedy opened a new issue called "Terms and Conditions - a legal thriller!". And it's been completed. Sweet!
On Fri Nov 06 2015 12:51:07, ikarth commented: "I don't think we've had massive numbers of characters attempted yet. I am curious about what emergent effects large groups will have. Or how it will read with all the characters acting."
On Fri Nov 06 2015 12:53:20, enkiv2 commented: "There are a large number of open source licenses floating around, if you
need more legalese to feed in. Here's a list:
https://en.wikipedia.org/wiki/Comparison_of_free_and_open-source_software_licenses

I did a cursory search to see if anybody has collected an archive of EULAs
for various products, but didn't see any.

On Fri, Nov 6, 2015 at 12:46 PM Greg Kennedy <notifications@github.com>
wrote:

> CODE: https://github.com/greg-kennedy/TermsAndConditions
> SAMPLE:
> https://github.com/greg-kennedy/TermsAndConditions/blob/master/SAMPLE.md
>
> I spent a day taking a break from my main entry to knock out this one.
> Inspired by the "TPP as Found Art" issue here: #136
> <https://github.com/dariusk/NaNoGenMo-2015/issues/136>
>
> I wrote a Perl script to parse Terms and Conditions / EULA from various
> major software vendors, then recombine them with Markov chains. The end
> result is a bunch of legalese formatted as a real ToS, complete with sign
> and date line. I think the Markdown formatting is what sets it apart from
> just spewing re-sentences around: it's amazing what some ALLCAPS and
> numbered lists can do for your otherwise boring document.
>
> Wish the corpus was bigger, though. I think a warning label from some
> popular prescription drug would spice it up.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/144>.
>
"
On Fri Nov 06 2015 12:59:41, hugovk commented: "Good idea! 

And a good idea to plot its own route -- the earth's landmass doesn't give a neat latitudinal line of places of interest, so plotting like that is a bit like real round-the-world routes.

It'd be really nice to have an appendix showing a world map of the route taken. You could even include smaller maps of the places visited."
On Fri Nov 06 2015 13:02:55, enkiv2 commented: "Side note: rich-get-richer networks (like most social networks and many
biological neural networks) wherein number of connections per neuron is
distributed according to zipf's law can be constructed either by
introducing new nodes by connecting them to randomly chosen existing nodes
or by starting with a randomly connected network and entirely removing
nodes at random. So, killing off characters at random is literally an
interesting and realistic way of creating potentially interesting social
and power dynamics.

On Fri, Nov 6, 2015 at 12:51 PM Isaac Karth <notifications@github.com>
wrote:

> I don't think we've had massive numbers of characters attempted yet. I am
> curious about what emergent effects large groups will have. Or how it will
> read with all the characters acting.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/143#issuecomment-154484068>
> .
>
"
On Fri Nov 06 2015 13:04:01, nadavoosh commented: "I'm thinking of creating literary "equivalence classes", and then rewriting one novel by swapping its elements for elements from the same equivalence class. 

Defining the classes in a sufficiently interesting way will be the fun part. Perhaps something like: sentences which start and end with the same word.  "
On Fri Nov 06 2015 13:05:16, hugovk commented: "Put EULA in the search box here and you get a fair few results:

http://textfiles.com/directory.html"
On Fri Nov 06 2015 13:06:45, hugovk commented: "> Please review these policies which are designed to protect against fraud and enforcing this agreement.

Huh? These policies are designed to protect against ... enforcing this agreement? :)"
On Fri Nov 06 2015 13:47:12, ikarth commented: "I don't think I've seen that exact approach here...there have been some slightly similar things,  like swapping proper nouns or dialog or picking the same part of speech that has the best word2vec score. I think you are correct that the key is to pick how you define the classes."
On Fri Nov 06 2015 17:29:24, d-baker commented: "added some random code:
![morbidity](https://cloud.githubusercontent.com/assets/5800214/11010282/e5bf99c8-8531-11e5-8706-04a63e19ba09.jpg)
![bump](https://cloud.githubusercontent.com/assets/5800214/11010283/e814775c-8531-11e5-83b7-6069b5976222.jpg)
![limbo](https://cloud.githubusercontent.com/assets/5800214/11010286/ea62e3cc-8531-11e5-9c1d-9806a4e7e9b7.jpg)
"
On Fri Nov 06 2015 17:42:42, araile commented: "So much poetry in these snippets!"
On Fri Nov 06 2015 17:45:45, suisea commented: "ilu :kissing_cat: @d-baker "
On Fri Nov 06 2015 18:12:51, saluk commented: "Fun with pronouns: https://docs.google.com/document/d/1Y5IvdJDc4kyHyK2AJzqbsFOS6w5BX_nbsYY1ndGMeDU/edit?usp=sharing"
On Fri Nov 06 2015 18:32:37, greg-kennedy commented: "    #!/bin/sh
    perl -pne '$_=join(" ",map{sub a{$_[0]^lc($_[0])^($_[1] x length($_[0]));}if($_=~m/^(\W*)(\w)(\w*?)(\w)?(\w)?(\w)?(\W*)$/){join("",$1,a($2,"h"),a($3,"o"),a($4,"d"),a($5,"o"),a($6,"r"),$7)}else{$_;}}split(/\s/,$_))."\n";' 1342.txt"
On Fri Nov 06 2015 20:06:44, nadavoosh opened a new issue called "Novel Switcher".
On Fri Nov 06 2015 20:07:45, nadavoosh commented: "This is awesome! Conceptually similar to http://itunestandc.tumblr.com"
On Fri Nov 06 2015 20:38:40, araile commented: "Sample chapter list:

1. Confringo (Epic 201445392 C)
2. Sól (Hd 21693 C)
3. Sinthgunt (Kepler-9 D)
4. Demeter (Hd 122430 B)
5. Furnunculus (Epic 201855371 B)
6. Orchid (Hd 92788 B)
7. Alohomora (Corot-23 B)
8. Lotus (Kepler-281 B)
9. Quietus (Hd 166724 B)
10. Þrúðr (Kepler-390 B)
"
On Fri Nov 06 2015 20:57:05, ikarth commented: "Not that i am aware of. There are several generators that generate books in the style of another book ( #72 combines _two_ books) and I think there are some that switch their generation according to various rules, but I can't recall any that are even particularly close to your plan.

In general, I think combining multiple approaches is an underutilized way to create order in novel-length generators. Probably because it can take a month to learn a single new approach, let alone figuring out how to combine multiple ones.  Your idea seems to strike a balan e between a strong concept and a relatively straightforward implementation.  I suspect the trick will be to have enough works to key off bits of the titles. May want to throw some Shakespeare in there: everyone is always quoting his lines as titles."
On Fri Nov 06 2015 21:05:44, araile commented: "I'm maintaining the source code at [araile/nanogenmo-atlas](https://github.com/araile/nanogenmo-atlas).

Currently it pulls names from a few [Corpora](https://github.com/dariusk/corpora) word lists and pairs them with a random selection of planets from the [Open Exoplanet Catalogue](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue) (OEC). I'm planning to use more of this data to generate a chapter about each planet.

There is a `make stats` command which prints statistics about the OEC data. This includes the "earliest settled" and "latest settled" years, which I'm generating based on the distance of each planet's star from our own, and the assumption that we will mysteriously learn to travel at the speed of light in the Future."
On Sat Nov 07 2015 02:39:36, almightyJanitor opened a new issue called "Sure Thing".
On Sat Nov 07 2015 03:25:33, hugovk commented: ""That Hamlet is full of clichés.""
On Sat Nov 07 2015 03:30:59, hugovk commented: "I wonder if you could pull in some demographic info from Wikipedia but change the years and proper nouns:

> In 2353, Confringo's population was around 5.5 million, with the majority living in its southern regions.[8] In terms of area, it is the eighth largest planet in Epic 201445392 and the most sparsely populated planet in the Epic 201445392 Union. Confringo is a parliamentary republic with a central government based in the capital Ferdasia, local governments in 317 municipalities,[9] and an autonomous region, the Trandal Islands. Over 1.4 million people live in the Greater Ferdasia metropolitan area, which produces a third of the country's GDP. Other large cities include Rempart, Krut, Luou, Kylajaki, Hilta, and Piok."
On Sat Nov 07 2015 04:55:21, araile commented: "Oh I like this!"
On Sat Nov 07 2015 05:14:04, araile commented: "For #17: I'm using **Python 3**."
On Sat Nov 07 2015 07:37:17, ikarth commented: "I really, really like this as the basis for a generated plot."
On Sat Nov 07 2015 07:39:56, araile commented: "Now I have it outputting HTML and Markdown. I'll regenerate the story here as I progress: https://github.com/araile/nanogenmo-atlas/blob/output/Atlas-of-Remote-Planets.markdown"
On Sat Nov 07 2015 08:30:31, hugovk commented: "> Meow meow meow meow meow meow meow
meow meow meow meow meow meow meow

The official NaNoGenMo lorem ipsum :)"
On Sat Nov 07 2015 18:33:32, kellyi commented: "Progress! Minimal progress, but still: I made a GitHub repo to hold my project:

https://github.com/kellyi/NaNoGenMo2015

I think what I'm going to do is make a Sinatra site designed to be read on mobile that uses some bank of text to make some sort of web equivalent to a BS Johnson novel like [The Unfortunates](https://en.wikipedia.org/wiki/The_Unfortunates). As far as a corpus goes, I was thinking about feeding the thing a long collection of paragraphs or sentences from public domain novels which describe rooms, then chaining them together in some way while generating connective text between them to write a story about passing through dozens and dozens of rooms ordered randomly at page load."
On Sat Nov 07 2015 19:11:37, hnotess opened a new issue called "Intent to participate: Completely new".
On Sat Nov 07 2015 20:41:46, translulaith commented: "I pretty much forgot about this for a week but I have a concept now, and I'm going to start work on it tomorrow.

I want to make the Journal or Memoir of Laplace's Demon, a big stream of consciousness block of text that describes the position of individual atoms and molecules in relation to big macro landmarks. Something like this, repeated.

"I am aware of an  oxygen atom inside a molecule of water, floating freely in the clouds between the shadow of the large mountain and the rivers running north to south.""
On Sat Nov 07 2015 20:43:09, ikarth commented: "I'm reminded of Gnostic letter expansion, as described in Irenaeus's _Adversus haereses_ (Book 1, Chapter 14):

> He also maintains that the letter itself, the sound of which followed that sound below, was received up again by the syllable to which it belonged, in order to the completion of the whole, but that the sound remained below as if cast outside. But the element itself from which the letter with its special pronunciation descended to that below, he affirms to consist of thirty letters, while each of these letters, again, contains other letters in itself, by means of which the name of the letter is expressed. And thus, again, others are named by other letters, and others still by others, so that the multitude of letters swells out into infinitude. You may more clearly understand what I mean by the following example:— The word Delta contains five letters, viz., D, E, L, T, A: these letters again, are written by other letters, and others still by others. "
On Sat Nov 07 2015 21:49:41, aeschright commented: "Nice! I've been playing with a similar idea at https://twitter.com/licensemee"
On Sat Nov 07 2015 22:42:21, ikarth commented: "There's a few different dictionaries around, depending on how involved you want to get and exactly what kind of shift you want to implement. Something like the public domain [Moby Parts of Speech list](http://icon.shef.ac.uk/Moby/mpos.html), while not the most recent and up-to-date project might be a good starting point if you're using the relationship of words in a dictionary, versus something like WordNet that has less concept of the alphabetized order of the words. It mostly depends on what twist you want to put on it."
On Sat Nov 07 2015 22:59:32, greg-kennedy commented: "Similar as well to NaNoGenMo-2014 entry "The Definition Book" by @samcoppini
https://github.com/dariusk/NaNoGenMo-2014/issues/48"
On Sun Nov 08 2015 00:00:44, jemisa commented: "you might want to look at https://github.com/ryankiros/neural-storyteller , in particular he 'style-shifting' operation that allows their model to transfer standard image captions to the style of stories from novels."
On Sun Nov 08 2015 07:48:46, ikarth commented: "#72 might also be relevant. That uses word2vec rather than the recurrent neural nets or deep convolutional neural nets, but you might be able to borrow some ideas. There's also this RNN implementation: https://github.com/karpathy/char-rnn (as decribed in [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)) that you might find interesting. "
On Sun Nov 08 2015 07:49:44, ikarth commented: "Sounds like an interesting approach. How is it coming along so far?"
On Sun Nov 08 2015 11:32:21, MichaelPaulukonis commented: "@mewo2 - pretend I've never used word2vec before (and hardly use Python). How would I generate the datasets? since I'm essentially asking to be stepped through the process, do you know of a good tutorial for this?

(I've managed to get this all set up on windows, amazingly enough.)"
On Sun Nov 08 2015 11:55:04, ikarth commented: "I've been messing with word2vec a bit, though I haven't finished enough to be able to speak authoritatively. For the main data, you can use prebuilt data sets, such as the ones from the [original Google release of the C version of word2vec](https://code.google.com/p/word2vec/). If you want to train your own, there's a [couple of tutorials](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors) out there, though I haven't far enough to vouch for them yet."
On Sun Nov 08 2015 14:37:01, okiyama opened a new issue called "Plan to leverage Dwarf Fortress to write a novel".
On Sun Nov 08 2015 14:41:40, michcioperz opened a new issue called "Amazement of Life".
On Sun Nov 08 2015 14:58:48, TheCommieDuck commented: "Been a bit swamped with real-world work so not done much, but just a quick update.

I've got goals working! The example I've got is hunger; it works well, I feel:
http://i.imgur.com/OWKaaXu.png

The best bit about Prolog is how homogeneous everything is; by changing 2 words from 'food' to 'character' I've managed to create cannibals! http://puu.sh/l9mcy/186576eeec.png 
You might notice that Bob tries to pick up Bob and fails...there was a bug originally where characters could pick up themselves. This ended up with an...interesting situation where Bob picked up Bob, and then *ate himself*. Alice also picked herself up..and ate herself. They still existed, in the void - just..having eaten themselves. At least it stopped them being hungry, I guess.

I also had a realisation that I was going under some serious feature creep. As such, for now - and probably it'll be left as such - the world will be the same between stories. Names, people, plot - all different; however the layout of the city and world and stuff will be the same and is hardcoded in for now.

Another realisation (as I think I discussed in the Slack) is that I couldn't continue with my original plan - that is, have something reading like The Swallows, but have a sensible and realistic plot thread throughout. The best idea was to move to one or the other, and I've chosen the former. Getting sensible, realistic plot and whatnot is far too hard for 3 weeks. I'm now entirely rolling with the absurdity! I can predict the story will be something like:

```
Bob went and ate some food.
Bob walked around and talked about birds.
Bob's mum went to sleep.
Bob's mum died suddenly. (As the absentation goal fires).
Bob eats food.
Bob goes to sleep.
Bob leaves for the city. (As the hero departs).

```

If this actually stretches to 50,000 words...I can imagine 30 pages of utter nonsense followed by 2 sentences containing the entire plot, repeated 30 times. 

All I hope is that I can make it good enough!

"
On Sun Nov 08 2015 15:06:13, marktanner22 opened a new issue called "what is love?". And it's been completed. Sweet!
On Sun Nov 08 2015 17:45:42, michelleful commented: "It sounds like what you'd need (if you did choose to somehow "translate" the names) is to have the names in the Spongebob corpus tagged for named entities, but in case it's useful to have a version of P&P that is name-tagged, the [P&P e-text at Pemberley.com](http://www.pemberley.com/janeinfo/pridprej.html) is conveniently so.

    <P>``<A HREF="ppdrmtis.html#MrBennet">Mr.&#32;Bennet</A>, how can you abuse your own children in such way?  You take delight in vexing me.  You have no compassion on my poor nerves.''</P>

"
On Sun Nov 08 2015 21:32:43, mattfister commented: "**Day 8 Update**

Sample: [The Implicit Story of Ages](https://mattfister.github.io/nanogenmo2015/samples/day8_The_Implicit_Story_of_Ages.html)

Another update. It was a crazy week so way less progress than I hoped for.

Still I got a bunch of new features done, so I'm happy.

* Attributes are now being tracked on characters. Think d6 sort of role-playing system. Each character has attributes for lore, survival, and combat, but it's really simple for me to add other attributes.
* When generating setting paragraphs, lore challenges are conducted to have character discuss the history of the place.
* Food and energy are now being tracked for the party. If they are low on food, a character will go fishing, hunting, or gathering. This is a new paragraph that is appended to the chapter.
*Characters have memories from their youth. If they face a challenging situation, like hunting, they may recall when they went hunting as kids or something.
* If characters have a success or failure, they may have their current 'quality' replaced by a new negative or positive quality. So Darren the Devious may become Darren the Dependable (alliteration unlikely).

I'm happy with the hunting, fishing, and gathering paragraphs I'm generating, but I realize the templates for them really show. I'm not going to pursue another text generation strategy for this year's nanogenmo at this point, but maybe next year I'll have something that allows for some more variety.

One thing I just ran out of time on today was generating camping paragraphs. I want to deliver the sense that the night is dangerous and a time of strangeness so you'll get weird dreams and sounds in the darkness. That should be easy to do and coming up soon.

Here's my favorite sample chapter from today's output.

>**The Windmill**
>
>Joaquin the profound, Billie Nicholson, and Darin traveled to a windmill. Joaquin the profound exclaimed, "I hate this place." There was a cow inside the windmill. There was a button inside the windmill. The windmill was a windpump. The windmill could power entire town. Darin Schuler stated, "This is a very dangerous windmill." The button was an artifact. Joaquin Hart mentioned, "This place was once known as 'Iaaemlbaehh Ladodwi'." Joaquin thought about how a button was similar to a foil. Darin proclaimed, "Let's move on."
>
>Billie the innocent mentioned, "Our food should last a bit longer." Billie decided to go hunting. Billie remembered hunting with her best friend as a child. She searched the windmill for signs of caribous. But she failed to find any caribou signs. Billie returned to Joaquin empty handed. Billie the innocent exclaimed, "We will have to just keep going, there was no food to find here." Because of this great failure Billie the innocent became known as 'The Interfering'."
On Sun Nov 08 2015 22:50:46, scazon commented: "Update 11/8:

Recipes now come with serving glass suggestions. Made other small optimizations. Also created a Manhattan template for the section on Manhattan style drinks (last 4):

```------------------------------
Tequila and Dr. Pepper

   2 oz Tequila
   3 oz Dr. Pepper
   A splash of Kirsch

 Serve straight up in a goblet.

------------------------------
Just Tequila

  .5 oz Tequila
  .5 oz Sweet vermouth
  .5 oz Blackberry brandy
  .5 oz Cranberry juice

 Serve straight up in a tall shot glass.

------------------------------
Female Neighbour

   1 oz Bourbon
   1 oz Jägermeister
  .5 oz Cognac
  .5 oz Grenadine syrup
   1 sugar cube

 Serve straight up in a Margarita glass and garnish with an orange slice.

------------------------------
The Burlington

1.25 oz Bourbon
 .75 oz Italian vermouth
   A dash of saffron bitters

 Serve on the rocks in a cordial glass and garnish with a slice of pineapple.

------------------------------
Scotch Spectacle

   2 oz Scotch
   1 oz Red vermouth
   A dash of plum bitters

 Serve on the rocks in a Manhattan glass and garnish with a dash of nutmeg.

------------------------------
Plane Whiskey

 2.5 oz Canadian Whiskey
  .5 oz White vermouth
   A dash of Peychaud's bitters

 Serve straight up in a Martini glass and garnish with a cherry.

------------------------------
Contrivance on the Disapproval

1.25 oz Rye
 .75 oz Sweet vermouth
   A dash of Angostura bitters

 Serve on the rocks in a cordial glass.
```"
On Sun Nov 08 2015 23:39:47, jeffdaze opened a new issue called "Alright -- NaNoWriMo turned into NaNOPENOPENOPE so now it's NaNoGenMo for me!".
On Mon Nov 09 2015 02:42:05, hugovk commented: "I like how Just Tequila is anything but just tequila. You can definitely have lots of fun naming things, and I also like how there are some straightforward names (Tequila and Dr. Pepper) and more intentive names (Contrivance on the Disapproval).

On 1st December, you should start trying these and reviewing them!"
On Mon Nov 09 2015 03:21:44, suisea commented: "omg meow @araile "
On Mon Nov 09 2015 04:06:51, araile commented: "😸"
On Mon Nov 09 2015 07:59:16, ikarth commented: "How is progress so far?"
On Mon Nov 09 2015 08:16:32, enkiv2 commented: "Welcome! I'm glad I'm not alone in taking this path :P

On Sun, Nov 8, 2015 at 11:39 PM Jeff Daze <notifications@github.com> wrote:

> I have some ideas that may or may not involve Markov chains...
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/151>.
>
"
On Mon Nov 09 2015 08:39:38, coleww commented: ":clap: i would be down to try a robot cocktail or two.
"
On Mon Nov 09 2015 09:35:34, enkiv2 commented: "I knew the idea seemed familiar :D

On Sat, Nov 7, 2015 at 10:59 PM Greg Kennedy <notifications@github.com>
wrote:

> Similar as well to NaNoGenMo-2014 entry "The Definition Book" by
> @samcoppini <https://github.com/samcoppini>
> dariusk/NaNoGenMo-2014#48
> <https://github.com/dariusk/NaNoGenMo-2014/issues/48>
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/66#issuecomment-154774391>
> .
>
"
On Mon Nov 09 2015 09:50:42, MichaelPaulukonis commented: "Another sample, not much changed - but some better corpii being used:

> A king had seven sons, and when the other six went off to find brides, he kept the youngest with him because he could not bear to be parted from them all. They were supposed to bring back a bride for him, as well, but they found a king with six daughters and wooed them, forgetting their brother. But when they returned, they passed too close to a giant's castle, and he turned them all, both princes and princesses, to stone in a fit of rage.

> When they did not return, the king, their father, tried to prevent their brother from following, but he went.

> On the way, he helped a wolverine-chimpanzee, he helped a Warg-Typhon, he helped a tiger-Manticore, he helped a Hippocampus-ocelot, and gave a starving mule-giraffe his horse to eat. The mule-giraffe let the prince ride on him, instead, and showed him the giant's castle, telling him to go inside. The prince was reluctant fearing the wrath of the giant, but the mule-giraffe consoled him. The mule-giraffe persuaded the prince to enter the castle for there he would encounter not the giant, but the princess the giant kept prisoner.

> The princess was very beautiful and the prince wanted to know how he could kill the giant and set her and his family free. The princess said that there was no way, as the giant did not keep his heart in his body and therefore could not be killed. When the giant returned, the princess hid the prince, and asked the giant where he kept his heart. He told her that it was under the door sill. The prince and princess dug there the next day and found no heart. The princess strewed flowers over the door sill, and when the giant returned, told him that it was because his heart lay there. The giant admitted it wasn't there and told her it was in the cupboard. As before, the princess and the prince searched, to no avail; once again, the princess strewed garlands of flowers on the cupboard and told the giant it was because his heart was there. Thereupon the giant revealed to her that, in fact, there was a dank drumlin field with a ocelot-Gnome, beyond that there was a fluorescent tepui with a Kobold-Hippogriff, beyond that there was a overlooked shield volcano with a Typhon-panda, beyond that there was a whimsical bayou with a Nix-Roc, beyond that there was a tiny subglacial mound with a Oni-mynah bird, and in the Oni-mynah bird's nest was an egg; and in the egg was the giant's heart.

> The prince rode to the dank drumlin field, where the mule-giraffe jumped to attention. The prince called on the wolverine-chimpanzee to defeat the ocelot-Gnome. The wolverine-chimpanzee defeated the ocelot-Gnome by using its Thaumaturgy. The prince rode on to the fluorescent tepui, where he was menaced by a Kobold-Hippogriff. The prince called on the Warg-Typhon to defeat the Kobold-Hippogriff. The Warg-Typhon defeated the Kobold-Hippogriff by using its Drought. The prince rode on to the overlooked shield volcano, where he was menaced by a Typhon-panda. The prince called on the tiger-Manticore to defeat the Typhon-panda. The tiger-Manticore defeated the Typhon-panda by using its Perspicuity. The prince rode on to the whimsical bayou, where he was menaced by a Nix-Roc. The prince called on the Hippocampus-ocelot to defeat the Nix-Roc. The Hippocampus-ocelot defeated the Nix-Roc by using its Physics Manipulation. The prince rode on to the tiny subglacial mound, where he was menaced by a Oni-mynah bird. The prince called on the mule-giraffe to defeat the Oni-mynah bird. The mule-giraffe defeated the Oni-mynah bird by using its Fish Manipulation. The mule-giraffe plucked the egg from the nest of the Oni-mynah bird, gave it to the prince, and told him to squeeze it. When he did, the giant screamed. The mule-giraffe told him to squeeze it again, and the giant promised anything if he would spare his life. The prince told him to change his brothers and their brides back to life, and the giant did so. Then the prince squeezed the egg into two and went home with the giant's captive princess as his bride; accompanying him were his brothers and their brides, and the king rejoiced.
"
On Mon Nov 09 2015 09:55:46, mattfister commented: "That sample is amazing. Nicely done!"
On Mon Nov 09 2015 13:28:56, tra38 commented: ">I want some better stuff for the locales, use of powers, dealing with the finality -- the flow from one thing to the next is awkward.

How about handwriting an "aftermath" template where the prince falls from grace and turns into a giant (which then gets slain by the next prince in the next story). A story never ends just because someone says "The End"."
On Mon Nov 09 2015 13:37:49, MichaelPaulukonis commented: "I like how this is coming along.

I also have some sympathy for the characters in that they seldom seem to find food. No doubt because they search specifically for avocados, caribous, and the like - totally overlooking the smaller wild game and grains that surround them.

Staring at your "reduced ConceptNet" implementation....
"
On Mon Nov 09 2015 14:03:52, MichaelPaulukonis commented: "`:blink blink:`

That's an interesting idea, and a nice framing-device for the 50K word-requirement.

And somewhat sad. Makes me think of _Let the Right One In_."
On Mon Nov 09 2015 15:00:54, greg-kennedy commented: ""Kandace discovered a hook inside the glacier. The glacier was a topographical feature. Marybeth considered how a hook is a golf stroke."
Adventurers who crack puns... is this Automated Terry Pratchett?"
On Mon Nov 09 2015 15:04:49, mattfister commented: "Actually, when I first discovered conceptnet, I think I was planning on searching through all English words for potential puns. Never got anything working though."
On Mon Nov 09 2015 16:03:42, flexo commented: "I've changed the location of the preview to https://github.com/flexo/nanogenmo2015/blob/master/novel/output.md so that it gets nicely formatted in github."
On Mon Nov 09 2015 20:41:39, tra38 commented: "I know this sound surprising, considering this is a novel-writing competition...but I don't actually like novels. I always believe in conveying messages in as few words as possible, rather than filling the page with worthless purple prose. I enjoy reading novel synopses, not the novel.

When writing this competition, I had two goals: 1) write a program that generates a novel, 2) write a program that generates a *meaningful* story.

Goal 2 is a complete success, in my opinion. Each sentence conveys information about what a real person said over the telephone. Theoretically, someone may want to read more of the novel to find out about more atheists, although boredom will eventually set in for the reader...

The problem I'm having is Goal 1. Each sentence in my template is *too effective* at conveying the data. Each data point only really gives me one sentence (and sometimes even less!), and while there's 99 different atheists, that only means 99 sentences.

My estimate is that each "chapter" is about 200 words long. This means far from getting a glorious 50,000 words novel, I am stuck with a ~19,800 word novella.

Now, there's a lot of data points that I am not using yet, and clearly using them will lead to more words to be stuffed into each chapter. But as text is efficient at conveying information, I'm going to have to exhaust a lot of data to reach my original quota of '505 words/chapter'. And each data point I add seems more and more tangential to the original purpose of the novel: to show the religious beliefs and practices of self-proclaimed atheists who believe in God. As a result, I may consider just abandoning Goal 1 entirely. It may be *possible* for me to get up to 505 words...but the cost may be a less meaningful story. And I think I prefer meaning more than mere word count.

There are also examples of people getting the Green *complete* tag despite not reaching the 50,000 word criteria too, so there's no shame in abandoning the pursuit for the Great White Novel."
On Tue Nov 10 2015 08:14:24, maetl commented: "Great idea. I’ve been working on something very similar to this, though I didn’t explain it quite so coherently: #57 "
On Tue Nov 10 2015 09:10:46, MichaelPaulukonis commented: "Markov chains are often fun. They are quite naive, but lead to unexpected (for a non-savant human) juxtapositions. "
On Tue Nov 10 2015 09:12:22, MichaelPaulukonis commented: "Nice.

Also: irritating, because I can't get that [!@#% song](https://www.youtube.com/watch?v=HEXWRTEbj1I) out of my head, now."
On Tue Nov 10 2015 09:21:31, MichaelPaulukonis commented: "I adapted your reduced-concept-net code to NodeJs - https://github.com/MichaelPaulukonis/NaNoGenMo2015/blob/master/conceptnet/cnSearch.js

I haven't figured out _how_ I'm going to be using it in my project, but it's a great resource....

It translates the relations into json, so we get back something that looks like this:

```
> search("volcano")
```
```
{ HasA: [ 'violent_eruption', 'crater', 'very_distinctive_peak' ],
  InstanceOf:
   [ 'album',
     'organisation',
     'film',
     'mountain',
     'album',
     'musical_work',
     'musical_work',
     'single',
     'band',
     'musical_work',
     'film',
     'film',
     'organisation',
     'cinder_cone',
     'television_episode',
     'single',
     'hollywood_cartoon',
     'musical_work',
     'musical_work',
     'cartoon',
     'band',
     'natural_place',
     'place',
     'album',
     'album',
     'musical_work' ],
  IsA:
   [ 'mountain',
     'single_broadcast_tv_show',
     'music_single',
     'movie',
     'album',
     'band',
     'either_cone_or_dome',
     'violent_force_of_nature',
     'mountain',
     'geography_topic',
     'physical_phenomenon',
     'land_topographical_feature',
     'book',
     'musical_composition',
     'mountain' ],
  RelatedTo:
   [ 'crater',
     'etna',
     'explode',
     'lava_spout',
     'melt',
     'mount',
     'tall_mountain',
     'erupt_magma',
     'geologic',
     'hot',
     'lave',
     'red',
     'spout',
     'violent',
     'violent_mountain',
     'ash',
     'bake_soda',
     'earth',
     'lava',
     'lava_mountain',
     'lava_spewer',
     'like',
     'mountain',
     'source',
     'throw',
     'very',
     'very_hot',
     'big',
     'explosion',
     'fiery',
     'fiery_mountain',
     'formation',
     'lava_source',
     'like_vulcan',
     'on_mountain',
     'saint',
     'throw_fire',
     'erupt',
     'erupt_lava',
     'explosive',
     'fire',
     'fire_mountain',
     'have',
     'hot_lava',
     'landscape',
     'lava_fire',
     'lave_mountain',
     'spewer',
     'tall',
     'lava',
     'magma',
     'ash_smoke',
     'earth_formation',
     'erupt',
     'explosive_lave',
     'magma',
     'mountain_like',
     'shape',
     'smoke',
     'soda',
     'vent',
     'gas',
     'moon',
     'active',
     'bake',
     'cone_shape',
     'crater_fire',
     'eruption',
     'event',
     'geologic_event',
     'have_magma',
     'melt_lava',
     'mountain_eruption',
     'vulcan',
     'yes',
     'planet',
     'active_mountain',
     'big_spout',
     'cone',
     'erupt_mountain',
     'hawaii',
     'vesuvius',
     'mantle' ],
  dbpedia:
   [ 'progressive_rock',
     'rock_music',
     'jimmy_buffett',
     'black_metal',
     'pop_punk',
     'indie_rock',
     'rock_music',
     'art_rock',
     'folk_rock',
     'alternative_rock',
     'post_grunge',
     'alternative_rock',
     'jimmy_buffett' ],
  CapableOf:
   [ 'form_new_island',
     'erupt',
     'form_new_land_mass',
     'release_toxic_gas_into_atmosphere',
     'send_lava_into_water' ],
  AtLocation:
   [ 'top_of_mountain',
     'surface_of_earth',
     'bottom_of_sea',
     'bottom_of_ocean' ],
  HasProperty: [ 'extinct', 'hot', 'dormant' ],
  ReceivesAction: [ 'fill_with_magma', 'cause_by_upwelling_magma' ],
  PartOf: [ 'crater' ],
  Antonym: [ 'dormant' ],
  MadeOf: [ 'lava' ] }
```"
On Tue Nov 10 2015 10:55:06, zachwhalen commented: "Just a quick update. I've set up <a href="https://github.com/zachwhalen/secretgardens/">a repository</a> but I'm going back and forth about what I actually want to do. My first idea was to use this password dump to generate a novel that already exists, Pierre Menard-style. 

So in my first version, the script scans the password dump for correctly-spelled English words, then looks for those words in the novel, *The Secret Garden*. Or rather, it scans *The Secret Garden*, and for each word it finds in the passwordlist, it prints that word. Words it fails to find are just printed as *****:

> **** Mary ****** was sent to ************* Manor to live with her ****** 
everybody said she was the most ******************** child ever ***** 
It was true too  She had a little thin face and a little thin ***** 
thin light hair and a sour expression  Her hair was yellow and **** 
face was yellow because she had been born in India and had always ***** 
ill in one way or another  Her father had held a position under **** 
English ********** and had always been busy and ill himself and **** 
mother had been a great beauty who ***** only to go to parties **** 
amuse herself with gay people  She had not wanted a little girl *** 
all and when Mary was born she handed her over to the care of an ***** 
who was made to understand that if she wished to please the *** ****** 
she must keep the child out of sight as much as possible  So when **** 
was a sickly ******* ugly little baby she was kept out of the **** 
and when she became a sickly ******* ******** thing she was kept **** 
of the way also  She never ********** seeing ********** anything **** 
the dark faces of her **** and the other native servants and as ***** 
always obeyed her and gave her her own way in everything because **** 
*** ***** would be angry if she was disturbed by her crying by **** 
time she was six years old she was as ********** and selfish a ******* 
pig as ever lived  The young English governess who came to teach **** 
to read and write ******** her so much that she gave up her place *** 
three months and when other *********** came to try to fill it ***** 
always went away in a shorter time than the first one  So if Mary **** 
not chosen to really want to know how to read books she would ****** 
have learned her letters at **** 

This is interesting, to me at least, because each word you see here is something that someone used as a password. I can't decide, though, if it's *interesting enough*, so I'm trying to learn how to use POS tagging to do some smarter parsing. I'm thinking perhaps of replacing parts of speech with the frequency matching. Like, replace the most frequently used noun in the book with the most frequently used noun password (that is, "password"). 
"
On Tue Nov 10 2015 11:00:50, MichaelPaulukonis commented: "_The **Secret** Garden_

....

I SEE WHAT YOU DID THERE

----

What if the words NOT found as passwords were left, and the secret-passwords were removed from the text-garden instead? 

Not a lot would be left, though."
On Tue Nov 10 2015 11:07:37, MichaelPaulukonis commented: "Is there a "browser" for Gitenberg - to find out what's in there? 

The project looks excellent, but I'm having a hard time understanding how to navigate the repos and the tools.
"
On Tue Nov 10 2015 11:54:18, greg-kennedy commented: "#144 is done in Perl"
On Tue Nov 10 2015 11:56:47, enkiv2 commented: "#65 is done in gg and #66 is done in python.

On Tue, Nov 10, 2015 at 11:54 AM Greg Kennedy <notifications@github.com>
wrote:

> #144 <https://github.com/dariusk/NaNoGenMo-2015/issues/144> is done in
> Perl
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/17#issuecomment-155484866>
> .
>
"
On Tue Nov 10 2015 12:25:41, ikarth commented: "> There are also examples of people getting the Green complete tag despite not reaching the 50,000 word criteria too, so there's no shame in abandoning the pursuit for the Great White Novel.

My belief is that the spirit of the 50,000 words should be interpreted in light of what NaNoGenMo was in reaction to: lots of successful Twitter bots and the like, very few long-form generators. Writing a novella that says everything that you wanted to is, to my mind, well within the bounds of NaNoGenMo. I've personally viewed the length as aspirational: we, collectively, don't know how to get a machine to write a _good_ novel, so we're slowly fumbling our way towards writing a _coherent_ novel, gradually trying to make each attempt more interesting.

As a practical matter, if I was in your place, I'd step back from thinking about the length and turn instead to the problem of keeping the reader's interest for more chapters. Is there a way you can add more variability? But I'm not in your place. You get to be the one to declare it complete, on your terms."
On Tue Nov 10 2015 12:39:57, MichaelPaulukonis commented: "Thanks for the updates! I had made some this morning, but neglected to click-comment before rebooting my machine..."
On Tue Nov 10 2015 12:43:28, mattfister commented: "Nice! I'm glad it could help."
On Tue Nov 10 2015 13:32:34, zachwhalen commented: "@MichaelPaulukonis Right, you can kind of see how it wouldn't be much left. Mostly proper nouns, I think. 

Still, it might be interesting to do something else with that smaller set of non-overlapping terms."
On Tue Nov 10 2015 13:33:45, zachwhalen commented: "By the way, I'm doing this in Perl because why not and I'm using with the `Lingua::EN::Tagger` package for POS tagging. It seems to work fine."
On Tue Nov 10 2015 14:44:57, hugovk commented: "> So in my first version, the script scans the password dump for correctly-spelled English words, then looks for those words in the novel, The Secret Garden. Or rather, it scans The Secret Garden, and for each word it finds in the passwordlist, it prints that word. Words it fails to find are just printed as *****:

Are "incorrectly-spelled" words left out of the equation? (The "Or rather" suggests this isn't the case.) But if so, I'd leave them it. "Correctly" spelled depends on the authority of the dictionary list, and you might miss some unusual (or misspelt) words in the original that are also passwords.

I'd have the *** the same length as the omitted words.

I quite like keeping the original text more intact than replacing, say, each "time" with "password". (But try both!) It's readable-ish. If you're generating HTML output you could style each word with some sort of intensity to represent how frequent that word is."
On Tue Nov 10 2015 15:32:30, zachwhalen commented: "> If you're generating HTML output you could style each word with some sort of intensity to represent how frequent that word is.

Interesting idea. I could do something with relative sizing or transparency. Sizing could be interesting since it would inevitable make the layout much more dynamic."
On Tue Nov 10 2015 16:49:44, hugovk commented: "Perhaps also change the background and foreground colours too."
On Tue Nov 10 2015 22:41:50, ikarth opened a new issue called "Beginner's Questions, Answers, & Tutorials". But it's an admin issue, so who cares?
On Tue Nov 10 2015 23:10:37, marythought commented: "## DAY ... TEN?

Ok, after taking some time off to learn all the data structures and algorithms (or not learn, as the case may be), I needed a quick win so I came back to this and was able to publish a version of the poem generator!

[Where I'm From](https://whereimfrom.herokuapp.com/)

It's not very fancy, and probably breaks all the Node/Express rules (I am a very proficient Ruby on Rails developer seriously you should hire me), but it meets the prime objective of generating a new poem on demand.

I like this so much I am not sure how to translate it into a novel... but let's not call it "done" yet, because I'm going to sleep on that.

I found a couple open-source texts that work well for "memoir" style (Anne of Green Gables is the frontrunner), so I played with using RiTA to markov it up. My idea was to start with the base text, and then see if there's any way to prioritize the keywords generated in the 'Where I'm From' poem (so it would be a poem followed by short vignette featuring terms mentioned in that poem, and then more in that pattern).
 
<img width="724" alt="screen shot 2015-11-10 at 8 01 09 pm" src="https://cloud.githubusercontent.com/assets/10136229/11083596/d8536614-87e5-11e5-9311-70392d3280b2.png">

It's interesting, but it isn't very readable in paragraph form. So I think I need to consider another method for text generation. Which puts me back at the starting line. :)

Maybe I'll just write more poems...? #NaPoGenMo! I'm not 100% invested in the novel form, at least not for my first experiment this year, but I'm shooting to adhere to the 50,000 word count..."
On Tue Nov 10 2015 23:29:47, ikarth commented: "### Markov Chains
Markov chains are a well-known way to do text generation. When used to generate text, the basic operation is to look at the last few words and then pick the next word based on what words have been observed to follow. 

Here's an [interactive explanation of Markov chain generation by tullyhansen](https://dl.dropboxusercontent.com/u/1032628/markovy.html#4i.7f.6a.e.5m.5k.7b) 
[Allison Parrish's tutorial on N-grams and Markov chains, with Python code](http://www.decontextualize.com/teaching/rwet/n-grams-and-markov-chains/)
[Jeff Atwood explains Markov chains via Garfield](http://blog.codinghorror.com/markov-and-you/)
[A visual explanation of Markov chains](http://setosa.io/blog/2014/07/26/markov-chains/index.html)
[Andrew Plotkin's Fun With Markov Chains](http://www.eblong.com/zarf/markov/)
[Bookmerge, an example Markov generator that lets you combine two books (uses Ruby)](http://bookmerge.herokuapp.com/)"
On Wed Nov 11 2015 04:44:07, cpressey commented: "If I may air another viewpoint re word count:

What attracted me to NaNoGenMo when I first heard of it was the absolutely absurd requirement of 50K words.  How do you get a machine to fill that space in a "sensible" way?  Or do you even try?  If not, what do you accept for "not sensible" ways to fill it?  And why?

To me, answering those questions is probably just as central to NaNoGenMo as the actual generating of text.  If it was simply billed as a computer story-writing jam or something, with a reasonable word count goal, I probably wouldn't have bothered to look into it.

So, you could say I am rather strongly pro-taking-50K-words-seriously.  (I'm also rather strongly pro-novel-means-NOVEL-gosh-darn-it, at least in odd-numbered years, but that's a slightly different matter.)

That said, I'm virtually in the same boat as you @tra38, and I don't have any good ideas.

My current plan is to generate two versions of my novel at the end: one at readable length, the other a 50K word version to get the Completed label even though 98% of it will be pure [wharrgarbl](http://i1.kym-cdn.com/photos/images/newsfeed/000/032/388/wharrgarbl.jpg).
"
On Wed Nov 11 2015 05:28:13, cpressey commented: "+1 NaAnOfGreGaGenMo!

Ah, if only I wasn't already overcommitted...

(There really is a [NaPoGenMo](https://github.com/NaPoGenMo/) too btw, but it's held in April.)"
On Wed Nov 11 2015 09:08:43, WhiteFangs commented: "Sounds really interesting! I'm looking forward to read this!"
On Wed Nov 11 2015 09:33:18, hangedmandesign commented: "It's been a bit tricky extricating the code from the game, and it's likely I'll have to wait til later in the month to make significant progress. Just a bit busy ^_^;
I'm also working on a simple schema for simple procedural descriptions of places and feelings associated with those places, so when the narrator finds themselves at IX-BETH, Cairn of Ice, they'll have something to say about it."
On Wed Nov 11 2015 10:32:10, cpressey commented: "> This ended up with an...interesting situation where Bob picked up Bob, and then ate himself. Alice also picked herself up..and ate herself.

I suppose you could call this a bug, but I fail to see how it is not also extremely entertaining literature."
On Wed Nov 11 2015 12:02:12, muffinista opened a new issue called "A Collection of Failed #NaNoGenMo Ideas". And it's been completed. Sweet!
On Wed Nov 11 2015 12:04:06, enkiv2 commented: "#NaNaNoGenMoEnGenMo ;-)

On Wed, Nov 11, 2015 at 12:02 PM Colin Mitchell <notifications@github.com>
wrote:

> This is not my main submission, but I'm still hashing out how I want my
> primary idea to work. Along the way I've had a few false starts and I joked
> on Twitter that it would be funny if my submission was a log of all my
> failed work.
>
> So, here's a generative version of that.
>
> https://github.com/muffinista/NaNoGenMo2015/blob/master/failed-nanogenmo.md
>
> The code
> <https://github.com/muffinista/NaNoGenMo2015/blob/master/failed-nanogenmo.rb>
> is pretty simple, it generates a random title using the Wordnik API, picks
> a reason for failure from a short list I wrote myself, adds a few
> adjectives also via Wordnik, and picks a random algorithm from a list I
> curated via poking around on http://dbpedia.org/page/Category:Algorithms
>
> The output is nothing amazing but it has a few funny moments, and was a
> fun warmup project. It was also fun to automate a list of failure reasons,
> since I fail a lot and there's a pretty good chance I'll fail at my actual
> idea.
>
> Enjoy!
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/153>.
>
"
On Wed Nov 11 2015 14:04:46, WhiteFangs commented: "I started coding some stuff to crawl the posts, clean the HTML, loop to next pages of a topic and manage the smileys, the repo is here : https://github.com/WhiteFangs/theatre-jeuxvideocom"
On Wed Nov 11 2015 21:45:23, MichaelPaulukonis commented: "ha-hah!
"
On Wed Nov 11 2015 22:41:35, ikarth commented: "### Reading and Writing Electronic Text

The course notes from [this class at NYU's Interactive Telecommunications Program](http://www.decontextualize.com/teaching/rwet/) cover a lot of subjects and are a good introduction if you're wondering where to start. It also includes [lots of example code in this github repository](https://github.com/aparrish/rwet-examples)."
On Thu Nov 12 2015 03:22:55, marythought commented: "## DAY ELEVENTHEN

Some quick text to share, I'm playing with the RiTA RiLexicon to find near replacement words for a classic poem (again with the poems!!! she just won't stop...). My goal here is to generate output that is clearly recognizable, but sounds bananas. 

You might be curious, what is the difference between Rita's RiLexicon methods similarBySound(), similarByLetter(), similarBySoundAndLetter(), and rhymes()? So glad you asked... let's take a look at each of these at play! Each method returns an array of matches, so the computer is choosing a random match (or the original word) each time. 

### Similar by Sound 
<em>Compares the phonemes of the input word (using a version of the min-edit distance algorithm) to each word in the lexicon, returning the set of closest matches.</em> 

Two reeds divert in a yell good,
And soggy I could not trammel berth
And be one travels, pong I staid
And cooked dean one as far as I curd
To where it burnt in the undergrowth;

### Similar by Letter
<em>Compares the characters of the input string (using a version of the min-edit distance algorithm) to each word in the lexicon, returning the set of closest matches.</em> 

Two loads diverged in a fellow wood,
And sorry I could not travel booth
And be one traveler, song I stood
And cooked dawn one as far as I mould
To where it bet in the undergrowth;

### Similar by Sound and Letter
<em>First calls similarBySound(), then filters the result set by the algorithm used in similarByLetter();</em>

Two rods diverge in a bellow good,
And sorry I could not travel bath
And be one traveled, pong I stood
And cooked doan one as far as I could
To where it vent in the underwrote;

### Rhyme
<em>Two words rhyme are considered as rhyming if their final stressed vowel and all following phonemes are identical</em>

Two episodes diverged in a mellow likelihood,
And safari I could not travel both
And be one both, strong I sainthood
And overlooked clown one as far as I withstood
To where it dissent in the undergrowth;

### Verdict
I hadn't tried by letter before this little exercise (thinking the sound would be more important) but I actually like that output the best, here. It does seem to be keeping the sound and rhythm of the word as well. Linguistical coincidence? Edit-distance magick? 

Rhyme is clearly variating greatest from the source text -- this could be fun to play with for replacing end words (or generating new rhyme words) but I won't use it in this "replace nearly every word" exercise.

### Just for fun: Alliteration
<em>Finds alliterations by comparing the phonemes of the input string to those of each word in the lexicon</em>

Two razor diverged in a abuse wings,
And scratch I could not fanatic both
And be one entitled, rebuilding I ceases
And consolidates deathbed one as far as I consul
To where it billionaires in the injuries

^^Yikes, that's dark, RiTA! I won't be using this but watch this:

Two [roads organizational] diverged in a [yellow impugning] [wood whittle],
And [sorry confessing] I could not [travel teapot] [both both]
And be one [traveler tumbler], [long inflict] I [stood autistic]
And [looked apologized] [down down] one as far as I [could clawed]
To where it [bent bittersweet] in the [undergrowth incinerators];

These are the word pairs it's claiming for alliteration. Some are truly weird. I feel like this would need some human editing if you were to use it in text generation, or else I might just throw out anything that doesn't start with the same letter as the base word (those all seem to work well!).

Signing off for now, I'm going to keep working on Bob Frost then see what else I can do in RiTA."
On Thu Nov 12 2015 03:47:18, marythought commented: "#### Bonus: here's the whole poem w/ Similar By Sound replacements:

<img width="440" alt="screen shot 2015-11-12 at 12 45 47 am" src="https://cloud.githubusercontent.com/assets/10136229/11114078/c29ed230-88d6-11e5-8b3a-1422c23b78c0.png">
"
On Thu Nov 12 2015 04:15:24, hugovk commented: "Good stuff!

Those programming languages and algorithms are good candidates for Corpora (if not already there)."
On Thu Nov 12 2015 07:08:54, Hrant-Khachatrian opened a new issue called "Constitution in Armenian generated by char-rnn". And it's been completed. Sweet!
On Thu Nov 12 2015 09:20:39, ikarth commented: "### Templating and Grammars (with Tracery)

A replacement grammar is a set of rules to replace symbols. It works kind of like mad-libs: we start with a sentence like "Was it done by #suspect# in the #room# with the #murder weapon#?", and then the apply a rule that says that every time we see the word #suspect#, replace it with one of the names from our list of suspects.

While this starts out pretty simple, many of the successful novels from past NaNoGenMos have been made using similar techniques. [Aggressive Passive](https://github.com/dariusk/NaNoGenMo/issues/79), [Redwreath and Goldstar Have Traveled to Deathsgate](https://github.com/dariusk/NaNoGenMo/issues/10), [Recipe Book Generator](https://github.com/dariusk/NaNoGenMo-2014/issues/87), and [Threnody for Abraxas](https://github.com/dariusk/NaNoGenMo-2014/issues/134) are a few examples of novels that use recursive replacement grammars in one way or another.

[Tracery](http://www.crystalcodepalace.com/traceryTut.html) is a library that makes replacement grammars easy. You can try out the [interactive tutorial](http://www.crystalcodepalace.com/traceryTut.html) for a playful introduction.

Some [examples of what you can do with Tracery](http://www.crystalcodepalace.com/tracery.html), including [a demo of the in-progress no-programming-required visual editor](http://www.brightspiral.com/tracery/)"
On Thu Nov 12 2015 09:23:05, ikarth commented: "Non-English and topical! Nice addition to the generated library."
On Thu Nov 12 2015 12:32:01, muffinista commented: "Looks like it's not there, I'll submit a PR for that now."
On Thu Nov 12 2015 13:34:34, enkiv2 opened a new issue called "Goal-driven use of scenes and sequels for capers". And it's been completed. Sweet!
On Thu Nov 12 2015 14:45:44, yourpalal commented: "Progress, for anyone who's inerested: I have my corpus being split into 3 semi-detached ngram models: questions/facts/declarations, and can create a question/fact/longer rambling  thing on demand. For instance, a question:

> What if the opponent says “animal”, naming a genus rather than in laboratory designed
> experiments which place the interface between observed and observing system and
> locates it in denying that she is in a view coherent?"

A fact:

> But this release process is called exocytosis.

A declaration:

> Not only is it possible that this should occur — after all, the whole of the quantum-circuit-based computation can be be simply represented as a single unitary transformation from the debate has origins in the hypothesis h 1: balance between inhibitory and excitatory connections between function, structure, distribution and evolutionary origin.

Next up is deciding on how to structure a conversation. I'm thinking either some kind of evolving ngram model where things other people say are added in as they say them, or a reversed-order ngram model to be able to pick words and work backwards to the start of a sentence. Also I will possibly be adding some kind of templating system into this, to add better flow into conversations.
"
On Thu Nov 12 2015 15:05:40, mewo2 opened a new issue called "A travel guide to unknown lands".
On Thu Nov 12 2015 15:16:12, hugovk commented: "These look great!"
On Thu Nov 12 2015 18:38:19, enkiv2 commented: "OK, I got it working well enough (with hard-coded settings and a hard-coded world structure) to generate a novel.

Here's the source: [SceneSequel](https://github.com/enkiv2/NaNoGenMo-2015/blob/master/sceneSequel.py)
Here's the completed novel: [Seven Bad Heists](https://github.com/enkiv2/NaNoGenMo-2015/blob/master/badHeists.md)"
On Thu Nov 12 2015 19:35:59, yourpalal commented: "I'm working on the dialogue now, and generated this interesting tidbit. There's definitely a bug that was causing this, but I kind of like it!


> TEACHER: The fact that they have a dorsal sigmoid bone establishes their food source is nothing like the wonder and pageantry of snarks in mating season, as they gyre and gimble on the ponds of such clocks in the animal kingdom.
TEACHER: 1.
STUDENT: The fact that they have a dorsal sigmoid bone establishes their descent from the slithy toves along with the other bandersnatches.
STUDENT: The fact that they have evolved from the slithy toves along with the other bandersnatches.
TEACHER: The fact that they have evolved from the slithy toves along with the other bandersnatches.
TEACHER: Mating Display
STUDENT: The fact that they have evolved from the slithy toves along with the other bandersnatches.
STUDENT: The fact that they have evolved from the principal model of such clocks in the animal kingdom.
TEACHER: The fact that they have evolved from the principal model of such clocks in the animal kingdom.
TEACHER: The fact that they have evolved from the principal model of such clocks in the animal kingdom.
STUDENT: The fact that they have evolved from the principal model of such clocks in the animal kingdom."
On Thu Nov 12 2015 19:36:13, yourpalal commented: "Another good snippet:

>SOCRATES: You do not understand,
ARISTOTLE (interrupting): Ontological holism in quantum mechanics?
"
On Thu Nov 12 2015 23:55:39, ikarth commented: "That's some speedy execution there. I have to confess, when you first started describing it, it sounded like a lot of work to pull together. I'm impressed.

I really like how it turned out. It's really repetitive at novel length, but that's not a weakness of the structure so much as each section glosses over the details of carrying out the actions. Feels like there's scope to build on it."
On Thu Nov 12 2015 23:58:06, ikarth commented: "> SOCRATES: You do not understand,
ARISTOTLE (interrupting): Ontological holism in quantum mechanics?

This is gold."
On Fri Nov 13 2015 00:03:21, ikarth commented: "Also, it reminds me of a Donald E. Westlake caper. Needs more small time cons and drivers, though."
On Fri Nov 13 2015 05:43:36, kumo commented: "Ok, so this month is too busy for a choose your own adventure style book unfortunately, so I'm going to write a story using Roman Numerals, something like: "I, II III IV, V VI VII, VIII IX X." "
On Fri Nov 13 2015 07:31:58, enkiv2 commented: "I'm thinking of several possible improvements.

One: for each goal, have a set of dependencies (with attached probabilities
-- i.e., if you pose as a museum employee but you didn't purchase a smaller
gun your success rate for posing as a museum employee is skewed downward by
the 0.5% likelihood that nobody notices your giant gun; or, if your wallet
is stolen at the gun store then you can't buy anything until you get your
wallet back).

Two: for each goal and each complication, have some sort of human-readable
descriptive 'texture'.

Three: integrate a little multi-level templating system into the printmsg
calls, so that the structure and vocabulary can exhibit *random* variation.


I'm also thinking that, in order to produce more various stories from the
"steal the jewels" caper-world, I should add another path to getting the
jewels -- say, purchasing a black leather catsuit and a grappling hook and
trying to steal them during the night.


I was able to pull it together quickly only because I really did the bare
minimum subset of work necessary to prove the concept. I literally did a
tiny dependency-free minimax without pruning and then made the choice of
goals probablistic and turned the debugging messages into first-person
narration. (It looks more like a story because 'steal the jewels' isn't a
familiar game. But, I could have done the same thing with tic-tac-toe and
written a story about a guy trying to get three in a row.)

On Fri, Nov 13, 2015 at 12:03 AM Isaac Karth <notifications@github.com>
wrote:

> Also, it reminds me of a Donald E. Westlake caper. Needs more small time
> cons and drivers, though.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/155#issuecomment-156322270>
> .
>
"
On Fri Nov 13 2015 15:20:09, MichaelPaulukonis commented: "@tra38 that sad, looping story has been implemented, with a few other minor features -- mostly regarding grammar of some sections. Could be worse!

I hold out the possibility of using ConceptNet to add .... something to the text -- where the powers relate to the holder, or some sort of descriptions. I dunno. Maybe it's time to move in a different direction for the rest of the month.

A 54K word chunk may be found [here](https://gist.github.com/MichaelPaulukonis/24b0b2bc77e546647784).

And an extract:

```
There came a king who had seven sons, and when the other six went off
to find brides, he kept the youngest with him because he could not
bear to be parted from them all. They were supposed to bring back a
bride for him, as well, but they found a king with six daughters and
wooed them, forgetting their brother. But when they returned, they
passed too close to a woodchuck-mouse's castle, and he turned them
all, both princes and princesses, to stone in a fit of rage.

When they did not return, the king, their father, tried to prevent
their brother from following, but he went.

On the way, he helped a Pegasus-mustang, he helped a otter-Centaur, he
helped a enthusiastic monkey, he helped a lynx-newt, he helped a
cow-hippopotamus, he helped a rat-Imp, he helped a ox-dingo, and gave
a starving mole-Pegasus his horse to eat. The mole-Pegasus let the
prince ride on him, instead, and showed him the woodchuck-mouse's
castle, telling him to go inside. The prince was reluctant fearing the
wrath of the woodchuck-mouse, but the mole-Pegasus consoled him. The
mole-Pegasus persuaded the prince to enter the castle for there he
would encounter not the woodchuck-mouse, but the princess the
woodchuck-mouse kept prisoner.

The princess was very beautiful and the prince wanted to know how he
could kill the woodchuck-mouse and set her and his family free. The
princess said that there was no way, as the woodchuck-mouse did not
keep his heart in his body and therefore could not be killed. When the
woodchuck-mouse returned, the princess hid the prince, and asked the
woodchuck-mouse where he kept his heart. He told her that it was under
the door sill. The prince and princess dug there the next day and
found no heart. The princess strewed flowers over the door sill, and
when the woodchuck-mouse returned, told him that it was because his
heart lay there. The woodchuck-mouse admitted it wasn't there and told
her it was in the cupboard. As before, the princess and the prince
searched, to no avail; once again, the princess strewed garlands of
flowers on the cupboard and told the woodchuck-mouse it was because
his heart was there. Thereupon the woodchuck-mouse revealed to her
that, in fact, there was a malodours woods with a whale-cheetah,
beyond that there was a daunting glacier foreland with a suspicious
llama, beyond that there was a vast and favorable terracettes with a
ibex-dog, beyond that there was a vast and favorable rapid with a
repugnant gorilla, beyond that there was a daunting oceanic plateau
with a marmoset-Succubus, beyond that there was a stinky plunge pool
with a Siren-mustang, beyond that there was a moderately-sized complex
crater with a unfriendly hog, beyond that there was a gross but not
altogether repulsive delta with a ape-koala, and in the ape-koala's
nest was an egg; and in the egg was the woodchuck-mouse's heart.

The prince rode to the malodours woods, where he was menaced by a
whale-cheetah. The prince called on the Pegasus-mustang to defeat the
whale-cheetah. The Pegasus-mustang defeated the whale-cheetah by using
its Water Veil. The prince rode on to the daunting glacier foreland,
where he was menaced by a suspicious llama. The prince called on the
otter-Centaur to defeat the suspicious llama. The otter-Centaur
defeated the suspicious llama by using its Wetland Adaptation. The
prince rode to the vast and favorable terracettes, where he was
menaced by a ibex-dog. The prince called on the enthusiastic monkey to
defeat the ibex-dog. The enthusiastic monkey defeated the ibex-dog by
using its Fish Manipulation. The prince rode to the vast and favorable
rapid, where he was menaced by a repugnant gorilla. The prince called
on the lynx-newt to defeat the repugnant gorilla. The lynx-newt
defeated the repugnant gorilla by using its Magic Bounce. The prince
rode to the daunting oceanic plateau, where he was menaced by a
marmoset-Succubus. The prince called on the cow-hippopotamus to defeat
the marmoset-Succubus. The cow-hippopotamus defeated the
marmoset-Succubus by using its Norse Deity Physiology. The prince rode
to the stinky plunge pool, where he was menaced by a Siren-mustang.
The prince called on the rat-Imp to defeat the Siren-mustang. The
rat-Imp defeated the Siren-mustang by using its Night Vision. The
prince rode to the moderately-sized complex crater, where he was
menaced by a unfriendly hog. The prince called on the ox-dingo to
defeat the unfriendly hog. The ox-dingo defeated the unfriendly hog by
using its White Smoke. The prince rode to the gross but not altogether
repulsive delta, where he was menaced by a ape-koala. The prince
called on the mole-Pegasus to defeat the ape-koala. The mole-Pegasus
defeated the ape-koala by using its Sonic Scream. The mole-Pegasus
plucked the egg from the nest of the ape-koala, gave it to the prince,
and told him to squeeze it. When he did, the woodchuck-mouse screamed.
The mole-Pegasus told him to squeeze it again, and the woodchuck-mouse
promised anything if he would spare his life. The prince told him to
change his brothers and their brides back to life, and the
woodchuck-mouse did so. Then the prince squeezed the egg into two and
went home with the woodchuck-mouse's captive princess as his bride;
accompanying him were his brothers and their brides, and the king
rejoiced.

Eventually, the prince, who lived a long an happy life, found his
happiness slipping from his fingers. In time, his heart became
hardened, his rule became corrupt, and he became a stupid gila
monster.




There came a king who had seven sons, and when the other six went off
to find brides, he kept the youngest with him because he could not
bear to be parted from them all. They were supposed to bring back a
bride for him, as well, but they found a king with six daughters and
wooed them, forgetting their brother. But when they returned, they
passed too close to a stupid gila monster's castle, and he turned them
all, both princes and princesses, to stone in a fit of rage.

When they did not return, the king, their father, tried to prevent
their brother from following, but he went.

[....]

```"
On Fri Nov 13 2015 15:24:12, MichaelPaulukonis commented: "It often comes down to pruning our plans."
On Fri Nov 13 2015 15:26:03, MichaelPaulukonis commented: "`novel` means **new**, gosh-darn-it!"
On Fri Nov 13 2015 15:34:36, enkiv2 commented: "So, I added improvement number two for goals and subgoals (texture will be
chosen for a goal-subgoal pair or for a goal if the subgoal's texture
doesn't exist, for a general description plus an extra piece of prose for
success or failure, and this overrides default success/failure
notification). I also added a route wherein the protagonist buys a black
leather catsuit and/or a grappling hook and tries to sneak into the museum
at night.

*This is the story of that time I decided to try and steal them jewels.*

*So, I thought, what if I tried to steal them jewels by trying to go to the
ninja supply store... So, I thought, what if I tried to steal them jewels
by trying to purchase a grappling hook... So, I thought, what if I tried to
steal them jewels by trying to sneak into the museum at night... So, I
thought, what if I tried to steal them jewels by trying to go to the
hospital... I'll try to remember that. *

*So, I figured, if I tried to steal them jewels by trying to sneak into the
museum at night I'd have maybe a 40% chance of succeding. I'll try to
remember that. *

*If I'm trying to purchase a grappling hook, what if in order to steal them
jewels, I tried to sneak into the museum at night. That has about a 3 in 10
chance of working. So, I thought, what if I tried to steal them jewels by
trying to go to the ninja supply store... So, I thought, what if I tried to
steal them jewels by trying to purchase a grappling hook... So, I thought,
what if I tried to steal them jewels by trying to sneak into the museum at
night... I already figured that if I tried to steal them jewels by trying
to sneak into the museum at night I'd only have about a 4 in 10 chance of
succeeding. If I'm trying to purchase a grappling hook, what if in order to
steal them jewels, I tried to sneak into the museum at night. That has
about a 3 in 10 chance of working. So, I thought, what if I tried to steal
them jewels by trying to go to the ninja supply store... So, I thought,
what if I tried to steal them jewels by trying to purchase a grappling
hook... So, I thought, what if I tried to steal them jewels by trying to
sneak into the museum at night... I already figured that if I tried to
steal them jewels by trying to sneak into the museum at night I'd only have
about a 4 in 10 chance of succeeding. If I'm trying to purchase a grappling
hook, what if in order to steal them jewels, I tried to sneak into the
museum at night. That has about a 3 in 10 chance of working. So, I thought,
what if I tried to steal them jewels by trying to go to the ninja supply
store... Geez, this is complicated. I can't think more than 5 moves ahead!*

*So, I figured, if I tried to steal them jewels by trying to purchase a
grappling hook I'd have maybe a 18% chance of succeding. I'll try to
remember that. *

*If I'm trying to go to the ninja supply store, what if in order to steal
them jewels, I tried to purchase a grappling hook. That has about a 1 in 10
chance of working. So, I thought, what if I tried to steal them jewels by
trying to purchase a black leather catsuit... So, I thought, what if I
tried to steal them jewels by trying to sneak into the museum at night... I
already figured that if I tried to steal them jewels by trying to sneak
into the museum at night I'd only have about a 4 in 10 chance of
succeeding. If I'm trying to purchase a black leather catsuit, what if in
order to steal them jewels, I tried to sneak into the museum at night. That
has about a 3 in 10 chance of working. So, I thought, what if I tried to
steal them jewels by trying to go to the ninja supply store... Geez, this
is complicated. I can't think more than 5 moves ahead!*

*So, I figured, if I tried to steal them jewels by trying to purchase a
black leather catsuit I'd have maybe a 16% chance of succeding. I'll try to
remember that. *

*On the other hand, if I try to purchase a black leather catsuit it'll give
me a 1 in 10 chance of succeeding. I'll try to remember that. *

*So, I figured, if I tried to steal them jewels by trying to purchase a
grappling hook I'd have maybe a 20% chance of succeding. If I'm trying to
go to the ninja supply store, what if in order to steal them jewels, I
tried to purchase a grappling hook. That has about a 1 in 10 chance of
working. So, I thought, what if I tried to steal them jewels by trying to
purchase a black leather catsuit... I already figured that if I tried to
steal them jewels by trying to purchase a black leather catsuit I'd only
have about a 1 in 10 chance of succeeding. On the other hand, if I try to
purchase a black leather catsuit it'll give me a 1 in 10 chance of
succeeding. So, I figured, if I tried to steal them jewels by trying to
purchase a grappling hook I'd have maybe a 21% chance of succeding. If I'm
trying to go to the ninja supply store, what if in order to steal them
jewels, I tried to purchase a grappling hook. That has about a 1 in 10
chance of working. So, I thought, what if I tried to steal them jewels by
trying to purchase a black leather catsuit... I already figured that if I
tried to steal them jewels by trying to purchase a black leather catsuit
I'd only have about a 1 in 10 chance of succeeding. On the other hand, if I
try to purchase a black leather catsuit it'll give me a 1 in 10 chance of
succeeding. So, I thought, what if I tried to steal them jewels by trying
to get a museum uniform... So, I thought, what if I tried to steal them
jewels by trying to pass as a museum employee... So, I thought, what if I
tried to steal them jewels by trying to go to the hospital... So, I
figured, if I tried to steal them jewels by trying to pass as a museum
employee I'd have maybe a 35% chance of succeding. I'll try to remember
that. *

*If I'm trying to get a museum uniform, what if in order to steal them
jewels, I tried to pass as a museum employee. That has about a 1 in 10
chance of working. I'll try to remember that. *

*The costume shop was tucked into a strip mall down town, between a
laundromat and a chinese take-out place. It smelled like soap. I also have
to steal them jewels. After looking through the racks several times, I
finally decided to ask the cashier -- a wrinkled but plump old woman with a
puff of curly white hair -- if she carried museum employee uniforms. She
shook her head, and I left, dejected. Now I have to get a smaller gun. I
still need to steal them jewels. *

*So, I thought, what if I tried to get a smaller gun by trying to go to the
ninja supply store... So, I thought, what if I tried to get a smaller gun
by trying to purchase a grappling hook... So, I thought, what if I tried to
get a smaller gun by trying to sneak into the museum at night... So, I
thought, what if I tried to get a smaller gun by trying to go to the
hospital... So, I thought, what if I tried to get a smaller gun by trying
to steal them jewels... I'll try to remember that. *

*So, I thought, what if I tried to get a smaller gun by trying to go to the
ninja supply store... So, I thought, what if I tried to get a smaller gun
by trying to purchase a grappling hook... So, I thought, what if I tried to
get a smaller gun by trying to sneak into the museum at night... So, I
thought, what if I tried to get a smaller gun by trying to go to the ninja
supply store... So, I thought, what if I tried to get a smaller gun by
trying to purchase a grappling hook... So, I thought, what if I tried to
get a smaller gun by trying to sneak into the museum at night... So, I
thought, what if I tried to get a smaller gun by trying to go to the ninja
supply store... Geez, this is complicated. I can't think more than 5 moves
ahead!*

*So, I thought, what if I tried to get a smaller gun by trying to purchase
a black leather catsuit... So, I thought, what if I tried to get a smaller
gun by trying to sneak into the museum at night... So, I thought, what if I
tried to get a smaller gun by trying to go to the ninja supply store...
Geez, this is complicated. I can't think more than 5 moves ahead!*

*So, I thought, what if I tried to get a smaller gun by trying to purchase
a black leather catsuit... So, I thought, what if I tried to steal them
jewels by trying to go to the ninja supply store... So, I thought, what if
I tried to get a smaller gun by trying to get a museum uniform... So, I
thought, what if I tried to get a smaller gun by trying to pass as a museum
employee... So, I thought, what if I tried to get a smaller gun by trying
to go to the hospital... So, I thought, what if I tried to get a smaller
gun by trying to steal them jewels... So, I thought, what if I tried to
steal them jewels by trying to get a museum uniform... The ninja supply
shop was in the middle of the second floor of the mall, between a Hot Topic
and a Zappo's. It was dimly lit, and the scuffed floors had a fake
tatami-pattern print. There was a wad of gum stuck to the doorway. I also
have to get a smaller gun. I also have to steal them jewels. *

*I totally succeeded in my attempt to go to the ninja supply store by
trying to go about it the obvious way. Yay! *

*So, I thought, what if I tried to get a smaller gun by trying to purchase
a grappling hook... So, I thought, what if I tried to steal them jewels by
trying to purchase a grappling hook... I already figured that if I tried to
steal them jewels by trying to purchase a grappling hook I'd only have
about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
a smaller gun by trying to purchase a black leather catsuit... So, I
thought, what if I tried to steal them jewels by trying to purchase a black
leather catsuit... I already figured that if I tried to steal them jewels
by trying to purchase a black leather catsuit I'd only have about a 1 in 10
chance of succeeding. I also have to get a smaller gun. I also have to
steal them jewels. All the black leather cat suits they had in stock were
way too big for me. *

*So, I thought, what if I tried to get a smaller gun by trying to purchase
a grappling hook... So, I thought, what if I tried to steal them jewels by
trying to purchase a grappling hook... I already figured that if I tried to
steal them jewels by trying to purchase a grappling hook I'd only have
about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
a smaller gun by trying to purchase a black leather catsuit... So, I
thought, what if I tried to steal them jewels by trying to purchase a black
leather catsuit... I already figured that if I tried to steal them jewels
by trying to purchase a black leather catsuit I'd only have about a 1 in 10
chance of succeeding. I also have to get a smaller gun. I also have to
steal them jewels. I spent twenty minutes looking through the discount bin,
before finding an absolutely perfect grappling hook for thirty cents. When
I went up to pay for it, the cashier waved me off -- no charge. *

*So, I thought, what if I tried to get a smaller gun by trying to sneak
into the museum at night... So, I thought, what if I tried to steal them
jewels by trying to sneak into the museum at night... I already figured
that if I tried to steal them jewels by trying to sneak into the museum at
night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
what if I tried to get a smaller gun by trying to go to the ninja supply
store... So, I thought, what if I tried to steal them jewels by trying to
go to the ninja supply store... I also have to get a smaller gun. I also
have to steal them jewels. *

*I failed to sneak into the museum at night while trying to purchase a
grappling hook. Bummer. *

*So, I thought, what if I tried to get a smaller gun by trying to sneak
into the museum at night... So, I thought, what if I tried to steal them
jewels by trying to sneak into the museum at night... I already figured
that if I tried to steal them jewels by trying to sneak into the museum at
night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
what if I tried to get a smaller gun by trying to go to the ninja supply
store... So, I thought, what if I tried to steal them jewels by trying to
go to the ninja supply store... I also have to get a smaller gun. I also
have to steal them jewels. *

*I failed to sneak into the museum at night while trying to purchase a
grappling hook. Bummer. *

*So, I thought, what if I tried to get a smaller gun by trying to purchase
a grappling hook... So, I thought, what if I tried to steal them jewels by
trying to purchase a grappling hook... I already figured that if I tried to
steal them jewels by trying to purchase a grappling hook I'd only have
about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
a smaller gun by trying to purchase a black leather catsuit... So, I
thought, what if I tried to steal them jewels by trying to purchase a black
leather catsuit... I already figured that if I tried to steal them jewels
by trying to purchase a black leather catsuit I'd only have about a 1 in 10
chance of succeeding. I also have to get a smaller gun. I also have to
steal them jewels. All the black leather cat suits they had in stock were
way too big for me. *

*So, I thought, what if I tried to get a smaller gun by trying to purchase
a grappling hook... So, I thought, what if I tried to steal them jewels by
trying to purchase a grappling hook... I already figured that if I tried to
steal them jewels by trying to purchase a grappling hook I'd only have
about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
a smaller gun by trying to purchase a black leather catsuit... So, I
thought, what if I tried to steal them jewels by trying to purchase a black
leather catsuit... I already figured that if I tried to steal them jewels
by trying to purchase a black leather catsuit I'd only have about a 1 in 10
chance of succeeding. I also have to get a smaller gun. I also have to
steal them jewels. "Are there any grappling hooks in stock?" The cashier,
impassive behind his mask, shook his head slowly in response to my
question. Then, after a moment of staring at me, he threw a smoke bomb at
his feet. I found myself outside the shop, which was now locked. *

*So, I thought, what if I tried to get a smaller gun by trying to go to the
ninja supply store... So, I thought, what if I tried to steal them jewels
by trying to go to the ninja supply store... So, I thought, what if I tried
to get a smaller gun by trying to get a museum uniform... So, I thought,
what if I tried to steal them jewels by trying to get a museum uniform...
The ninja supply shop was in the middle of the second floor of the mall,
between a Hot Topic and a Zappo's. It was dimly lit, and the scuffed floors
had a fake tatami-pattern print. There was a wad of gum stuck to the
doorway. I also have to get a smaller gun. I also have to steal them
jewels. *

*I totally succeeded in my attempt to go to the ninja supply store by
trying to go about it the obvious way. Yay! *
*So, I thought, what if I tried to get a smaller gun by trying to purchase
a grappling hook... So, I thought, what if I tried to steal them jewels by
trying to purchase a grappling hook... I already figured that if I tried to
steal them jewels by trying to purchase a grappling hook I'd only have
about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
a smaller gun by trying to purchase a black leather catsuit... So, I
thought, what if I tried to steal them jewels by trying to purchase a black
leather catsuit... I already figured that if I tried to steal them jewels
by trying to purchase a black leather catsuit I'd only have about a 1 in 10
chance of succeeding. I also have to get a smaller gun. I also have to
steal them jewels. There was a grappling hook with two hundred feet of rope
sitting right behind the counter, on display. *

*So, I thought, what if I tried to get a smaller gun by trying to sneak
into the museum at night... So, I thought, what if I tried to steal them
jewels by trying to sneak into the museum at night... I already figured
that if I tried to steal them jewels by trying to sneak into the museum at
night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
what if I tried to get a smaller gun by trying to go to the ninja supply
store... So, I thought, what if I tried to steal them jewels by trying to
go to the ninja supply store... I also have to get a smaller gun. I also
have to steal them jewels. *

*I failed to sneak into the museum at night while trying to purchase a
grappling hook. Bummer. *

*So, I thought, what if I tried to get a smaller gun by trying to sneak
into the museum at night... So, I thought, what if I tried to steal them
jewels by trying to sneak into the museum at night... I already figured
that if I tried to steal them jewels by trying to sneak into the museum at
night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
what if I tried to get a smaller gun by trying to go to the ninja supply
store... So, I thought, what if I tried to steal them jewels by trying to
go to the ninja supply store... The ninja supply shop was in the middle of
the second floor of the mall, between a Hot Topic and a Zappo's. It was
dimly lit, and the scuffed floors had a fake tatami-pattern print. There
was a wad of gum stuck to the doorway. I also have to get a smaller gun. I
also have to steal them jewels. *

*I totally succeeded in my attempt to go to the ninja supply store by
trying to purchase a grappling hook. Yay! *

*So, I thought, what if I tried to get a smaller gun by trying to purchase
a grappling hook... So, I thought, what if I tried to steal them jewels by
trying to purchase a grappling hook... I already figured that if I tried to
steal them jewels by trying to purchase a grappling hook I'd only have
about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
a smaller gun by trying to purchase a black leather catsuit... So, I
thought, what if I tried to steal them jewels by trying to purchase a black
leather catsuit... I already figured that if I tried to steal them jewels
by trying to purchase a black leather catsuit I'd only have about a 1 in 10
chance of succeeding. I also have to get a smaller gun. I also have to
steal them jewels. A beautiful black leather catsuit greeted me from the
rack to the left of the doorway. *

*So, I thought, what if I tried to get a smaller gun by trying to sneak
into the museum at night... So, I thought, what if I tried to steal them
jewels by trying to sneak into the museum at night... I already figured
that if I tried to steal them jewels by trying to sneak into the museum at
night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
what if I tried to get a smaller gun by trying to go to the ninja supply
store... So, I thought, what if I tried to steal them jewels by trying to
go to the ninja supply store... I also have to get a smaller gun. I also
have to steal them jewels. *

*I failed to sneak into the museum at night while trying to purchase a
black leather catsuit. Bummer. *

*So, I thought, what if I tried to get a smaller gun by trying to sneak
into the museum at night... So, I thought, what if I tried to steal them
jewels by trying to sneak into the museum at night... I already figured
that if I tried to steal them jewels by trying to sneak into the museum at
night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
what if I tried to get a smaller gun by trying to go to the ninja supply
store... So, I thought, what if I tried to steal them jewels by trying to
go to the ninja supply store... I also have to get a smaller gun. I also
have to steal them jewels. *

*I failed to sneak into the museum at night while trying to purchase a
black leather catsuit. Bummer. *

*So, I thought, what if I tried to get a smaller gun by trying to purchase
a grappling hook... So, I thought, what if I tried to steal them jewels by
trying to purchase a grappling hook... I already figured that if I tried to
steal them jewels by trying to purchase a grappling hook I'd only have
about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
a smaller gun by trying to purchase a black leather catsuit... So, I
thought, what if I tried to steal them jewels by trying to purchase a black
leather catsuit... I already figured that if I tried to steal them jewels
by trying to purchase a black leather catsuit I'd only have about a 1 in 10
chance of succeeding. I also have to get a smaller gun. I also have to
steal them jewels. "Are there any grappling hooks in stock?" The cashier,
impassive behind his mask, shook his head slowly in response to my
question. Then, after a moment of staring at me, he threw a smoke bomb at
his feet. I found myself outside the shop, which was now locked. *

*So, I thought, what if I tried to get a smaller gun by trying to purchase
a grappling hook... So, I thought, what if I tried to steal them jewels by
trying to purchase a grappling hook... I already figured that if I tried to
steal them jewels by trying to purchase a grappling hook I'd only have
about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
a smaller gun by trying to purchase a black leather catsuit... So, I
thought, what if I tried to steal them jewels by trying to purchase a black
leather catsuit... I already figured that if I tried to steal them jewels
by trying to purchase a black leather catsuit I'd only have about a 1 in 10
chance of succeeding. I also have to get a smaller gun. I also have to
steal them jewels. There was a grappling hook with two hundred feet of rope
sitting right behind the counter, on display. *

*So, I thought, what if I tried to get a smaller gun by trying to sneak
into the museum at night... So, I thought, what if I tried to steal them
jewels by trying to sneak into the museum at night... I already figured
that if I tried to steal them jewels by trying to sneak into the museum at
night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
what if I tried to get a smaller gun by trying to go to the ninja supply
store... So, I thought, what if I tried to steal them jewels by trying to
go to the ninja supply store... The ninja supply shop was in the middle of
the second floor of the mall, between a Hot Topic and a Zappo's. It was
dimly lit, and the scuffed floors had a fake tatami-pattern print. There
was a wad of gum stuck to the doorway. I also have to get a smaller gun. I
also have to steal them jewels. *

*I totally succeeded in my attempt to go to the ninja supply store by
trying to purchase a grappling hook. Yay! *

*So, I thought, what if I tried to get a smaller gun by trying to purchase
a grappling hook... So, I thought, what if I tried to steal them jewels by
trying to purchase a grappling hook... I already figured that if I tried to
steal them jewels by trying to purchase a grappling hook I'd only have
about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
a smaller gun by trying to purchase a black leather catsuit... So, I
thought, what if I tried to steal them jewels by trying to purchase a black
leather catsuit... I already figured that if I tried to steal them jewels
by trying to purchase a black leather catsuit I'd only have about a 1 in 10
chance of succeeding. I also have to get a smaller gun. I also have to
steal them jewels. A beautiful black leather catsuit greeted me from the
rack to the left of the doorway. *

*So, I thought, what if I tried to get a smaller gun by trying to sneak
into the museum at night... So, I thought, what if I tried to steal them
jewels by trying to sneak into the museum at night... I already figured
that if I tried to steal them jewels by trying to sneak into the museum at
night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
what if I tried to get a smaller gun by trying to go to the ninja supply
store... So, I thought, what if I tried to steal them jewels by trying to
go to the ninja supply store... I also have to get a smaller gun. I also
have to steal them jewels. *

*I failed to sneak into the museum at night while trying to purchase a
black leather catsuit. Bummer. *

*So, I thought, what if I tried to get a smaller gun by trying to sneak
into the museum at night... So, I thought, what if I tried to steal them
jewels by trying to sneak into the museum at night... I already figured
that if I tried to steal them jewels by trying to sneak into the museum at
night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
what if I tried to get a smaller gun by trying to go to the ninja supply
store... So, I thought, what if I tried to steal them jewels by trying to
go to the ninja supply store... I also have to get a smaller gun. I also
have to steal them jewels. *

*I totally succeeded in my attempt to sneak into the museum at night by
trying to purchase a black leather catsuit. Yay! *

*So, I thought, what if I tried to get a smaller gun by trying to go to the
hospital... So, I thought, what if I tried to steal them jewels by trying
to go to the hospital... So, I thought, what if I tried to get a smaller
gun by trying to steal them jewels... So, I thought, what if I tried to
steal them jewels by trying to steal them jewels... It turns out there's no
way to steal them jewels by trying to go to the hospital after you already
tried to sneak into the museum at night. *

*I also have to get a smaller gun. *

*I failed to steal them jewels while trying to sneak into the museum at
night. Bummer. Now I have to heal my chest wound. I still need to get a
smaller gun. I also still need to steal them jewels. *

*So, I thought, what if I tried to get a smaller gun by trying to go to the
hospital... So, I thought, what if I tried to heal my chest wound by trying
to go to the hospital... So, I figured, if I tried to heal my chest wound
by trying to go to the hospital I'd have maybe a 17% chance of succeding.
So, I thought, what if I tried to steal them jewels by trying to go to the
hospital... So, I thought, what if I tried to get a smaller gun by trying
to steal them jewels... So, I thought, what if I tried to heal my chest
wound by trying to steal them jewels... So, I thought, what if I tried to
steal them jewels by trying to steal them jewels... I also have to get a
smaller gun. I also have to heal my chest wound. *

*I totally succeeded in my attempt to steal them jewels by trying to sneak
into the museum at night. Yay! Now I no longer need to steal them jewels.
Now I have to heal my arm wound. Now I have to heal my leg wound, too. I
still need to get a smaller gun. I also still need to heal my chest wound. *

*THE END*


On Fri, Nov 13, 2015 at 7:31 AM John Ohno <john.ohno@gmail.com> wrote:

> I'm thinking of several possible improvements.
>
> One: for each goal, have a set of dependencies (with attached
> probabilities -- i.e., if you pose as a museum employee but you didn't
> purchase a smaller gun your success rate for posing as a museum employee is
> skewed downward by the 0.5% likelihood that nobody notices your giant gun;
> or, if your wallet is stolen at the gun store then you can't buy anything
> until you get your wallet back).
>
> Two: for each goal and each complication, have some sort of human-readable
> descriptive 'texture'.
>
> Three: integrate a little multi-level templating system into the printmsg
> calls, so that the structure and vocabulary can exhibit *random* variation.
>
>
> I'm also thinking that, in order to produce more various stories from the
> "steal the jewels" caper-world, I should add another path to getting the
> jewels -- say, purchasing a black leather catsuit and a grappling hook and
> trying to steal them during the night.
>
>
> I was able to pull it together quickly only because I really did the bare
> minimum subset of work necessary to prove the concept. I literally did a
> tiny dependency-free minimax without pruning and then made the choice of
> goals probablistic and turned the debugging messages into first-person
> narration. (It looks more like a story because 'steal the jewels' isn't a
> familiar game. But, I could have done the same thing with tic-tac-toe and
> written a story about a guy trying to get three in a row.)
>
> On Fri, Nov 13, 2015 at 12:03 AM Isaac Karth <notifications@github.com>
> wrote:
>
>> Also, it reminds me of a Donald E. Westlake caper. Needs more small time
>> cons and drivers, though.
>>
>> —
>> Reply to this email directly or view it on GitHub
>> <https://github.com/dariusk/NaNoGenMo-2015/issues/155#issuecomment-156322270>
>> .
>>
>
"
On Fri Nov 13 2015 16:25:27, enkiv2 commented: "Implemented a limited version of improvement number one (dependencies).
Now, the protagonist won't even consider trying to steal the jewels by
going to the hospital (for instance). This is called 'goal_reqs': the
planner ignores any path wherein the list of required goals doesn't have
the appropriate intersection with the goal pool. I also implemented plain
old "reqs", wherein an attempt can fail based on required items not
existing (so, you can't pose as a museum employee without at one point
having successfully purchased a museum employee uniform, and you need
either a grappling hook or a black leather catsuit to sneak into the museum
at night but you can do so after doing something else).

On Fri, Nov 13, 2015 at 3:34 PM John Ohno <john.ohno@gmail.com> wrote:

> So, I added improvement number two for goals and subgoals (texture will be
> chosen for a goal-subgoal pair or for a goal if the subgoal's texture
> doesn't exist, for a general description plus an extra piece of prose for
> success or failure, and this overrides default success/failure
> notification). I also added a route wherein the protagonist buys a black
> leather catsuit and/or a grappling hook and tries to sneak into the museum
> at night.
>
> *This is the story of that time I decided to try and steal them jewels.*
>
> *So, I thought, what if I tried to steal them jewels by trying to go to
> the ninja supply store... So, I thought, what if I tried to steal them
> jewels by trying to purchase a grappling hook... So, I thought, what if I
> tried to steal them jewels by trying to sneak into the museum at night...
> So, I thought, what if I tried to steal them jewels by trying to go to the
> hospital... I'll try to remember that. *
>
> *So, I figured, if I tried to steal them jewels by trying to sneak into
> the museum at night I'd have maybe a 40% chance of succeding. I'll try to
> remember that. *
>
> *If I'm trying to purchase a grappling hook, what if in order to steal
> them jewels, I tried to sneak into the museum at night. That has about a 3
> in 10 chance of working. So, I thought, what if I tried to steal them
> jewels by trying to go to the ninja supply store... So, I thought, what if
> I tried to steal them jewels by trying to purchase a grappling hook... So,
> I thought, what if I tried to steal them jewels by trying to sneak into the
> museum at night... I already figured that if I tried to steal them jewels
> by trying to sneak into the museum at night I'd only have about a 4 in 10
> chance of succeeding. If I'm trying to purchase a grappling hook, what if
> in order to steal them jewels, I tried to sneak into the museum at night.
> That has about a 3 in 10 chance of working. So, I thought, what if I tried
> to steal them jewels by trying to go to the ninja supply store... So, I
> thought, what if I tried to steal them jewels by trying to purchase a
> grappling hook... So, I thought, what if I tried to steal them jewels by
> trying to sneak into the museum at night... I already figured that if I
> tried to steal them jewels by trying to sneak into the museum at night I'd
> only have about a 4 in 10 chance of succeeding. If I'm trying to purchase a
> grappling hook, what if in order to steal them jewels, I tried to sneak
> into the museum at night. That has about a 3 in 10 chance of working. So, I
> thought, what if I tried to steal them jewels by trying to go to the ninja
> supply store... Geez, this is complicated. I can't think more than 5 moves
> ahead!*
>
> *So, I figured, if I tried to steal them jewels by trying to purchase a
> grappling hook I'd have maybe a 18% chance of succeding. I'll try to
> remember that. *
>
> *If I'm trying to go to the ninja supply store, what if in order to steal
> them jewels, I tried to purchase a grappling hook. That has about a 1 in 10
> chance of working. So, I thought, what if I tried to steal them jewels by
> trying to purchase a black leather catsuit... So, I thought, what if I
> tried to steal them jewels by trying to sneak into the museum at night... I
> already figured that if I tried to steal them jewels by trying to sneak
> into the museum at night I'd only have about a 4 in 10 chance of
> succeeding. If I'm trying to purchase a black leather catsuit, what if in
> order to steal them jewels, I tried to sneak into the museum at night. That
> has about a 3 in 10 chance of working. So, I thought, what if I tried to
> steal them jewels by trying to go to the ninja supply store... Geez, this
> is complicated. I can't think more than 5 moves ahead!*
>
> *So, I figured, if I tried to steal them jewels by trying to purchase a
> black leather catsuit I'd have maybe a 16% chance of succeding. I'll try to
> remember that. *
>
> *On the other hand, if I try to purchase a black leather catsuit it'll
> give me a 1 in 10 chance of succeeding. I'll try to remember that. *
>
> *So, I figured, if I tried to steal them jewels by trying to purchase a
> grappling hook I'd have maybe a 20% chance of succeding. If I'm trying to
> go to the ninja supply store, what if in order to steal them jewels, I
> tried to purchase a grappling hook. That has about a 1 in 10 chance of
> working. So, I thought, what if I tried to steal them jewels by trying to
> purchase a black leather catsuit... I already figured that if I tried to
> steal them jewels by trying to purchase a black leather catsuit I'd only
> have about a 1 in 10 chance of succeeding. On the other hand, if I try to
> purchase a black leather catsuit it'll give me a 1 in 10 chance of
> succeeding. So, I figured, if I tried to steal them jewels by trying to
> purchase a grappling hook I'd have maybe a 21% chance of succeding. If I'm
> trying to go to the ninja supply store, what if in order to steal them
> jewels, I tried to purchase a grappling hook. That has about a 1 in 10
> chance of working. So, I thought, what if I tried to steal them jewels by
> trying to purchase a black leather catsuit... I already figured that if I
> tried to steal them jewels by trying to purchase a black leather catsuit
> I'd only have about a 1 in 10 chance of succeeding. On the other hand, if I
> try to purchase a black leather catsuit it'll give me a 1 in 10 chance of
> succeeding. So, I thought, what if I tried to steal them jewels by trying
> to get a museum uniform... So, I thought, what if I tried to steal them
> jewels by trying to pass as a museum employee... So, I thought, what if I
> tried to steal them jewels by trying to go to the hospital... So, I
> figured, if I tried to steal them jewels by trying to pass as a museum
> employee I'd have maybe a 35% chance of succeding. I'll try to remember
> that. *
>
> *If I'm trying to get a museum uniform, what if in order to steal them
> jewels, I tried to pass as a museum employee. That has about a 1 in 10
> chance of working. I'll try to remember that. *
>
> *The costume shop was tucked into a strip mall down town, between a
> laundromat and a chinese take-out place. It smelled like soap. I also have
> to steal them jewels. After looking through the racks several times, I
> finally decided to ask the cashier -- a wrinkled but plump old woman with a
> puff of curly white hair -- if she carried museum employee uniforms. She
> shook her head, and I left, dejected. Now I have to get a smaller gun. I
> still need to steal them jewels. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to go to
> the ninja supply store... So, I thought, what if I tried to get a smaller
> gun by trying to purchase a grappling hook... So, I thought, what if I
> tried to get a smaller gun by trying to sneak into the museum at night...
> So, I thought, what if I tried to get a smaller gun by trying to go to the
> hospital... So, I thought, what if I tried to get a smaller gun by trying
> to steal them jewels... I'll try to remember that. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to go to
> the ninja supply store... So, I thought, what if I tried to get a smaller
> gun by trying to purchase a grappling hook... So, I thought, what if I
> tried to get a smaller gun by trying to sneak into the museum at night...
> So, I thought, what if I tried to get a smaller gun by trying to go to the
> ninja supply store... So, I thought, what if I tried to get a smaller gun
> by trying to purchase a grappling hook... So, I thought, what if I tried to
> get a smaller gun by trying to sneak into the museum at night... So, I
> thought, what if I tried to get a smaller gun by trying to go to the ninja
> supply store... Geez, this is complicated. I can't think more than 5 moves
> ahead!*
>
> *So, I thought, what if I tried to get a smaller gun by trying to purchase
> a black leather catsuit... So, I thought, what if I tried to get a smaller
> gun by trying to sneak into the museum at night... So, I thought, what if I
> tried to get a smaller gun by trying to go to the ninja supply store...
> Geez, this is complicated. I can't think more than 5 moves ahead!*
>
> *So, I thought, what if I tried to get a smaller gun by trying to purchase
> a black leather catsuit... So, I thought, what if I tried to steal them
> jewels by trying to go to the ninja supply store... So, I thought, what if
> I tried to get a smaller gun by trying to get a museum uniform... So, I
> thought, what if I tried to get a smaller gun by trying to pass as a museum
> employee... So, I thought, what if I tried to get a smaller gun by trying
> to go to the hospital... So, I thought, what if I tried to get a smaller
> gun by trying to steal them jewels... So, I thought, what if I tried to
> steal them jewels by trying to get a museum uniform... The ninja supply
> shop was in the middle of the second floor of the mall, between a Hot Topic
> and a Zappo's. It was dimly lit, and the scuffed floors had a fake
> tatami-pattern print. There was a wad of gum stuck to the doorway. I also
> have to get a smaller gun. I also have to steal them jewels. *
>
> *I totally succeeded in my attempt to go to the ninja supply store by
> trying to go about it the obvious way. Yay! *
>
> *So, I thought, what if I tried to get a smaller gun by trying to purchase
> a grappling hook... So, I thought, what if I tried to steal them jewels by
> trying to purchase a grappling hook... I already figured that if I tried to
> steal them jewels by trying to purchase a grappling hook I'd only have
> about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
> a smaller gun by trying to purchase a black leather catsuit... So, I
> thought, what if I tried to steal them jewels by trying to purchase a black
> leather catsuit... I already figured that if I tried to steal them jewels
> by trying to purchase a black leather catsuit I'd only have about a 1 in 10
> chance of succeeding. I also have to get a smaller gun. I also have to
> steal them jewels. All the black leather cat suits they had in stock were
> way too big for me. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to purchase
> a grappling hook... So, I thought, what if I tried to steal them jewels by
> trying to purchase a grappling hook... I already figured that if I tried to
> steal them jewels by trying to purchase a grappling hook I'd only have
> about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
> a smaller gun by trying to purchase a black leather catsuit... So, I
> thought, what if I tried to steal them jewels by trying to purchase a black
> leather catsuit... I already figured that if I tried to steal them jewels
> by trying to purchase a black leather catsuit I'd only have about a 1 in 10
> chance of succeeding. I also have to get a smaller gun. I also have to
> steal them jewels. I spent twenty minutes looking through the discount bin,
> before finding an absolutely perfect grappling hook for thirty cents. When
> I went up to pay for it, the cashier waved me off -- no charge. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to sneak
> into the museum at night... So, I thought, what if I tried to steal them
> jewels by trying to sneak into the museum at night... I already figured
> that if I tried to steal them jewels by trying to sneak into the museum at
> night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
> what if I tried to get a smaller gun by trying to go to the ninja supply
> store... So, I thought, what if I tried to steal them jewels by trying to
> go to the ninja supply store... I also have to get a smaller gun. I also
> have to steal them jewels. *
>
> *I failed to sneak into the museum at night while trying to purchase a
> grappling hook. Bummer. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to sneak
> into the museum at night... So, I thought, what if I tried to steal them
> jewels by trying to sneak into the museum at night... I already figured
> that if I tried to steal them jewels by trying to sneak into the museum at
> night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
> what if I tried to get a smaller gun by trying to go to the ninja supply
> store... So, I thought, what if I tried to steal them jewels by trying to
> go to the ninja supply store... I also have to get a smaller gun. I also
> have to steal them jewels. *
>
> *I failed to sneak into the museum at night while trying to purchase a
> grappling hook. Bummer. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to purchase
> a grappling hook... So, I thought, what if I tried to steal them jewels by
> trying to purchase a grappling hook... I already figured that if I tried to
> steal them jewels by trying to purchase a grappling hook I'd only have
> about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
> a smaller gun by trying to purchase a black leather catsuit... So, I
> thought, what if I tried to steal them jewels by trying to purchase a black
> leather catsuit... I already figured that if I tried to steal them jewels
> by trying to purchase a black leather catsuit I'd only have about a 1 in 10
> chance of succeeding. I also have to get a smaller gun. I also have to
> steal them jewels. All the black leather cat suits they had in stock were
> way too big for me. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to purchase
> a grappling hook... So, I thought, what if I tried to steal them jewels by
> trying to purchase a grappling hook... I already figured that if I tried to
> steal them jewels by trying to purchase a grappling hook I'd only have
> about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
> a smaller gun by trying to purchase a black leather catsuit... So, I
> thought, what if I tried to steal them jewels by trying to purchase a black
> leather catsuit... I already figured that if I tried to steal them jewels
> by trying to purchase a black leather catsuit I'd only have about a 1 in 10
> chance of succeeding. I also have to get a smaller gun. I also have to
> steal them jewels. "Are there any grappling hooks in stock?" The cashier,
> impassive behind his mask, shook his head slowly in response to my
> question. Then, after a moment of staring at me, he threw a smoke bomb at
> his feet. I found myself outside the shop, which was now locked. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to go to
> the ninja supply store... So, I thought, what if I tried to steal them
> jewels by trying to go to the ninja supply store... So, I thought, what if
> I tried to get a smaller gun by trying to get a museum uniform... So, I
> thought, what if I tried to steal them jewels by trying to get a museum
> uniform... The ninja supply shop was in the middle of the second floor of
> the mall, between a Hot Topic and a Zappo's. It was dimly lit, and the
> scuffed floors had a fake tatami-pattern print. There was a wad of gum
> stuck to the doorway. I also have to get a smaller gun. I also have to
> steal them jewels. *
>
> *I totally succeeded in my attempt to go to the ninja supply store by
> trying to go about it the obvious way. Yay! *
> *So, I thought, what if I tried to get a smaller gun by trying to purchase
> a grappling hook... So, I thought, what if I tried to steal them jewels by
> trying to purchase a grappling hook... I already figured that if I tried to
> steal them jewels by trying to purchase a grappling hook I'd only have
> about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
> a smaller gun by trying to purchase a black leather catsuit... So, I
> thought, what if I tried to steal them jewels by trying to purchase a black
> leather catsuit... I already figured that if I tried to steal them jewels
> by trying to purchase a black leather catsuit I'd only have about a 1 in 10
> chance of succeeding. I also have to get a smaller gun. I also have to
> steal them jewels. There was a grappling hook with two hundred feet of rope
> sitting right behind the counter, on display. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to sneak
> into the museum at night... So, I thought, what if I tried to steal them
> jewels by trying to sneak into the museum at night... I already figured
> that if I tried to steal them jewels by trying to sneak into the museum at
> night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
> what if I tried to get a smaller gun by trying to go to the ninja supply
> store... So, I thought, what if I tried to steal them jewels by trying to
> go to the ninja supply store... I also have to get a smaller gun. I also
> have to steal them jewels. *
>
> *I failed to sneak into the museum at night while trying to purchase a
> grappling hook. Bummer. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to sneak
> into the museum at night... So, I thought, what if I tried to steal them
> jewels by trying to sneak into the museum at night... I already figured
> that if I tried to steal them jewels by trying to sneak into the museum at
> night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
> what if I tried to get a smaller gun by trying to go to the ninja supply
> store... So, I thought, what if I tried to steal them jewels by trying to
> go to the ninja supply store... The ninja supply shop was in the middle of
> the second floor of the mall, between a Hot Topic and a Zappo's. It was
> dimly lit, and the scuffed floors had a fake tatami-pattern print. There
> was a wad of gum stuck to the doorway. I also have to get a smaller gun. I
> also have to steal them jewels. *
>
> *I totally succeeded in my attempt to go to the ninja supply store by
> trying to purchase a grappling hook. Yay! *
>
> *So, I thought, what if I tried to get a smaller gun by trying to purchase
> a grappling hook... So, I thought, what if I tried to steal them jewels by
> trying to purchase a grappling hook... I already figured that if I tried to
> steal them jewels by trying to purchase a grappling hook I'd only have
> about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
> a smaller gun by trying to purchase a black leather catsuit... So, I
> thought, what if I tried to steal them jewels by trying to purchase a black
> leather catsuit... I already figured that if I tried to steal them jewels
> by trying to purchase a black leather catsuit I'd only have about a 1 in 10
> chance of succeeding. I also have to get a smaller gun. I also have to
> steal them jewels. A beautiful black leather catsuit greeted me from the
> rack to the left of the doorway. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to sneak
> into the museum at night... So, I thought, what if I tried to steal them
> jewels by trying to sneak into the museum at night... I already figured
> that if I tried to steal them jewels by trying to sneak into the museum at
> night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
> what if I tried to get a smaller gun by trying to go to the ninja supply
> store... So, I thought, what if I tried to steal them jewels by trying to
> go to the ninja supply store... I also have to get a smaller gun. I also
> have to steal them jewels. *
>
> *I failed to sneak into the museum at night while trying to purchase a
> black leather catsuit. Bummer. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to sneak
> into the museum at night... So, I thought, what if I tried to steal them
> jewels by trying to sneak into the museum at night... I already figured
> that if I tried to steal them jewels by trying to sneak into the museum at
> night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
> what if I tried to get a smaller gun by trying to go to the ninja supply
> store... So, I thought, what if I tried to steal them jewels by trying to
> go to the ninja supply store... I also have to get a smaller gun. I also
> have to steal them jewels. *
>
> *I failed to sneak into the museum at night while trying to purchase a
> black leather catsuit. Bummer. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to purchase
> a grappling hook... So, I thought, what if I tried to steal them jewels by
> trying to purchase a grappling hook... I already figured that if I tried to
> steal them jewels by trying to purchase a grappling hook I'd only have
> about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
> a smaller gun by trying to purchase a black leather catsuit... So, I
> thought, what if I tried to steal them jewels by trying to purchase a black
> leather catsuit... I already figured that if I tried to steal them jewels
> by trying to purchase a black leather catsuit I'd only have about a 1 in 10
> chance of succeeding. I also have to get a smaller gun. I also have to
> steal them jewels. "Are there any grappling hooks in stock?" The cashier,
> impassive behind his mask, shook his head slowly in response to my
> question. Then, after a moment of staring at me, he threw a smoke bomb at
> his feet. I found myself outside the shop, which was now locked. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to purchase
> a grappling hook... So, I thought, what if I tried to steal them jewels by
> trying to purchase a grappling hook... I already figured that if I tried to
> steal them jewels by trying to purchase a grappling hook I'd only have
> about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
> a smaller gun by trying to purchase a black leather catsuit... So, I
> thought, what if I tried to steal them jewels by trying to purchase a black
> leather catsuit... I already figured that if I tried to steal them jewels
> by trying to purchase a black leather catsuit I'd only have about a 1 in 10
> chance of succeeding. I also have to get a smaller gun. I also have to
> steal them jewels. There was a grappling hook with two hundred feet of rope
> sitting right behind the counter, on display. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to sneak
> into the museum at night... So, I thought, what if I tried to steal them
> jewels by trying to sneak into the museum at night... I already figured
> that if I tried to steal them jewels by trying to sneak into the museum at
> night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
> what if I tried to get a smaller gun by trying to go to the ninja supply
> store... So, I thought, what if I tried to steal them jewels by trying to
> go to the ninja supply store... The ninja supply shop was in the middle of
> the second floor of the mall, between a Hot Topic and a Zappo's. It was
> dimly lit, and the scuffed floors had a fake tatami-pattern print. There
> was a wad of gum stuck to the doorway. I also have to get a smaller gun. I
> also have to steal them jewels. *
>
> *I totally succeeded in my attempt to go to the ninja supply store by
> trying to purchase a grappling hook. Yay! *
>
> *So, I thought, what if I tried to get a smaller gun by trying to purchase
> a grappling hook... So, I thought, what if I tried to steal them jewels by
> trying to purchase a grappling hook... I already figured that if I tried to
> steal them jewels by trying to purchase a grappling hook I'd only have
> about a 2 in 10 chance of succeeding. So, I thought, what if I tried to get
> a smaller gun by trying to purchase a black leather catsuit... So, I
> thought, what if I tried to steal them jewels by trying to purchase a black
> leather catsuit... I already figured that if I tried to steal them jewels
> by trying to purchase a black leather catsuit I'd only have about a 1 in 10
> chance of succeeding. I also have to get a smaller gun. I also have to
> steal them jewels. A beautiful black leather catsuit greeted me from the
> rack to the left of the doorway. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to sneak
> into the museum at night... So, I thought, what if I tried to steal them
> jewels by trying to sneak into the museum at night... I already figured
> that if I tried to steal them jewels by trying to sneak into the museum at
> night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
> what if I tried to get a smaller gun by trying to go to the ninja supply
> store... So, I thought, what if I tried to steal them jewels by trying to
> go to the ninja supply store... I also have to get a smaller gun. I also
> have to steal them jewels. *
>
> *I failed to sneak into the museum at night while trying to purchase a
> black leather catsuit. Bummer. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to sneak
> into the museum at night... So, I thought, what if I tried to steal them
> jewels by trying to sneak into the museum at night... I already figured
> that if I tried to steal them jewels by trying to sneak into the museum at
> night I'd only have about a 4 in 10 chance of succeeding. So, I thought,
> what if I tried to get a smaller gun by trying to go to the ninja supply
> store... So, I thought, what if I tried to steal them jewels by trying to
> go to the ninja supply store... I also have to get a smaller gun. I also
> have to steal them jewels. *
>
> *I totally succeeded in my attempt to sneak into the museum at night by
> trying to purchase a black leather catsuit. Yay! *
>
> *So, I thought, what if I tried to get a smaller gun by trying to go to
> the hospital... So, I thought, what if I tried to steal them jewels by
> trying to go to the hospital... So, I thought, what if I tried to get a
> smaller gun by trying to steal them jewels... So, I thought, what if I
> tried to steal them jewels by trying to steal them jewels... It turns out
> there's no way to steal them jewels by trying to go to the hospital after
> you already tried to sneak into the museum at night. *
>
> *I also have to get a smaller gun. *
>
> *I failed to steal them jewels while trying to sneak into the museum at
> night. Bummer. Now I have to heal my chest wound. I still need to get a
> smaller gun. I also still need to steal them jewels. *
>
> *So, I thought, what if I tried to get a smaller gun by trying to go to
> the hospital... So, I thought, what if I tried to heal my chest wound by
> trying to go to the hospital... So, I figured, if I tried to heal my chest
> wound by trying to go to the hospital I'd have maybe a 17% chance of
> succeding. So, I thought, what if I tried to steal them jewels by trying to
> go to the hospital... So, I thought, what if I tried to get a smaller gun
> by trying to steal them jewels... So, I thought, what if I tried to heal my
> chest wound by trying to steal them jewels... So, I thought, what if I
> tried to steal them jewels by trying to steal them jewels... I also have to
> get a smaller gun. I also have to heal my chest wound. *
>
> *I totally succeeded in my attempt to steal them jewels by trying to sneak
> into the museum at night. Yay! Now I no longer need to steal them jewels.
> Now I have to heal my arm wound. Now I have to heal my leg wound, too. I
> still need to get a smaller gun. I also still need to heal my chest wound. *
>
> *THE END*
>
>
> On Fri, Nov 13, 2015 at 7:31 AM John Ohno <john.ohno@gmail.com> wrote:
>
>> I'm thinking of several possible improvements.
>>
>> One: for each goal, have a set of dependencies (with attached
>> probabilities -- i.e., if you pose as a museum employee but you didn't
>> purchase a smaller gun your success rate for posing as a museum employee is
>> skewed downward by the 0.5% likelihood that nobody notices your giant gun;
>> or, if your wallet is stolen at the gun store then you can't buy anything
>> until you get your wallet back).
>>
>> Two: for each goal and each complication, have some sort of
>> human-readable descriptive 'texture'.
>>
>> Three: integrate a little multi-level templating system into the printmsg
>> calls, so that the structure and vocabulary can exhibit *random* variation.
>>
>>
>> I'm also thinking that, in order to produce more various stories from the
>> "steal the jewels" caper-world, I should add another path to getting the
>> jewels -- say, purchasing a black leather catsuit and a grappling hook and
>> trying to steal them during the night.
>>
>>
>> I was able to pull it together quickly only because I really did the bare
>> minimum subset of work necessary to prove the concept. I literally did a
>> tiny dependency-free minimax without pruning and then made the choice of
>> goals probablistic and turned the debugging messages into first-person
>> narration. (It looks more like a story because 'steal the jewels' isn't a
>> familiar game. But, I could have done the same thing with tic-tac-toe and
>> written a story about a guy trying to get three in a row.)
>>
>> On Fri, Nov 13, 2015 at 12:03 AM Isaac Karth <notifications@github.com>
>> wrote:
>>
>>> Also, it reminds me of a Donald E. Westlake caper. Needs more small time
>>> cons and drivers, though.
>>>
>>> —
>>> Reply to this email directly or view it on GitHub
>>> <https://github.com/dariusk/NaNoGenMo-2015/issues/155#issuecomment-156322270>
>>> .
>>>
>>
"
On Fri Nov 13 2015 17:51:18, almightyJanitor commented: "Ended up going in a completely different direction. The problem was that I was basically coding a puzzle generator, and then writing most of the story to plug the puzzle into, which I felt was not in the spirit of nanogenmo.

This new idea is inspired by the play [Sure Thing](https://en.wikipedia.org/wiki/Sure_Thing_(play)) by David Ives. The text I'm currently modifying is [The Importance of Being Earnest](http://www.gutenberg.org/cache/epub/844/pg844.txt), but I'm trying to get it to work with a novel format.

Every time a character asks a question, the answerer diverges from the script before being reprimanded by a bell. Right now it's just randomly selecting a new answer from the entire play, working on giving it more variation and avoiding characters addressing the wrong person, repeating the same answer, etc.

Snippet of output:

>Gwendolen:  [Slowly and seriously.]  You will call me sister, will you not?
>
Cecily:  Of course it was. On the 22nd of last March. You can see the entry if you like. [Shows diary.]  'To-day I broke off my engagement with Ernest. I feel it is better to do so. The weather still continues charming.' 
>
>(Bell)
>
>Gwendolen:  [Slowly and seriously.]  You will call me sister, will you not?  [They embrace. Jack and Algernon groan and walk up and down.] 
>
>Cecily:  [Rather brightly.]  There is just one question I would like to be allowed to ask my guardian. 
>
>Gwendolen:  An admirable idea!  Mr. Worthing, there is just one question I would like to be permitted to put to you. Where is your brother Ernest?
>
>Jack:  [Embracing her.]  Yes . . . mother! 
>
>(Bell)
>
>Gwendolen:  An admirable idea!  Mr. Worthing, there is just one question I would like to be permitted to put to you. Where is your brother Ernest?  We are both engaged to be married to your brother Ernest, so it is a matter of some importance to us to know where your brother Ernest is at present. 
>
>Jack:  [Slowly and hesitatingly.]  Gwendolen--Cecily--it is very painful for me to be forced to speak the truth. It is the first time in my life that I have ever been reduced to such a painful position, and I am really quite inexperienced in doing anything of the kind. However, I will tell you quite frankly that I have no brother Ernest. I have no brother at all. I never had a brother in my life, and I certainly have not the smallest intention of ever having one in the future. 
>
>Cecily:  [Surprised.]  No brother at all?
>
>Jack:  Who? 
>
>(Bell)
>
>Cecily:  [Surprised.]  No brother at all?
>
>Jack:  [Airily.]  Oh, neighbours, neighbours. 
>
>(Bell)
>
>Cecily:  [Surprised.]  No brother at all? 
>
>Jack:  [Cheerily.]  None! 
>
>Gwendolen:  [Severely.]  Had you never a brother of any kind?
>
>Jack:  I have lost both my parents. 
>
>(Bell)
>
>Gwendolen:  [Severely.]  Had you never a brother of any kind? 
>
>Jack:  [Pleasantly.]  Never. Not even of any kind."
On Fri Nov 13 2015 19:41:50, MichaelPaulukonis commented: "That's different. Trying to wrap my head around it."
On Fri Nov 13 2015 20:09:02, MichaelPaulukonis commented: "I will pray that you get this done.
"
On Fri Nov 13 2015 21:33:59, bredfern commented: "Ok i got a decent result with torch interestingly it did better with 4 layers than 10 using all of lovecraft as a source, 10 layers requires a lot larger data spurce 4 seemed to be the sweet spot. I need to process the result into a pretty web format for consumption or at least convert to epub from txt."
On Fri Nov 13 2015 22:45:16, ikarth commented: "It'd be even more obvious with multiple source texts, of course. A Shakespeare play that keeps getting derailed by an actor who think's he's in an Oscar Wilde play? Salome crossed with Arms and the Man? Though keeping it in the same family, so to speak, has an obvious advantage in your excerpt, where it preserves the tone across the alteration."
On Fri Nov 13 2015 23:31:58, maetl commented: ":heart: "
On Sat Nov 14 2015 03:52:15, hugovk commented: "A simple way of making a web-format is to generate markdown. Then if it's on GitHub, it'll render it nicely.

And tools like multimarkdown can convert MD to HTML easily.

And then Chrome can be used to print HTML to PDF (shout if you'd like some CSS pointers how to format printable pages)."
On Sat Nov 14 2015 06:00:36, hugovk opened a new issue called "405 Love Letters". And it's been completed. Sweet!
On Sat Nov 14 2015 06:02:05, hugovk commented: "Second idea done:

*405 Love Letters*

https://github.com/dariusk/NaNoGenMo-2015/issues/157"
On Sat Nov 14 2015 07:57:03, mattfister commented: "This is beautifully done. I really like the hypertext additions to jump between letters with the same words."
On Sat Nov 14 2015 17:03:30, yourpalal commented: "Unfortunately, it's not all as good as that snippet. The quality seems to be a bit all over the place at the moment, but I'm trying to improve on that! Instead of actually typesetting it with latex, I decided to produce html, but use the standard latex font, which still looks latexy. I have put a [WIP version of the document online](http://yourpalal.github.io/NaNoGenMo-2015) (it's 50k words, but I'm still improving things and regenerating it).

Things left to do:

* Add a generated table of contents.
* Turn the knobs and see if I can make things a bit more readable/coherent"
On Sat Nov 14 2015 19:33:37, emdaniels commented: "I wrote a python script to change [The Adventures of Sherlock Holmes] (http://www.gutenberg.org/ebooks/1661) into [The Adventures of Charlotte Holmes] (https://github.com/emdaniels/character-swap/blob/master/The_Adventures_of_Charlotte_Holmes.txt) and switch all the character genders to the opposite gender. I took creative grammatical license with a few pronouns, in the hope that future iterations would be able to change them more effectively. Here is an excerpt:

>To Charlotte Holmes he is always THE man. I have seldom heard her mention him under any other name. In herr eyes he eclipses and predominates the whole of him sex. It was not that she felt any emotion akin to love for Ivan Adler. All emotions, and that one particularly, were abhorrent to herr cold, precise but admirably balanced mind. She was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover she would have placed herself in a false position. She never spoke of the softer passions, save with a gibe and a sneer. They were admirable things for the observer -- excellent for drawing the veil from women's motives and actions. But for the trained reasoner to admit such intrusions into herr own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all herr mental results. Grit in a sensitive instrument, or a crack in one of herr own high-power lenses, would not be more disturbing than a strong emotion in a nature such as herr. And yet there was but one man to her, and that man was the late Ivan Adler, of dubious and questionable memory. 

The repo is [here] (https://github.com/emdaniels/character-swap), the text is 104,880 words long. If you have any questions or suggestions for improvement, let me know!"
On Sat Nov 14 2015 23:36:41, bhickey commented: "Attached is a draft of my text, **The -2147483648 Nights.** Source to follow.
[sleeper.pdf](https://github.com/dariusk/NaNoGenMo-2015/files/34807/sleeper.pdf)"
On Sun Nov 15 2015 03:58:04, hugovk commented: "Good stuff!

Little formatting suggestion: perhaps something like `textwrap` would be useful to wrap the lines to something like 70-80 characters, like the PG input, rather having one sentence per line."
On Sun Nov 15 2015 07:12:42, ikarth commented: "Surprisingly effective result from a small swerve.

Looks like the one-per-line thing is because it's not tracking the paragraphs. That'd be a good next step I think. Plus the quotation marks at the end of paragraphs are getting missed, and occasionally some other ones.

An alternate way to do it is how #72 does it: rather than writing the new file from scratch, it takes the original text file and constructs a regex that replaces the words it wants to swap. Downside: you have to algorithmically construct a complex regex. Though I suppose, since you're processing the text bit-by-bit anyway, you could re-write write it so it just goes line-by-line and doesn't care about sentences.

You could also grab nltk and use it to parse the sentences out with

    import nltk.data
    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
    sentences = sent_detector.tokenize(text)

But that may actually be overkill, since you seem to have split all the sentences correctly already."
On Sun Nov 15 2015 11:31:53, tra38 commented: "I actually did end up generating a novel of over 50,000 words, simply by duplicating the data (so instead of analyzing 99 atheists, I analyzed 99 atheists *twice*). This brings the final word count to 61,662, but I suspect most people are going to skim past the first chapter to look for any interesting combinations.

Full Text: https://github.com/tra38/The-Atheists-Who-Believe-In-God/blob/master/atheists.md
Repo: https://github.com/tra38/The-Atheists-Who-Believe-In-God

I'll also have to stop here, since I won't have much time to work onward on this project. If I had more time, I would try to create multiple templates, such as "atheist drives to work", "atheist writes a novel to promote his/her non-religion", etc.

As a proof of concept, it works and I really like the resulting output. It's much more interesting than reading a table, at least."
On Sun Nov 15 2015 13:47:07, bredfern commented: "Yeah another approach I'm thinking of is using text to speech and then
having an ajax call pull down the data file and use that as the source for
 a text to speech running where I have this animation doing a reading of
the text, it works better hearing it out loud than reading off the page
cause you can let it run in the background with text to speech.

On Sat, Nov 14, 2015 at 12:52 AM, Hugo van Kemenade <
notifications@github.com> wrote:

> A simple way of making a web-format is to generate markdown. Then if it's
> on GitHub, it'll render it nicely.
>
> And tools like multimarkdown can convert MD to HTML easily.
>
> And then Chrome can be used to print HTML to PDF (shout if you'd like some
> CSS pointers how to format printable pages).
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/51#issuecomment-156671821>
> .
>
"
On Sun Nov 15 2015 15:53:35, emdaniels commented: "Thanks for your feedback! It does seem that eventually to get to a place where gender choice happens on the fictional character level I'll need to use a tokenizer, but it's not there yet. In the interim, I added an 80 character line limit and support for creating additional texts that are all female or all male. Here's how the book reads from an entirely female point of view in [The Womanly Adventures of Charlotte Holmes](https://github.com/emdaniels/character-swap/blob/master/The_Womanly_Adventures_of_Charlotte_Holmes.txt):

>To Charlotte Holmes she is always THE woman. I have seldom heard her mention her under any other name. In herr eyes she eclipses and predominates the whole of her sex. It was not that she felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to herr cold, precise but admirably balanced mind. She was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover she would have placed herself in a false position. She never spoke of the softer passions, save with a gibe and a sneer. They were admirable things for the observer -- excellent for drawing the veil from women's motives and actions. But for the trained reasoner to admit such intrusions into herr own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all herr mental results. Grit in a sensitive instrument, or a crack in one of herr own high-power lenses, would not be more disturbing than a strong emotion in a nature such as herr. And yet there was but one woman to her, and that woman was the late Irene Adler, of dubious and questionable memory.

And here's how the book reads from an entirely male point of view in [The Manly Adventures of Sherlock Holmes](https://github.com/emdaniels/character-swap/blob/master/The_Manly_Adventures_of_Sherlock_Holmes.txt):

>To Sherlock Holmes he is always THE man. I have seldom heard him mention his under any other name. In his eyes he eclipses and predominates the whole of his sex. It was not that he felt any emotion akin to love for Ivan Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position. He never spoke of the softer passions, save with a gibe and a sneer. They were admirable things for the observer -- excellent for drawing the veil from men's motives and actions. But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his. And yet there was but one man to him, and that man was the late Ivan Adler, of dubious and questionable memory."
On Sun Nov 15 2015 17:33:17, mewo2 commented: "I've been playing around with some text generation. It started out as an Invisible Cities pastiche, but it's gone in a slightly different direction.

![map](https://cloud.githubusercontent.com/assets/360952/11171437/6b8cbc9a-8be8-11e5-82c2-654e8ffaba4d.png)

Shurgulz
========

On entering the city, one may catch sight of the colossal castle of
Barturk. From here, a traveller cannot discern how merchants crowd
around its gate. Slightly closer to the heart of the city, the traveller
will begin to hear songs of dirgeful ecstasy, sung by the bourgeoisie of
the city. In the end the fragrance of *ritig* fruit fills the wind, and
one has undeniably arrived.

In the Ancient Quarter, the alleyways are lined with *lirbsha* plants.
Philosophers flock here, practising their skill at *daplar*. In the
event that a traveller is so lucky as to stumble upon an argument
between two lovers, it may be a humbling experience. The caged *nanil*
birds will be obvious to one. These denote the home of an artisan. In
the afternoon, the traveller can sometimes hear the song of the *raski*
birds.

Kirrab
======

From Shurgulz a traveller may travel west to Kirrab. It is a pleasant
journey. As the traveller enters Zhiturk, the greenery changes, giving
way to open fields and thickets of *lirbra* flowers. At one point the
route crosses a deep gorge. Near the roadside, mighty *nirshuch* graze.

Near to the monumental city of Kirrab, one will come across a *nirpi*,
prized for its horns. Somewhat nearer to the centre of the city, the
traveller will hear the song of the *raski* birds. At last the musk of
*biltarb* trees fills the air, and one is beyond doubt in Kirrab.

In the Grey District, the boulevards are paved with lead, inlaid with
obsidian. A traveller may usually hear the sound of the prison clock.
The traveller will take note of the *raltarb* flowers. These are a
warning to evil spirits.

Rutig
=====

Leaving Kirrab a traveller can journey north to Rutig. It is an
enjoyable journey. Along the way the track crosses a broad valley. When
the traveller crosses into Dirchals, there is a variation in the light,
and a harsh cast falls over the terrain. By the verge of the road,
*lirbra* trees grow.

Entering famed Rutig, a traveller can encounter a series of pedlars,
setting out their stalls. A little nearer to the inner parts of the
city, a traveller will hear songs of plaintive joy, sung by citizens of
Rutig. Eventually the bouquet of *chalsri* fruit fills the wind, and one
is surely in Rutig.

Around the citadel, the squares are paved with granite, inlaid with
limestone. Writers mill here, praying. Should the traveller be
unfortunate enough to see artisans talking, one may be drawn in, and
unable to leave. The caged *nanil* birds will be clear. These serve to
warn off unwanted influences.

Gulztig
=======

From Rutig one may travel to Gulztig. The route is well-trodden, and the
journey takes but an afternoon. When a traveller crosses into Pibsir,
the foliage changes, giving way to hedgerows and *biltarb* plants. On
the track, *raltarb* plants grow. At one point the road crosses a deep
gorge, spotted with *shapi* flowers.

Outside the tremendous city of Gulztig, the traveller will espy the
sandstone lookout of the gargantuan fortress of Shurti. A little closer
to the inner parts of the city, a traveller can faintly hear the
striking of the theatre clock. Finally the smell of *kira* flowers fills
the breeze, and one has truly arrived.

Near the dancing-hall of Rizhti, the back-streets are lined with
*lirbsha* trees, and the blue petals sway in the air. A distracted
visitor will miss the sandstone carvings which decorate the rooftops.
These commemorate the pestilence which recently devastated a nearby
village. The traveller may usually hear the delicate song of the *raski*
birds. The idle rich teem here, laughing and dancing.

Tubil
=====

Leaving Gulztig one can journey to Tubil. It is a smooth journey. As a
traveller enters Chalszhi, there is a change in the light, and a grey
cast falls over the land. Along the way the track crosses a broad
valley. By the road, hairy *ruzhsir* graze.

At the outskirts of Tubil, one can espy the iniquitous mint of Zhubar.
From so far, the traveller has no way to observe how actors steer clear
of it. Closer to the centre of the city, the traveller begins to hear
songs of sorrowful delight, sung by petty criminals of the surrounding
countryside. Ultimately the perfume of *ritig* fruit fills the air, and
a traveller has without a doubt arrived.

Close to the counting-house of Mizhbar, the streets are lined with
*raltarb* trees, and the pleasant scent fills the late morning breeze.
Farmers congregate here, talking. A traveller will see the *zhatsal*
plants. These function as a caution to hostile influences.

Rushib
======

Leaving Tubil one can journey west to Rushib. The route is long, but
rewarding, taking two days. On the verge of the road, shaggy *ruzhsir*
graze. Along the way the track crosses a broad valley, spotted with
*lirbsha* plants. When the traveller enters Mabzirsh, there is a shift
in the light, and a pale cast falls over the landscape.

On entering the monumental city, the traveller may encounter goldsmiths,
hawking their wares. Slightly nearer to the inner parts of the city, a
traveller can faintly hear the clanging of the barbican clock.
Ultimately the bouquet of *kira* flowers fills the breeze, and one is
undoubtedly in Rushib.

In the New Quarter of the city, the roadways are paved with stone,
inlaid with ivory. Soldiers throng here, singing and telling stories. An
inattentive observer will overlook the obsidian statues that grace the
eaves. These act as a relic of the war which long ago beset a nearby
village.
"
On Sun Nov 15 2015 17:39:20, ikarth commented: "I love it! Though it is making me feel like I should hurry up if I want to try something clever with a travel journal."
On Sun Nov 15 2015 17:53:26, spenteco opened a new issue called "Browne Garden Commonplace Book".
On Sun Nov 15 2015 21:01:44, MichaelPaulukonis commented: "We are _all_ naked in the Garden, and Dreaming of Eden."
On Sun Nov 15 2015 21:07:09, MichaelPaulukonis commented: "So, your map is displayed East to West, South to North?

Since traveling west from Shurgulz leads to Kirrab, and not the ocean.

Likewise, north of Kirrab lies Rutig, not the sleepy hamlet of Rubil.

----

Otherwise, I am liking the look and sound of this.
"
On Mon Nov 16 2015 00:34:12, benblankley commented: "Okay, finished this one up! This code in golang generates 1,800 poems in the style of William Carlos Williams' "This is Just to Say", and replaces the fruits, appliances, and action words. It outputs both text and PDF, by using the gopdf library.

https://github.com/benblankley/NaNoGenMo2015"
On Mon Nov 16 2015 00:35:21, benblankley commented: "Whoops, accidentally closed it"
On Mon Nov 16 2015 03:07:55, mewo2 commented: "Yes, the map is displayed with south at the top, as is cartographic tradition in these lands."
On Mon Nov 16 2015 03:26:03, hugovk commented: "> Yes, the map is displayed with south at the top, as is cartographic tradition in these lands.

Nice! How about a compass point on the map to make it more evident?"
On Mon Nov 16 2015 04:22:15, mewo2 commented: "I didn't want to make too big a deal about it, but I'll consider it."
On Mon Nov 16 2015 05:47:52, cpressey commented: "Does [this](http://paper.li/ActiveNick/1330052730?edition_id=a066f8a0-8a39-11e5-852b-0cc47a0d15fd) count?  (Scroll down, or click the "Business" tab.)
"
On Mon Nov 16 2015 06:29:53, hugovk commented: "Nope, it's a (spammy) auto-generated thing that just points to this repo."
On Mon Nov 16 2015 08:09:43, ikarth commented: "> Nope, it's a (spammy) auto-generated thing that just points to this repo.

New idea: a generator that writes news coverage about NaNoGenMo. There should be enough from the past couple of years to make a nice little source corpus."
On Mon Nov 16 2015 08:28:44, enkiv2 commented: "Oh god. I will admit, most press coverage on generative text is pretty
formulaic. Bonus points if we take advantage of a large database of
entirely out of context quotes from Darius and Nick Montfort.

On Mon, Nov 16, 2015 at 8:09 AM Isaac Karth <notifications@github.com>
wrote:

> Nope, it's a (spammy) auto-generated thing that just points to this repo.
>
> New idea: a generator that writes news coverage about NaNoGenMo. There
> should be enough from the past couple of years to make a nice little source
> corpus.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/9#issuecomment-157023810>
> .
>
"
On Mon Nov 16 2015 08:33:31, MichaelPaulukonis commented: "@hugovk  auto-generated texts are _the worst_!"
On Mon Nov 16 2015 10:02:26, tra38 commented: ">Nope, it's a (spammy) auto-generated thing that just points to this repo.

Paper.li uses automation to engage in content curation, which is essential because there's too much content on the Internet as it is. It provides a valuable service, and is not particularly spammy (even though there are a lot of 'inactive' Paper.li newsletters that just keep on being generated with no human readers, and the periodic tweets that it sends out to its unwitting "human" curators can be incredibly annoying)."
On Mon Nov 16 2015 10:18:57, hugovk commented: "https://paper.li/stop-mentions.html"
On Mon Nov 16 2015 11:52:36, enkiv2 opened a new issue called "Fake press coverage of NaNoGenMo: a novel". And it's been completed. Sweet!
On Mon Nov 16 2015 11:57:54, enkiv2 commented: "Here's some actually machine-generated low-quality clickbait articles about
NaNoGenMo: https://github.com/enkiv2/NaNoGenMo-2015/blob/master/clickbait.md

On Mon, Nov 16, 2015 at 10:18 AM Hugo van Kemenade <notifications@github.com>
wrote:

> https://paper.li/stop-mentions.html
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/9#issuecomment-157064306>
> .
>
"
On Mon Nov 16 2015 12:17:56, dariusk commented: "Oh wow I hate paper.li! Discovering their "stop mentions" url a few years
ago (which @Hugovk links above) was very important.
"
On Mon Nov 16 2015 12:41:06, tra38 commented: ">Says Mister Kazemi, "It's more about doing something that is entertaining to yourself and possibly to other people". "It's not hard to tell a story. It's hard to tell good stories. How do you get a computer to understand what good means?", says Computer Scientist Mark Riedl. "But there's no guarantee of quality in NaNoWriMo proper, either, and there's probably less risk of emergent cryptozoological erotica", writes an article from last month in The Verge.

While the computer does not care what lines it prints out, this following paragraph 'flows' and make sense. Of course, you had to generate hundreds of paragraphs that made less sense...in order to get this one paragraph that made sense.

Consider the following near-future: Journalists stop writing entirely. "Journalists" instead proudly call themselves "editors" and "content curators", out to make sense of a insane world. Their 'best practices' are the following:
1) Write code that takes information from a corpus (or use an open-source program to generate the code for them).
2) Generate 50,000 words of nonsense, from said corpus.
3) Read through the nonsense to find something interesting.
4) Copy and paste.

Whether that future is dystopian (jobs are going away, writing loses meaning) or utopian (symbiosis of man and machine) depends on your thoughts about automation in general."
On Mon Nov 16 2015 12:46:22, ikarth commented: "> Whether that future is dystopian (jobs are going away, writing loses meaning) or utopian (symbiosis of man and machine) depends on your thoughts about automation in general.

I love how this one book says something about journalism, clickbait, automation, and the human condition, mostly through its processes rather than the individual output per se."
On Mon Nov 16 2015 12:57:59, enkiv2 commented: "Just like how in Montfort's 1k generators the evocative sentences are
essentially randomly chosen, all the quotes in the articles are real quotes
that are just chosen and placed randomly. It's sort of surprising that so
many of these collections of quotes seem to follow each other and make
sense, since they were taken from different articles and interviews, and
since more than half of Hugo's are about meow.py.

On Mon, Nov 16, 2015 at 12:46 PM Isaac Karth <notifications@github.com>
wrote:

> Whether that future is dystopian (jobs are going away, writing loses
> meaning) or utopian (symbiosis of man and machine) depends on your thoughts
> about automation in general.
>
> I love how this one book says something about journalism, clickbait,
> automation, and the human condition, mostly through its processes rather
> than the individual output per se.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/159#issuecomment-157116377>
> .
>
"
On Mon Nov 16 2015 12:58:30, VincentToups opened a new issue called "Dreams Before Speeches". And it's been completed. Sweet!
On Mon Nov 16 2015 16:51:38, MichaelPaulukonis commented: "> No description provided.

That's apt."
On Mon Nov 16 2015 17:16:52, enkiv2 commented: "I want to see this output *so hard*.

On Mon, Nov 16, 2015 at 4:51 PM Michael Paulukonis <notifications@github.com>
wrote:

> No description provided.
>
> That's apt.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/dariusk/NaNoGenMo-2015/issues/81#issuecomment-157181807>
> .
>
"
On Tue Nov 17 2015 08:49:24, enkiv2 opened a new issue called "PKD Pitches For Screenwriters: A Novel". And it's been completed. Sweet!
On Tue Nov 17 2015 12:38:45, kumo commented: "Ok. I am going to be sketching out my idea in Swift, so code can be found in this [gist](https://gist.github.com/kumo/fcf107ebc78c82cfc7f5)"
On Tue Nov 17 2015 16:33:36, dariusk commented: "Into it."
On Tue Nov 17 2015 21:56:32, MichaelPaulukonis opened a new issue called "The Tale of the Github Repository".
On Tue Nov 17 2015 21:57:04, MichaelPaulukonis commented: "It has not escaped my attention that this is also the story of NaNoGenMo-2015."
